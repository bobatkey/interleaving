\RequirePackage{amsmath} % this is here because using 'usepackage' after the \documentclass conflicts with the jfp class
\documentclass{jfp1}

\usepackage{stmaryrd}

\usepackage[usenames]{color}
\usepackage{hyperref}
\hypersetup{colorlinks=true,
            pdftitle=Interleaving data and effects,
            pdfauthor={Robert Atkey and Patricia Johann},
            pdfkeywords={inductive types, initial algebras, effects, monads, eilenberg-moore algebras, interleaved data and effects}}
\def\sectionautorefname{Section}
\def\subsectionautorefname{Section}

\usepackage{todonotes}
\newcommand{\pattynote}[1]{\todo[inline, color=green!40]{#1}}

\usepackage[all]{xy}


\newcommand{\fold}[1]{\llparenthesis #1 \rrparenthesis}
\newcommand{\eFold}[2]{\llparenthesis #1|#2 \rrparenthesis}
\newcommand{\fmext}[1]{\langle\kern-0.25em\langle #1 \rangle\kern-0.25em\rangle}
\newcommand{\construct}{\mathsf{in}}
\newcommand{\mbind}{\mathrel{>\kern-0.45em>\kern-0.45em=}}

\newcommand{\eqAnnotation}[1]{\hspace{2cm}\left\{\textrm{#1}\right\}}
\newcommand{\eqAnnotationS}[1]{\hspace{1cm}\left\{\textrm{#1}\right\}}

\newcommand{\cat}[1]{\mathcal{#1}}

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\newtheorem{proofprinciple}{Proof Principle}

\newcommand{\proofprinref}[1]{\hyperref[#1]{Proof Principle \ref*{#1}}}
\newcommand{\thmref}[1]{\hyperref[#1]{Theorem \ref*{#1}}}
\newcommand{\defref}[1]{\hyperref[#1]{Definition \ref*{#1}}}
\newcommand{\appendixref}[1]{Appendix \ref*{#1}}

\newcommand{\kw}[1]{\textbf{#1}}

\title{Interleaving data and effects}

\author[R. Atkey, P. Johann]
       {ROBERT ATKEY \\
         \vspace{1em}
         PATRICIA JOHANN \\
         Appalachian State University \\
         \email{bob.atkey@gmail.com, johannp@appstate.edu}}
       
\begin{document}

\label{firstpage}

\maketitle

\begin{abstract}
  The study of programming with and reasoning about inductive
  datatypes such as lists and trees has benefited from the simple
  categorical principle of initial algebras. In initial algebra
  semantics, each inductive datatype is represented by an initial
  $f$-algebra for an appropriate functor $f$. The initial algebra
  principle then supports the straightforward derivation of
  definitional principles and proof principles for these datatypes.
  This technique has been expanded to a whole methodology of
  structured functional programming, often called origami programming.

  In this article we show how to extend initial algebra semantics
  from pure inductive datatypes to inductive datatypes interleaved
  with computational effects. Inductive datatypes interleaved with
  effects arise naturally in many computational settings. For example,
  incrementally reading characters from a file generates a list of
  characters interleaved with input/output actions, and lazily
  constructed infinite values can be represented by pure data
  interleaved with the possibility of non-terminating
  computation. Straightforward application of initial algebra
  techniques to effectful datatypes leads either to unsound
  conclusions if we ignore the possibility of effects, or to
  unnecessarily complicated reasoning because the pure and effectful
  concerns must be considered simultaneously. We show how pure and
  effectful concerns can be separated using the abstraction of initial
  $f$-and-$m$-algebras, where the functor $f$ describes the pure part
  of a datatype and the monad $m$ describes the interleaved
  effects. Because initial $f$-and-$m$-algebras are the analogue for
  the effectful setting of initial $f$-algebras, they support the
  extension of the standard definitional and proof principles to the
  effectful setting. Because initial $f$-and-$m$-algebras separate
  pure and effectful concerns, they support the direct transfer of
  definitions and proofs from the pure setting to the effectful
  setting.

  Initial $f$-and-$m$-algebras are originally due to Filinski and
  St\o{}vring, who studied them in the category Cpo. They were
  subsequently generalised to arbitrary categories by Atkey, Ghani,
  Jacobs, and Johann in a FoSSaCS 2012 paper. In this article we aim
  to introduce the general concept of initial $f$-and-$m$-algebras to
  a general functional programming audience.
\end{abstract}

\section{Introduction}

One of the attractions of functional programming is the ease by which
programmers may lift the level of abstraction. A central example is
the use of higher-order combinators for defining and reasoning about
programs that operate on recursively defined datatypes. For example,
recursive functions on lists can often by re-expressed in terms of the
higher-order function $\mathit{foldr}$, which has the type:
\begin{displaymath}
  \mathit{foldr} :: a \to (e \to a \to a) \to [e] \to a
\end{displaymath}
The benefits of expressing recursive functions in terms of combinators
like $\mathit{foldr}$, rather than through direct use of recursion,
are twofold. Firstly, we are automatically guaranteed several
desirable properties, such as totality (on finite input), without
having to do any further reasoning. Secondly, functions defined using
$\mathit{foldr}$ obey a uniqueness property that allows us to easily
derive further properties about them. The style of
programming that uses combinators such as $\mathit{foldr}$ and its
uniqueness property has become known as ``origami programming''
\cite{gibbons03origami}, and forms a key part of the general Algebra
of Programming methodology \cite{bdm97}.

Programming and reasoning using higher-order recursion combinators is
built upon the category theoretic foundation of initial $f$-algebras
for functors $f$ \cite{GoguenTW78}. In initial algebra semantics,
datatypes are represented by carriers of initial $f$-algebras -- i.e.,
least fixed points of functors $f$ -- and combinators such as
$\mathit{foldr}$ are derived from the universal properties of initial
$f$-algebras. The initial $f$-algebra methodology has been successful
in unifying and clarifying structured functional programming and
reasoning on values of recursive datatypes that go far beyond lists
and $\mathit{foldr}$.

In this article we present a class of recursive datatypes where direct
use of the initial $f$-algebra methodology does \emph{not} provide the
right level of abstraction. Specifically, we consider recursive
datatypes that interleave pure data with effectful computation. For
example, lists of characters that are interleaved with input/output
operations that read them from an external source can be described by
the following datatype declaration:
\begin{displaymath}\label{defn:listio}
  \begin{array}{ll}
    \kw{data}~\mathit{List'_{io}}
    &
    \kw{newtype}~\mathit{List_{io}} = 
    \\
    \quad
    \begin{array}[t]{c@{\hspace{0.5em}}l}
      = & \mathsf{Nil_{io}} \\
      | & \mathsf{Cons_{io}}~\mathit{Char}~\mathit{List_{io}} \\
    \end{array}
    &
    \quad \mathsf{List_{io}}~(\mathit{IO}~\mathit{List'_{io}})
  \end{array}
\end{displaymath}
Similarly, as we shall see in \autoref{sec:motivate-interleaving},
Haskell's lazy datatypes can be thought of as pure data interleaved
with the possibility of non-termination effects.

Using the initial $f$-algebra methodology to program with and reason
about such datatypes forces us to mingle the pure and effectful parts
of our programs and proofs in a way that obscures their essential
properties (as we demonstrate in \autoref{sec:direct-eappend}). By
abstracting out the effectful parts, we arrive at the concept of
initial $f$-and-$m$-algebras, where $f$ is a functor whose initial
algebra describes the pure part of the datatype, and $m$ is a monad
that describes the effects. In this article we will show that initial
$f$-and-$m$-algebras represent a better level of abstraction for
dealing with interleaved data and effects.

The key idea is to separate the concerns of dealing with pure data,
via $f$-algebras, and effects, via $m$-Eilenberg-Moore-algebras. We
shall see in \autoref{sec:f-and-m-append} that this separation has the
following benefits:
\begin{itemize}
\item \emph{Definitions} of functions on datatypes that interleave
  data and effects look very similar to their counterparts on pure
  datatypes. We will use the example of adapting the append function
  on lists to a datatype of lists interleaved with effects to
  demonstrate this. The pure part of the computation remains the same,
  and the effectful part is straightforward. Therefore, definitions of
  functions on pure datatypes can often be transferred directly to
  their effectful counterparts. Moreover, the new definitions are
  generic in the interleaved monad we use for representing effects ---
  for example, the $\mathit{IO}$ monad for input/output effects, or
  the non-termination monad for laziness.
\item \emph{Proofs} about functions on interleaved datatypes also
  carry over almost unchanged from their pure counterparts. We
  demonstrate this through the proof of associativity for append on
  effectful lists, again generic in the monad representing the
  effects. The proof carries over almost unchanged from the proof of
  associativity of append for pure lists, except for an additional
  side condition that is discharged almost trivially.
\end{itemize}

% FIXME: side benefit of making things explicit?

The concept of initial $f$-and-$m$-algebras is originally due to
Filinski and St\o{}vring in the specific setting of Cpo (the category
of complete partial orders and continuous functions)
\cite{filinski07inductive}, and was subsequently extended to a general
category-theoretic setting for arbitrary functors $f$ by Atkey, Ghani,
Jacobs, and Johann \cite{atkey12fibrational}.  In this article, we aim
to introduce the concept of initial $f$-and-$m$-algebras to a general
functional programming audience and show how they can be used to
structure and reason about functional programs in practice, without
the heavy category-theoretic prerequisites of Atkey \emph{et al.}'s
work.

\subsection{Interleaving data and effects}
\label{sec:motivate-interleaving}

To motivate our consideration of interleaved data and effects, in this
section we give two scenarios where Haskell \emph{implicitly}
interleaves effects with pure data. By making this implicit
interleaving explicit, we will see in the main body of this article
how the $f$-and-$m$-algebra formalism allows for the implicit
assumptions made when reasoning about Haskell datatypes using initial
$f$-algebras can be made explicit as well.

\paragraph{I/O Effects}

The $\mathit{hGetContents}$ function from the Haskell standard library
provides an example of implicit interleaving of data with input/output
effects. The $\mathit{hGetContents}$ function has the following type:
\begin{displaymath}
  \mathit{hGetContents} :: \mathit{Handle} \to \mathit{IO}~[\mathit{Char}]
\end{displaymath}
Reading the type of this function, we might assume that it operates by
reading all the available data from the file referenced by the given
handle as an $\mathit{IO}$ action, yielding the list of characters as
pure data. In fact, the standard implementation of this function
postpones the reading of data from the handle until the list is
actually accessed by the program. The effect of reading from the file
handle is implicitly \emph{interleaved} with any later pure
computation on the list. This interleaving is not made apparent in the
type of $\mathit{hGetContents}$, with the following undesirable
consequences:
\begin{itemize}
\item Input/output errors that occur during reading (e.g.,~network
  failure) are reported by throwing exceptions from pure code, using
  Haskell's imprecise exceptions facility. Since the actual reading
  may occur long after the call to $\mathit{hGetContents}$ has
  apparently finished, it can be extremely difficult to determine the
  scope in which such an exception will be thrown.
\item Since it is difficult to predict when the read effects will
  occur, it is no longer safe for the programmer to close the file
  handle. The handle is implicitly closed when the end of the file is
  reached. This means that if the string returned by
  $\mathit{hGetContents}$ is never completely read, the handle will
  never be closed. Since open file handles are a finite resource
  shared by all processes on a system, the non-deterministic closing
  of file handles can be a serious problem with long-running programs.
\end{itemize}
Despite these flaws, there are good reasons for wishing to interleave
the effect of reading with data processing. A primary one is that the
file being read may be larger than the available memory, so reading it
all into a buffer may not be possible. However, the type of
$\mathit{hGetContents}$ fails to make the interleaving explicit.

Using the $\mathit{List_{io}}$ types defined on
\hyperref[defn:listio]{Page \pageref*{defn:listio}}, we can give an
implementation of $\mathit{hGetContents}$ whose type makes explicit
the interleaving of data and effects. A simple implementation can be
given in terms of the standard Haskell primitives for performing IO on
file handles:
\begin{displaymath}
  \begin{array}{l}
  \mathit{hGetContents} :: \mathit{Handle} \to \mathit{List_{io}}~\mathit{Char} \\
  \begin{array}{@{}l@{\hspace{0.2em}}l}
    \mathit{hGetContents}~h = \mathsf{List_{io}}~(\kw{do} & \mathit{isEOF} \leftarrow \mathit{hIsEOF}~h \\
    & \kw{if}~\mathit{isEOF}~\kw{then}~\mathit{return}~\mathsf{Nil}_{\mathit{io}} \\
    & \kw{else}~\kw{do}~
    \begin{array}[t]{@{}l}
      c \leftarrow \mathit{hGetChar}~h \\
      \mathit{return}~(\mathsf{Cons}_{\mathit{io}}~c~(\mathit{hGetContents}~h)))
    \end{array}
  \end{array}
\end{array}
\end{displaymath}
By using the $\mathit{List_{io}}~\mathit{Char}$ datatype, we
have made the possibility of effects between the elements of the list
explicit. Therefore, the problems we identified above with implicit
interleaving are solved: input/output failures are reported within the
scope of $\mathit{IO}$ actions, and we have access to the
$\mathit{IO}$ monad to explicitly close the file.

We return to the example of interleaved I/O effects in
\autoref{sec:coproducts-with-free-monads}, where we will see how
practical techniques that have been proposed by the Haskell community
for making the interleaving explicit can be handled neatly by using
$f$-and-$m$-algebras.

\paragraph{Non-termination} A second scenario involving implicitly
interleaved effects is built in to every Haskell type: the possibility
of non-termination while inspecting a pure value. Haskell has a
non-strict semantics, which is usually implemented using a lazy
evaluation strategy, in which the computation of a value is only
invoked if the value is actually needed. For the purposes of reasoning
about the behaviour of Haskell programs, we can model the possibility
of non-termination using the \emph{lifting} or \emph{non-termination}
monad, $(-)_\bot$. This monad adds a bottom element $\bot$ to a type,
representing the possibility of non-termination at that type.  Every
Haskell type is implicitly lifted using this monad.

A well-known benefit of Haskell's implicit possibility of
non-termination at every type is the easy representation of infinite
data structures. Laziness means that a computation that generates an
infinite value is evaluated on demand as the structure is explored. We
implicitly used this facility in our definition of
$\mathit{hGetContents}$ above to deal with the possibility of
$\mathit{Handle}$s that may return infinite streams of
values. Unfortunately, the beneficial capability of representing
infinite data structures comes with the downside that we can no longer
distinguish, just by looking at the types, between finite lists and
possibly infinite lists. Both are assigned the type $[a]$ for some
$a$. It is often the case that functions are written under the
implicit assumption that they are only applied to finite lists (for
example, the standard $\mathit{reverse}$ function). Likewise, when
reasoning about Haskell programs it is often implicitly assumed that
lists are finite, so that standard techniques like induction can be
applied. We examine the assumptions implicit in the $\mathit{reverse}$
function in \autoref{sec:reverse}.

To make these implicit assumptions explicit, we can modify the type
$\mathit{List_{io}}$ from the introduction to get the type
$\mathit{List_{lazy}}$ of lists interleaved with the possibility of
non-termination:
\begin{displaymath}
  \begin{array}{ll}
    \kw{data}~\mathit{List'_{\mathit{lazy}}}~a
    &
    \kw{newtype}~\mathit{List}_{\mathit{lazy}}~a =
    \\
    \quad
    \begin{array}[t]{c@{\hspace{0.5em}}l}
      = & \mathsf{Nil_{\mathit{lazy}}} \\
      | & \mathsf{Cons_{\mathit{lazy}}}~a~(\mathit{List_{lazy}}~a) \\
    \end{array}
    &
    \quad \mathsf{List}_{\mathit{lazy}}~(\mathit{List'_{lazy}}~a)_\bot
  \end{array}
\end{displaymath}
A value of type $\mathit{List_{lazy}}~a$ is thus a possibly
non-terminating computation that results in either a
$\mathsf{Nil}_{\mathit{lazy}}$ constructor, or
$\mathsf{Cons}_{\mathit{lazy}}$ constructor applied to a value of type
$a$ and another $\mathit{List_{lazy}}~a$. (Note that, for simplicity's
sake, we have not modelled the fact that constructors of datatypes in
Haskell also evaluate their arguments lazily.)

Correctly reasoning about values of type $\mathit{List_{lazy}}~a$ and
other lazy data structures has traditionally required the use of
domain-theoretic techniques (Pitts \cite{pitts96relational} provides a
comprehensive overview). The technique of using $f$-and-$m$-algebras
that we present in this article allows sound reasoning about lazy data
structures at an abstract level, dispensing with the need to directly
invoke domain-theoretic concepts. Indeed, Filinski and St\o{}vring
used lazy data structures as their initial motivation for introducing
$f$-and-$m$-algebras in the category Cpo \cite{filinski07inductive}.

\paragraph{A common generalisation} We have now seen two scenarios in
which list-like datatypes with interleaved effects naturally arise,
namely the $\mathit{List_{io}}$ datatype from
\hyperref[defn:listio]{Page \pageref*{defn:listio}} and the
$\mathit{List_{lazy}}$ datatype above. The obvious common
generalisation abstracts over the monad $m$:
\begin{displaymath}
  \begin{array}{ll}
    \kw{data}~\mathit{List'}~m~a
    &
    \kw{newtype}~\mathit{List}~m~a = 
    \\
    \quad
    \begin{array}[t]{c@{\hspace{0.5em}}l}
      = & \mathsf{Nil} \\
      | & \mathsf{Cons}~a~(\mathit{List}~m~a) \\
    \end{array}
    &
    \quad \mathsf{List}~(m~(\mathit{List'}~m~a))
  \end{array}
\end{displaymath}
A value of type $\mathit{List}~m~a$ consists of an effect described by
$m$, then either a $\mathsf{Nil}$ to indicate the end of the list,
or a $\mathsf{Cons}$ with a value of type $a$ and another value of
type $\mathit{List}~m~a$. Thus, this datatype describes lists of values
of type $a$ interleaved with effects from the monad $m$.

We can now generalise further by replacing the constructors
$\mathsf{Nil}$ and $\mathsf{Cons}$ with an arbitrary functor $f$ that
describes the data to be interleaved with the effects of the monad
$m$. Doing so, we arrive at the following definition:
\begin{displaymath}
  \begin{array}{ll}
    \kw{data}~\mathit{MuFM_0'}~f~m
    &
    \kw{newtype}~\mathit{MuFM_0}~f~m =
    \\
    \quad
    \begin{array}[t]{c@{\hspace{0.5em}}l}
      = & \mathsf{In}~(f~(\mathit{MuFM_0}~f~m))
    \end{array}
    &
    \quad \mathsf{Mu}~(m~(\mathit{MuFM_0'}~f~m))
  \end{array}
\end{displaymath}
(We have named these types $\mathit{MuFM_0}~f~m$ and
$\mathit{MuFM'_0}~f~m$ with a $0$ subscript because we will introduce
a more refined, but isomorphic, presentation in
\autoref{sec:f-and-m-alg-impl}.) This definition makes it clear that
the datatypes we are considering interleave pure data, represented by
the functor $f$, with effects, represented by the monad $m$. The aim
of this article is to show that $f$-and-$m$-algebras are the
appropriate level of abstraction both for defining functions that
operate on values of type $\mathit{MuFM_0}~f~m$, and for reasoning
about them.

% \bigskip

% FIXME: move this lot to the related work section.

% The Haskell community
% %\todo{link to Hackage}
% has also defined many other datatypes that capture the interleaving of
% effects with pure data\footnote{For example, the \texttt{iteratees},
%   \texttt{iterIO}, \texttt{conduits}, \texttt{enumerators}, and
%   \texttt{pipes} Haskell libraries all make use of interleaved data
%   and effects. These libraries are all available from the Hackage
%   archive of Haskell libraries
%   (\url{http://hackage.haskell.org/packages/hackage.html}).}, in order
% to make explicit the interleaving implicit in the
% $\mathit{hGetContents}$ function. One of the earliest was Kiselyov's
% Iteratees, later described in a conference publication
% \cite{kiselyov12iteratees}. In essence, Iteratees are descriptions of
% functions that alternate reading from some input with effects in some
% monad, eventually yielding some output. This can be described by the
% following datatype, which follows the same pattern of mutual recursion
% as the $\mathit{List}~m~a$ datatype declaration:
% \begin{displaymath}
%   \begin{array}{ll}
%     \kw{data}~\mathit{Reader'}~m~a~b
%     &
%     \kw{newtype}~\mathit{Reader}~m~a~b = 
%     \\
%     \quad
%     \begin{array}[t]{c@{\hspace{0.5em}}l}
%       = & \mathsf{Input}~(\mathit{Maybe}~a \to \mathit{Reader}~m~a~b) \\
%       | & \mathsf{Yield}~b
%     \end{array}
%     &
%     \quad \mathsf{Reader}~(m~(\mathit{Reader'}~m~a~b))
%   \end{array}
% \end{displaymath}
% A value of type $\mathit{Reader}~m~a~b$ is some effect described by
% the monad $m$, yielding either a result of type $b$, or a request for
% input. As Kiselyov demonstrates, the fact that values of type
% $\mathit{Reader}~m~a~b$ abstract the source of the data that they read
% is extremely powerful: different constructions allow values of type
% $\mathit{Reader}~m~a~b$ to be chained together, or connected to actual
% input/output devices, all while retaining the ability to perform
% concrete effects in the monad $m$.  Kiselyov treats the
% $\mathit{Reader}~m~a~b$ type in isolation. In this article, we show
% that it can be regarded as an instance of an application of
% interleaved data and effects: the general construction of the
% coproduct of a free monad and an arbitrary monad. Monad coproducts
% provide a general and canonical way of specifying the combination of
% two monads \cite{luth02composing}. The construction of the coproduct
% of a free monad and an arbitrary monad builds on the general
% foundation of initial $f$-and-$m$-algebras.

\subsection{The contents of this article}

We aim to make this article relatively self-contained, so we include the
necessary background to enable the reader to follow our proofs and
definitions. The structure of the remainder of the article is as
follows:
\begin{itemize}
\item In \autoref{sec:f-algebras}, we recall the standard definitions
  of $f$-algebras, initial $f$-algebras, and monads, all in a
  functional programming context. We highlight the proof principle
  associated with initial $f$-algebras
  (\proofprinref{pp:initial-alg}), and demonstrate that $f$-algebras
  can be thought of as abstract interfaces for programming and
  reasoning.
\item We introduce our main running example of list append and its
  associativity property in \autoref{sec:pure-append}. In this
  section, we make use of the initial $f$-algebra methodology for pure
  datatypes to define list append, and also to show how
  \proofprinref{pp:initial-alg} is used to prove its associativity
  property.
\item To motivate the use of $f$-and-$m$-algebras, in
  \autoref{sec:direct-eappend} we attempt to define and prove
  associative the append function for effectful lists directly from
  \proofprinref{pp:initial-alg}. This turns out to be unnecessarily
  complicated and loses the direct simplicity of the proof in the pure
  case.
\item In \autoref{sec:f-and-m-algebras}, we present the definition of
  $f$-and-$m$-algebras, and highlight the associated proof principle
  (\proofprinref{pp:initial-f-m-alg}). Initial $f$-and-$m$-algebras
  raise our level of abstraction by separating the concerns of pure
  data and effectful computation. We demonstrate the usefulness of
  this separation in \autoref{sec:f-and-m-append}, where we revisit
  the definition of list append on effectful lists, and its
  associativity property. Using initial algebra semantics for
  $f$-and-$m$-algebras, we are able to reuse much of the
  definition and proof from the pure case in
  \autoref{sec:pure-append}, and the additional work that we need to
  carry out to deal with effects is minimal.
\item In \autoref{sec:impl-f-and-m}, we show that the construction of
  initial $f$-and-$m$-algebras can be reduced to initial $(f \circ
  m)$-algebras. Consequently, we are able to give a generic
  construction of initial $f$-and-$m$-algebras for arbitrary functors
  $f$ and monads $m$.
\item In \autoref{sec:coproducts-with-free-monads}, we present an
  extended example of the use of initial
  $f$-and-$m$-algebras. Motivated by the undesirable properties of
  implicitly interleaving pure lists with I/O effects that we
  described in the previous section, the Haskell community has
  developed several approaches that explicitly interleave effects with
  data. Examples include Kiselyov's Iteratees
  \cite{kiselyov12iteratees} and Gonzalez's \texttt{pipes}
  library\footnote{\url{http://hackage.haskell.org/package/pipes}}. We
  show that these constructions are often instances of the general
  construction of the coproduct of a free monad with another
  monad. Hyland, Plotkin and Power previously gave this coproduct
  construction using purely categorical techniques. In
  \autoref{sec:coproducts-with-free-monads}, we reconstruct this
  result using $f$-and-$m$-algebras.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Background: \texorpdfstring{$f$}{f}-algebras, initial \texorpdfstring{$f$}{f}-algebras, and monads}
\label{sec:f-algebras}

Initial $f$-and-$m$-algebras build upon the foundations of initial
$f$-algebras, and of monads. We recall the definition of $f$-algebras,
initial $f$-algebras, and monads in this section, and derive the
accompanying definitional and proof principles. We will make use of
the basic definitions of the polymorphic identity function
$\mathit{id} = \lambda x.~x$ and function composition $g \circ h =
\lambda x.~g~(h~x)$.

\subsection{Basic definitions}

The initial $f$-algebra methodology uses functors $f$ to describe the
individual ``layers'' of recursive datatypes.
Formally, functors are defined as follows:
\begin{definition}\label{defn:functor}
  A \emph{functor} is a pair $(f, \mathit{fmap}_f)$ of a type operator
  $f$ and a function $\mathit{fmap}_f$ of type:
  \begin{displaymath}
    \mathit{fmap}_f :: (a \to b) \to f~a \to f~b
  \end{displaymath}
  such that $\mathit{fmap}_f$ preserves the identity function and
  composition:
  \begin{align}
    \label{eq:fmap-id}
    \mathit{fmap}_f~\mathit{id} & = \mathit{id} \\
    \label{eq:fmap-comp}
    \mathit{fmap}_f~(g \circ h) & = \mathit{fmap}_f~g \circ \mathit{fmap}_f~h
  \end{align}
\end{definition}

In Haskell, the fact that a type operator $f$ has an associated
$\mathit{fmap}_f$ is usually expressed by declaring that $f$ is a
member of the $\mathit{Functor}$ typeclass:
\begin{displaymath}
  \begin{array}{l}
    \kw{class}~\mathit{Functor}~f~\kw{where} \\
    \quad \mathit{fmap} :: (a \to b) \to f~a \to f~b
  \end{array}
\end{displaymath}
It is left to the programmer to verify that the identity and
composition laws are satisfied.
%\todo{link back to this when we talk about EM-algs}
The use of typeclasses to represent functors allows
the programmer to just write $\mathit{fmap}$ and let the type checker
infer which $f$'s associated $\mathit{fmap}$ was intended. However, in
the interest of clarity, we shall always use a subscript on
$\mathit{fmap}$ to indicate which type operator is intended.

An $f$-algebra for a given functor $f$ is a way of describing an
operation for reducing each layer in an inductive data structure to a
value. Formally, $f$-algebras are defined as follows:
\begin{definition}
  An \emph{$f$-algebra} is a pair $(a, k)$ of a
  \emph{carrier type} $a$ and a \emph{structure map}
  $k :: f~a \to a$.
\end{definition}
Given a pair of $f$-algebras, there is also the concept of a
homomorphism between them:
\begin{definition}
  Given a pair of $f$-algebras $(a,k_a)$ and $(b, k_b)$, an
  \emph{$f$-algebra homomorphism} between them is a function $h :: a
  \to b$ such that the following diagram commutes:
  \begin{equation}
    \label{eq:falgebra-homomorphism}
    \xymatrix{
      {f~a} \ar[r]^{\mathit{fmap}_f~h} \ar[d]_{k_a}
      &
      {f~b} \ar[d]^{k_b}
      \\
      {a} \ar[r]^h
      &
      {b}
    }
  \end{equation}


  % \begin{equation}
  %   h \circ \mathit{fAlgebra}_a = \mathit{fAlgebra}_b \circ \mathit{fmap}_f~h
  % \end{equation}
\end{definition}

\begin{definition}
  An \emph{initial $f$-algebra} is an $f$-algebra $(\mu \! f,
  \mathit{in})$ such that for any $f$-algebra $(a, k)$, there exists
  a unique $f$-algebra homomorphism $\fold{k} :: \mu f \to a$.
\end{definition}

The requirement that an initial $f$-algebra always has an $f$-algebra
homomorphism to any $f$-algebra allows us to define functions on the
datatypes represented by carriers $\mu \! f$ of initial
$f$-algebras. The uniqueness requirement yields the following proof
principle for functions defined on initial $f$-algebras.

\begin{proofprinciple}[Initial $f$-Algebras]\label{pp:initial-alg}
  Suppose that $(\mu f, \mathit{in})$ is an initial $f$-algebra.

  Let $(a, k)$ be an $f$-algebra, and $g :: \mu f \to
  a$ be a function. The equation
  \begin{displaymath}
    \fold{k} = g,
  \end{displaymath}
  holds if and only if $g$ is an $f$-algebra homomorphism:
  \begin{displaymath}
    g \circ \mathit{in} = k \circ \mathit{fmap}_f~g.
  \end{displaymath}
\end{proofprinciple}

We demonstrate the use of \proofprinref{pp:initial-alg} in
\autoref{sec:pure-append} below, to set up our presentation of
$f$-and-$m$-algebras and their associated proof principle. Jacobs and
Rutten \cite{JacobsR11} further develop the use of
\proofprinref{pp:initial-alg} (and its dual notion for final
coalgebras) for reasoning about recursive programs on pure data.

\subsection{Examples of initial \texorpdfstring{$f$}{f}-algebras}
\label{sec:example-initial-f}

The usefulness of the initial $f$-algebra abstraction for functional
programming lies in the fact that we can directly implement initial
$f$-algebras in functional programming languages. We give two examples
of implementations of initial $f$-algebras. The first example shows
that standard recursively defined Haskell datatypes can be retrofitted
with the initial $f$-algebra structure. The second example shows that
it is possible, in Haskell, to construct an initial $f$-algebra for
any functor $(f,\mathit{fmap_f})$.

\begin{example}
  The functor $\mathit{ListF}~a$ describes the individual layers of a
  list:
  \begin{displaymath}
    \begin{array}{@{}l@{\hspace{3em}}l}
      \begin{array}{l}
        \kw{data}~\mathit{ListF}~a~x \\
        \quad
        \begin{array}{c@{\hspace{0.3em}}l}
          = & \mathsf{Nil} \\
          | & \mathsf{Cons}~a~x
        \end{array}
      \end{array}
      &
      \begin{array}{l}
        \mathit{fmap}_{\mathit{ListF}~a} :: (x \to y) \to \mathit{ListF}~a~x \to \mathit{ListF}~a~y \\
        \begin{array}{@{}l@{\hspace{0.3em}}l@{\hspace{0.3em}}l@{\hspace{0.3em}}c@{\hspace{0.3em}}l}
          \mathit{fmap}_{\mathit{ListF}~a}&g&\mathsf{Nil} &=& \mathsf{Nil} \\
          \mathit{fmap}_{\mathit{ListF}~a}&g&(\mathsf{Cons}~a~x) &=& \mathsf{Cons}~a~(g~x)
        \end{array}
      \end{array}
    \end{array}
  \end{displaymath}
  Assuming for the moment that the Haskell datatype $[a]$ only
  contains \emph{finite} lists, the following definitions witness that
  $[a]$ is the carrier of an initial $\mathit{ListF}~a$ algebra:
  \begin{displaymath}
    \begin{array}{l}
      \mathit{in} :: \mathit{ListF}~a~[a] \to [a] \\
      \begin{array}{@{}l@{\hspace{0.3em}}l@{\hspace{0.3em}}c@{\hspace{0.3em}}l}
        \mathit{in}&\mathsf{Nil} &=& [~] \\
        \mathit{in}&(\mathsf{Cons}~a~\mathit{xs}) &=& a : \mathit{xs}
      \end{array}
    \end{array}
  \end{displaymath}
  and
  \begin{displaymath}
    \begin{array}[t]{l}
      \fold{-} :: (\mathit{ListF}~a~b \to b) \to [a] \to b \\
      \begin{array}{@{}l@{\hspace{0.3em}}l@{\hspace{0.3em}}c@{\hspace{0.3em}}l}
        \fold{k}&[~]&=&k~\mathsf{Nil} \\
        \fold{k}&(a : \mathit{xs})&=&k~(\mathsf{Cons}~a~(\fold{k}~\mathit{xs}))
      \end{array}
    \end{array}
  \end{displaymath}
  As we pointed out in \autoref{sec:motivate-interleaving}, the
  assumption that the type $[a]$ only contains finite lists is
  unsound. We have failed to account for the possibility of
  non-termination effects interleaved between the elements of the
  list. With extra effort, it is possible to integrate non-termination
  effects into the $f$-algebra formalism, as we show in
  \autoref{sec:direct-eappend}. However, in
  \autoref{sec:f-and-m-algebras} we show how $f$-and-$m$-algebras
  offer a simple and direct solution to reasoning about Haskell's lazy
  lists, as well as other datatypes interleaved with effects.
\end{example}

\begin{example}
  Again ignoring the possibility of non-termination, we can implement
  the carrier of an initial $f$-algebra for an arbitrary functor $(f,
  \mathit{fmap}_f)$ as a recursive datatype:
  \begin{equation}\label{eq:mu-defn}
    \kw{data}~\mathit{Mu}~f = \mathsf{In}~\{ \mathit{unIn} :: f~(\mathit{Mu}~f) \}
  \end{equation}
  We have used Haskell's record definition syntax to implicitly define
  a function $\mathit{unIn} :: \mathit{Mu}~f \to f~(\mathit{Mu}~f)$
  that is the inverse of the value constructor $\mathsf{In}$.  The
  $f$-algebra structure map is defined as the value
  constructor $\mathsf{In}$:
  \begin{displaymath}
    \begin{array}{l}
      \mathit{in} :: f~(\mathit{Mu}~f) \to \mathit{Mu}~f \\
      \mathit{in} = \mathsf{In}
    \end{array}
  \end{displaymath}
  and the $f$-algebra homomorphisms out of $\mathit{Mu}~f$ are defined
  in terms of the functor structure $\mathit{fmap}_f$ and Haskell's
  general recursion:
  \begin{displaymath}
    \begin{array}{l}
      \fold{-} :: \mathit{Functor}~f \Rightarrow (f~a \to a) \to \mathit{Mu}~f \to a \\
      \fold{k} = k \circ \mathit{fmap}_f~\fold{k} \circ \mathit{unIn}
    \end{array}
  \end{displaymath}
  This construction has been called ``two-level types''
  \cite{sheard04twolevel}, due to the separation between the functor
  $f$ and the recursive datatype $\mathit{Mu}$.
\end{example}

These two examples demonstrate that initial algebras for a given
functor are not unique: the types $[a]$ and
$\mathit{Mu}~(\mathit{ListF}~a)$ are not identical, but they are both
initial $(\mathit{ListF}~a)$-algebras. Therefore, we regard the
initial $f$-algebra abstraction as an interface to program against,
rather than thinking in terms of specific implementations such as
$\mathit{Mu}~f$. Note that it is possible to prove that any two
initial $f$-algebras are isomorphic, by using the initial algebra
property to define the translations between them, and
\proofprinref{pp:initial-alg} to prove that the translations are
mutually inverse. This isomorphism result is known as Lambek's Lemma
\cite{LAMBEK68}.

\subsection{Monads}

As is standard in Haskell programming, we describe effectful
computations in terms of monads \cite{moggi91notions,
  peytonjones93imperative}. We have opted to use the ``categorical''
definition of monad in terms of a $\mathit{join}$ (or
\emph{multiplication}) operation, rather than the Kleisli-triple
presentation with a bind operation ($\mbind$) that is more standard in
Haskell programming because the categorical definition is more
convenient for equational reasoning. Standard references such as the
lecture notes by Benton, Hughes and Moggi \cite{benton00monads}
discuss the translations between the two presentations.

\begin{definition}\label{defn:monad}
  A monad is a quadruple $(m, \mathit{fmap}_m, \mathit{return}_m,
  \mathit{join}_m)$ of a type constructor $m$, and three functions:
  \begin{displaymath}
    \begin{array}{r@{\hspace{0.3em}}c@{\hspace{0.3em}}l}
      \mathit{fmap}_m   & :: & (a \to b) \to m~a \to m~b \\
      \mathit{return}_m & :: & a \to m~a \\
      \mathit{join}_m   & :: & m~(m~a) \to m~a
    \end{array}
  \end{displaymath}
  such that the pair $(m, \mathit{fmap}_m)$ is a functor
  (\defref{defn:functor}), and the following properties are satisfied:
  \begin{align}
    \label{eq:monad-join-return}
    \mathit{join}_m \circ \mathit{return}_m & = \mathit{id} \\
    \label{eq:monad-join-fmap-return}
    \mathit{join}_m \circ \mathit{fmap}_m~\mathit{return}_m & = \mathit{id} \\
    \label{eq:monad-join-join}
    \mathit{join}_m \circ \mathit{fmap}_m~\mathit{join}_m & = \mathit{join}_m \circ \mathit{join}_m
  \end{align}
  and also the naturality laws:
  \begin{align}
    \label{eq:monad-return-natural}
    \mathit{return}_m \circ f & = \mathit{fmap}_m~f \circ \mathit{return}_m \\
    \label{eq:monad-join-natural}
    \mathit{join}_m \circ \mathit{fmap}_m~(\mathit{fmap}_m~f) & = \mathit{fmap}_m~f \circ \mathit{join}_m
  \end{align}
\end{definition}

As with functors and the $\mathit{Functor}$ typeclass, monads in
Haskell are usually represented in terms of the $\textit{Monad}$
typeclass. Again, for this article, we will always use subscripts on
$\mathit{return}_m$ and $\mathit{join}_m$ to disambiguate which monad
is being referred to, instead of leaving it for the reader to infer.

Finally in this short recap of monads, we recall the definition of a
\emph{monad morphism} between two monads. Monad morphisms represent
structure preserving maps between monads. We will use monad morphisms
in our extended example of the use of $f$-and-$m$-algebras to
construct the coproduct of two monads in
\autoref{sec:coproducts-with-free-monads}.

\begin{definition}
  Let $(m_1, \mathit{fmap_{m_1}}, \mathit{return}_{m_1},
  \mathit{join_{m_1}})$ and $(m_2, \mathit{fmap_{m_1}},
  \mathit{return}_{m_2}, \mathit{join}_{m_2})$ be two monads. A
  \emph{monad morphism} between them is a function $h :: m_1~a \to
  m_2~a$ such that:
  \begin{align}
    \label{eq:monad-mor-natural}
    h \circ \mathit{fmap}_{m_1}~g & = \mathit{fmap}_{m_2}~g \circ h \\
    \label{eq:monad-mor-return}
    h \circ \mathit{return}_{m_1} & = \mathit{return}_{m_2} \\
    \label{eq:monad-mor-join}
    h \circ \mathit{join}_{m_1} & = \mathit{join}_{m_2} \circ h \circ \mathit{fmap}_{m_1}~h
  \end{align}
\end{definition}

\section{List append I: pure lists}
\label{sec:pure-append}

We now introduce our running example of list append and its
associativity property. In this section, we use an initial
$(\mathit{ListF}~a)$-algebra and \proofprinref{pp:initial-alg} to
define and prove associative the append function on pure lists. In
\autoref{sec:direct-eappend} we attempt to use the initial $f$-algebra
technique to prove the analogous property in a setting with
interleaved effects, and see that direct use of initial $f$-algebras
makes the definition and proof unnecessarily complicated. In
\autoref{sec:f-and-m-algebras}, we use $f$-and-$m$-algebras to
simplify the definition and proof, and show that this lets us reuse
much of the definition and proof that we give in this section.

The definition and proof that we present here are standard and have
appeared many times in the literature. We present them in some detail
in order to use them as a reference when we cover the analogous proof
for append for lists interleaved with effects.

We program and reason against the abstract interface of initial
$f$-algebras. Hence we assume that an initial
$(\mathit{ListF}~a)$-algebra $(\mu(\mathit{ListF}~a), \mathit{in})$
exists, and we write $\fold{-}$ for the unique homomorphism induced by
initiality. We can define $\mathit{append}$ in terms of $\fold{-}$ as:
\begin{displaymath}
  \begin{array}{l}
    \mathit{append} :: \mu(\mathit{ListF}~a) \to \mu(\mathit{ListF}~a) \to \mu(\mathit{ListF}~a) \\
    \mathit{append}~\mathit{xs}~\mathit{ys} = \fold{k}~\mathit{xs} \\
    \quad\begin{array}{r@{\hspace{0.4em}}l@{\hspace{0.5em}}c@{\hspace{0.5em}}l}
      \kw{where} & k :: \mathit{ListF}~a~(\mu(\mathit{ListF}~a)) &\to& \mu(\mathit{ListF}~a) \\
      & k~\mathsf{Nil} &=& \mathit{ys} \\
      & k~(\mathsf{Cons}~a~\mathit{xs}) &=& \mathit{in}~(\mathsf{Cons}~a~\mathit{xs})
    \end{array}
  \end{array}
\end{displaymath}
Immediately from the definition of $\mathit{append}$ we know that it
is a $(\mathit{ListF}~a)$-algebra homomorphism in its first argument
because it is defined in terms of $\fold{-}$. Unfolding the
definitions shows that the following two equational properties of
$\mathit{append}$ hold. These tell us how it operates on lists of the
form $\mathit{in}~\mathsf{Nil}$ and
$\mathit{in}~(\mathsf{Cons}~a~\mathit{xs})$. We have:
\begin{align}
  \label{eq:append-nil}
  \mathit{append}~(\mathit{in}~\mathsf{Nil})~\mathit{ys} & = \mathit{ys} \\
  \label{eq:append-cons}
  \mathit{append}~(\mathit{in}~(\mathsf{Cons}~a~\mathit{xs}))~\mathit{ys} & = \mathit{in}~(\mathsf{Cons}~a~(\mathit{append}~\mathit{xs}~\mathit{ys}))
\end{align}
We now make use of these properties and \proofprinref{pp:initial-alg}
to prove associativity:

\begin{theorem}\label{thm:append-assoc}
  For all $\mathit{xs}, \mathit{ys}, \mathit{zs} :: \mu(\mathit{ListF}~a)$,
  \begin{displaymath}
    \mathit{append}~\mathit{xs}~(\mathit{append}~\mathit{ys}~\mathit{zs}) = \mathit{append}~(\mathit{append}~\mathit{xs}~\mathit{ys})~\mathit{zs}
  \end{displaymath}
\end{theorem}

\begin{proof*}
  The function $\mathit{append}$ is defined in terms of the initial
  algebra property of $\mu(\mathit{ListF}~a)$, so we can use
  \proofprinref{pp:initial-alg} to prove the equation:
  \begin{displaymath}
    \fold{k}~\mathit{xs} = \mathit{append}~(\mathit{append}~\mathit{xs}~\mathit{ys})~\mathit{zs}
  \end{displaymath}
  In this instantiation of \proofprinref{pp:initial-alg}, $g = \lambda
  \mathit{xs}.~\mathit{append}~(\mathit{append}~\mathit{xs}~\mathit{ys})~\mathit{zs}$,
  and:
  \begin{align}
    \label{eq:append-fAlgebra-nil}
    k~\mathsf{Nil} &= \mathit{append}~\mathit{ys}~\mathit{zs} \\
    \label{eq:append-fAlgebra-cons}
    k~(\mathsf{Cons}~a~\mathit{xs}) &= \mathit{in}~(\mathsf{Cons}~a~\mathit{xs})
  \end{align}
  Thus we need to prove that for all $x ::
  \mathit{ListF}~a~(\mu(\mathit{ListF}~a))$,
  \begin{displaymath}
    \begin{array}{cl}
      &\mathit{append}~(\mathit{append}~(\mathit{in}~x)~\mathit{ys})~\mathit{zs}\\
      =&k~(\mathit{fmap}_{\mathit{ListF}~a}~(\lambda \mathit{xs}.~\mathit{append}~(\mathit{append}~\mathit{xs}~\mathit{ys})~\mathit{zs})~x)
    \end{array}
  \end{displaymath}
  There are two cases to consider, depending on whether $x =
  \mathsf{Nil}$ or $x = \mathsf{Cons}~a~\mathit{xs}$. In the first
  case, we reason as follows:
  \begin{displaymath}
    \begin{array}{cl}
      & \mathit{append}~(\mathit{append}~(\mathit{in}~\mathsf{Nil})~\mathit{ys})~\mathit{zs}\\
      =&\eqAnnotation{\autoref{eq:append-nil}} \\
      & \mathit{append}~\mathit{ys}~\mathit{zs} \\
      =&\eqAnnotation{definition of $k$ (\autoref{eq:append-fAlgebra-nil})} \\
      & k~\mathsf{Nil} \\
      =&\eqAnnotation{definition of $\mathit{fmap}_{\mathit{ListF}~a}$} \\
      & k~(\mathit{fmap}_{\mathit{ListF}~a}~(\lambda \mathit{xs}.~\mathit{append}~(\mathit{append}~\mathit{xs}~\mathit{ys})~\mathit{zs})~\mathsf{Nil})
    \end{array}
  \end{displaymath}
  The other possibility is that $x = \mathsf{Cons}~a~\mathit{xs}$, and
  we reason as follows:
  \begin{displaymath}
    \begin{array}{cl}
      & \mathit{append}~(\mathit{append}~(\mathit{in}~(\mathsf{Cons}~a~\mathit{xs}))~\mathit{ys})~\mathit{zs}\\
      =&\eqAnnotation{\autoref{eq:append-cons}} \\
      & \mathit{append}~(\mathit{in}~(\mathsf{Cons}~a~(\mathit{append}~\mathit{xs}~\mathit{ys})))~\mathit{zs}\\
      =&\eqAnnotation{\autoref{eq:append-cons}} \\
      & \mathit{in}~(\mathsf{Cons}~a~(\mathit{append}~(\mathit{append}~\mathit{xs}~\mathit{ys})~\mathit{zs}))\\
      =&\eqAnnotation{definition of $k$ (\autoref{eq:append-fAlgebra-cons})} \\
      & k~(\mathsf{Cons}~a~(\mathit{append}~(\mathit{append}~\mathit{xs}~\mathit{ys})~\mathit{zs}))\\
      =&\eqAnnotation{definition of $\mathit{fmap}_{\mathit{ListF}~a}$} \\
      & k~(\mathit{fmap}_{\mathit{ListF}~a}~(\lambda \mathit{xs}.~\mathit{append}~(\mathit{append}~\mathit{xs}~\mathit{ys})~\mathit{zs})~(\mathsf{Cons}~a~\mathit{xs})) \mathproofbox\\
    \end{array}
  \end{displaymath}
\end{proof*}

Thus the proof that $\mathit{append}$ is associative is relatively
straightforward, using \proofprinref{pp:initial-alg}. We shall see
below, in \autoref{sec:direct-eappend}, that attempting to use
\proofprinref{pp:initial-alg} again to reason about lists interleaved
with effects leads to a more complicated proof that mingles the
reasoning above with reasoning about monadic effects. We then make use
of $f$-and-$m$-algebras in \autoref{sec:f-and-m-algebras} to prove the
same property for lists interleaved with effects, and show that we are
able to reuse the core of the above proof.
%\todo{Make sure this is general}

% \section{Background: monadic effects and monadic recursion schemes}
% \label{sec:monads}


% \subsection{Monadic Recursion Schemes}

% The initial $f$-algebra methodology has been extended to effectful
% computation on pure data by Fokkinga \cite{fokkinga94monadic} and
% Pardo \cite{pardo04combining}. The generic recursion combinator they
% use for effectful recursive computations has the following type:
% \begin{displaymath}
%   \fold{-}_m : (f~a \to m~a) \to \mu f \to m~a
% \end{displaymath}
% where they also make the implicit assumption that there is a
% \emph{distributive law} $d :: f~(m~a) \to m~(f~a)$ that describes how
% effects percolate through pure data. The assumption that a
% distributive law exists is too strong for our purposes. In the
% theories of Fokkinga and Pardo, the distributive law is used to push
% effects through the pure data in a uniform way. Therefore, they treat
% the case of effectful structural recursion over \emph{pure} data,
% where all the effects are pushed to the ``outside''. By contrast, we
% wish to deal with structural recursion over \emph{effectful} data,
% where data and effects are interleaved.

\section{List append II: lists with interleaved effects, via \texorpdfstring{$f$}{f}-algebras}
\label{sec:direct-eappend}

Given the success of initial $f$-algebras for defining and reasoning
about programs that operate on pure datatypes, it seems reasonable
that they might extend to programming and reasoning about programs
that operate on effectful datatypes like $\mathit{List}~m~a$ and
$\mathit{Reader}~m~a~b$. As we shall see, it is possible to use
initial $f$-algebras for reasoning about programs on effectful
datatypes, but the proofs become unnecessarily complicated. We
demonstrate this through an extension of the list append example from
\autoref{sec:pure-append} to the case of lists with interleaved
effects.

Our presentation is parametric in the kind of effects that
are interleaved with the list. We merely assume that they can be
described by some monad $(m, \mathit{fmap_m}, \mathit{return_m},
\mathit{join_m})$.

By inspecting the auxillary declaration of $\mathit{List'}~m~a$, and
comparing it to the examples of initial $f$-algebras that we presented
in the \autoref{sec:f-algebras}, we can see that they are themselves
carriers of initial $(f \circ m)$-algebras, where $f$ is an
appropriate functor and $\circ$ denotes functor composition. For
example, $\mathit{List}~m~a$ is isomorphic to $m~(\mu
(\mathit{ListF}~a \circ m))$, where $\mu (\mathit{ListF}~a \circ m)$
is the carrier of some initial $(\mathit{ListF}~a \circ m)$-algebra.

Equipped with this observation, we can proceed with adapting the
definition of $\mathit{append}$ that we gave in
\autoref{sec:pure-append} to the setting of lists interleaved with
effects. As above, we program and reason against the abstract
interface of initial algebras. We assume that an initial
$(\mathit{ListF}~a \circ m)$-algebra $(\mu(\mathit{ListF}~a \circ m),
\mathit{in})$ exists, and we write $\fold{-}$ for the unique
homomorphism induced by initiality. We now define $\mathit{eAppend}$
(``$\mathit{e}$'' for effectful) by:
\begin{displaymath}
  \begin{array}{l}
    \mathit{eAppend} :: m~(\mu ((\mathit{ListF}~a) \circ m)) \to m~(\mu ((\mathit{ListF}~a) \circ m)) \to m~(\mu ((\mathit{ListF}~a) \circ m)) \\
    \mathit{eAppend}~\mathit{xs}~\mathit{ys} = \mathit{join}_m~(\mathit{fmap}_m~\fold{k}~\mathit{xs}) \\
    \begin{array}{r@{\hspace{0.4em}}l}
      \textbf{where} & k :: \mathit{ListF}~a~(m~(m~(\mu ((\mathit{ListF}~a) \circ m)))) \to m~(\mu ((\mathit{ListF}~a) \circ m)) \\
                     &
                     \begin{array}{@{}l@{\hspace{0.5em}}c@{\hspace{0.5em}}l}
                       k~\mathsf{Nil} &=& \mathit{ys} \\
                       k~(\mathsf{Cons}~a~\mathit{xs}) &=& \mathit{return}_m~(\mathit{in}~(\mathsf{Cons}~a~(\mathit{join}_m~\mathit{xs})))
                     \end{array}
    \end{array}
  \end{array}
\end{displaymath}
This definition bears a slight resemblance to the definition of
$\mathit{append}$ above, but we have had to insert uses of the monadic
structure $\mathit{return_m}$, $\mathit{join_m}$ and $\mathit{fmap_m}$
to deal with the management of effects. Thus we have had to
intermingle the effectful parts of the definition with the pure
parts. This is a result of the fact that the initial $f$-algebra
abstraction is unaware of the presence of effects.

As we did for $\mathit{append}$ in \autoref{eq:append-nil} and
\autoref{eq:append-cons} above, we can derive two properties of
$\mathit{eAppend}$ that tell us how it acts on the two list
constructors:
\begin{equation}
  \label{eq:eappend-direct-nil}
  \mathit{eAppend}~(\mathit{return}_m~(\mathit{in}~\mathsf{Nil}))~\mathit{ys} = \mathit{ys}
\end{equation}
and
\begin{equation}\label{eq:eappend-direct-cons}
  \begin{array}{cl}
    & \mathit{eAppend}~(\mathit{return}_m~(\mathit{in}~(\mathsf{Cons}~a~\mathit{xs})))~\mathit{ys} \\ 
    =&\mathit{return}_m~(\mathit{in}~(\mathsf{Cons}~a~(\mathit{eAppend}~\mathit{xs}~\mathit{ys})))
  \end{array}
\end{equation}
We note that the derivations of these equations involve more work
than their counterparts for $\mathit{append}$. In particular, we are
forced to spend time shuffling the $\mathit{return_m}$,
$\mathit{join_m}$ and $\mathit{fmap_m}$ around in order to apply the
monad laws. Evidently, if we were to always use initial $f$-algebras
to define functions on datatypes with interleaved effects, we would be
repeating this work over again. Moreover, as we shall see in the proof
of \thmref{thm:direct-eappend-assoc} below, we cannot make direct use
of \autoref{eq:eappend-direct-nil} because we are forced to unfold
the definition of $\mathit{eAppend}$ too early.

\begin{theorem}\label{thm:direct-eappend-assoc}
  For all $\mathit{xs}, \mathit{ys}, \mathit{zs} :: m~(\mu (\mathit{ListF}~a \circ m))$,
  \begin{displaymath}
    \mathit{eAppend}~\mathit{xs}~(\mathit{eAppend}~\mathit{ys}~\mathit{zs}) = \mathit{eAppend}~(\mathit{eAppend}~\mathit{xs}~\mathit{ys})~\mathit{zs}
  \end{displaymath}
\end{theorem}

\begin{proof*}
  We will eventually be able to use \proofprinref{pp:initial-alg}, but
  first we must rearrange both sides of the equation to be of a
  suitable form. We use $k_l$ to denote an instance of the function
  $k$ defined in the body of $\mathit{eAppend}$ with the free variable
  $\mathit{ys}$ replaced by $l$.

  The left hand side of the equation to be proved is equal to:
  \begin{displaymath}
    \begin{array}{cl}
       &\mathit{eAppend}~\mathit{xs}~(\mathit{eAppend}~\mathit{ys}~\mathit{zs})
      \\ =&\eqAnnotation{definition of $\mathit{eAppend}$}
      \\ &\mathit{join_m}~(\mathit{fmap_m}~\fold{k_{\mathit{eAppend}~\mathit{ys}~\mathit{zs}}}~\mathit{xs})
    \end{array}
  \end{displaymath}
  The right hand side of the equation requires a little more work:
  \begin{displaymath}
    \begin{array}{cl}
      &\mathit{eAppend}~(\mathit{eAppend}~\mathit{xs}~\mathit{ys})~\mathit{zs}
      \\ =&\eqAnnotation{definition of $\mathit{eAppend}$}
      \\ &\mathit{eAppend}~(\mathit{join}_m~(\mathit{fmap}_m~\fold{k_{\mathit{ys}}}~\mathit{xs}))~\mathit{zs}
      \\ =&\eqAnnotation{definition of $\mathit{eAppend}$}
      \\ &\mathit{join}_m~(\mathit{fmap}_m~\fold{k_{zs}}~(\mathit{join}_m~(\mathit{fmap}_m~\fold{k_{\mathit{ys}}}~\mathit{xs})))
      \\ =&\eqAnnotation{naturality of $\mathit{join_m}$
        (\autoref{eq:monad-join-natural})}
      \\ &\mathit{join}_m~(\mathit{join}_m~(\mathit{fmap}_m~(\mathit{fmap}_m~\fold{k_{zs}})~(\mathit{fmap}_m~\fold{k_{ys}}~\mathit{xs})))
      \\ =&\eqAnnotation{monad law:
$\mathit{join_m} \circ \mathit{join_m} = \mathit{join_m} \circ \mathit{fmap_m}~\mathit{join_m}$ 
\autoref{eq:monad-join-join}} \\
      &\mathit{join_m}~(\mathit{fmap_m}~\mathit{join_m}~(\mathit{fmap}_m~(\mathit{fmap}_m~\fold{k_{zs}})~(\mathit{fmap}_m~\fold{k_{ys}}~\mathit{xs}))) \\
      =&\eqAnnotation{$\mathit{fmap_m}$ preserves composition (\autoref{eq:fmap-comp})} \\
      &\mathit{join_m}~(\mathit{fmap_m}~(\mathit{join_m} \circ \mathit{fmap}_m~\fold{k_{zs}} \circ \fold{k_{ys}})~\mathit{xs}) \\
      =&\eqAnnotation{definition of $\mathit{eAppend}$} \\
      &\mathit{join_m}~(\mathit{fmap_m}~((\lambda l.~\mathit{eAppend}~l~\mathit{zs}) \circ \fold{k_{ys}})~\mathit{xs})
    \end{array}
  \end{displaymath}
  Looking at the final lines of these two chains of equations, we see
  that the problem reduces to proving the following equation:
  \begin{equation}
    \label{eq:fold-fusion-target}
    \fold{k_{\mathit{eAppend}~\mathit{ys}~\mathit{zs}}} = (\lambda l.~\mathit{eAppend}~l~\mathit{zs}) \circ \fold{k_{ys}}
  \end{equation}
  To prove this equation, we use \proofprinref{pp:initial-alg}, which
  reduces the problem to proving the following equation for all $x ::
  \mathit{ListF}~a~(m~(\mu(\mathit{ListF}~a \circ m)))$:
  \begin{displaymath}
    \begin{array}{cl}
      &\mathit{eAppend}~(\fold{k_{ys}}~(\mathit{in}~x))~\mathit{zs} \\
      =& k_{\mathit{eAppend}~\mathit{xs}~\mathit{ys}}~(\mathit{fmap}_{\mathit{ListF}~a}~(\mathit{fmap}_m~((\lambda l.~\mathit{eAppend}~l~\mathit{zs}) \circ \fold{k_{ys}}))~x)
    \end{array}
  \end{displaymath}
  There are two cases to consider, depending on whether $x =
  \mathsf{Nil}$ or $x = \mathsf{Cons}~a~\mathit{xs}$. In the first
  case, we reason as follows. Note that, we are unable to directly
  apply our knowledge of the effect of $\mathit{eAppend}$ on
  $\mathsf{Nil}$ (\autoref{eq:eappend-direct-nil}), unlike in the
  proof of \thmref{thm:append-assoc} where we could use
  \autoref{eq:append-nil}. This is because we had to unfold the
  definition of $\mathit{eAppend}$ in order to apply
  \proofprinref{pp:initial-alg}.
  \begin{displaymath}
    \begin{array}{cl}
      &\mathit{eAppend}~(\fold{k_{ys}}~(\mathit{in}~\mathsf{Nil}))~\mathit{zs} \\
      =&\eqAnnotation{$\fold{k_{ys}}$ is a $(\mathit{ListF}~a \circ m)$-algebra homomorphism} \\
      &\mathit{eAppend}~(k_{ys}~(\mathit{fmap}_{\mathit{ListF}~a}~(\mathit{fmap}_m~\fold{k_{ys}})~\mathsf{Nil}))~\mathit{zs} \\
      =&\eqAnnotation{definition of $\mathit{fmap_{\mathit{ListF}~a}}$} \\
      &\mathit{eAppend}~(k_{ys}~\mathsf{Nil})~\mathit{zs} \\
      =&\eqAnnotation{definition of $k_{ys}$} \\
      &\mathit{eAppend}~\mathit{ys}~\mathit{zs} \\
      =&\eqAnnotation{definition of $k_{\mathit{eAppend}~\mathit{ys}~\mathit{zs}}$} \\
      &k_{\mathit{eAppend}~\mathit{ys}~\mathit{zs}}~\mathsf{Nil} \\
      =&\eqAnnotation{definition of $\mathit{fmap}_{\mathit{ListF}~a}$} \\
      &k_{\mathit{eAppend}~\mathit{xs}~\mathit{ys}}~(\mathit{fmap}_{\mathit{ListF}~a}~(\mathit{fmap}_m~((\lambda l.~\mathit{eAppend}~l~\mathit{zs}) \circ \fold{k_{ys}}))~\mathsf{Nil})
    \end{array}
  \end{displaymath}
  In the second case, when $x = \mathsf{Cons}~a~\mathit{xs}$, we
  reason using the following steps:
  \begin{displaymath}
    \begin{array}{cl}
      &\mathit{eAppend}~(\fold{k_{ys}}~(\mathit{in}~(\mathsf{Cons}~a~\mathit{xs})))~\mathit{zs} \\
      =&\eqAnnotation{$\fold{k_{ys}}$ is a $(\mathit{ListF}~a \circ m)$-algebra homomorphism} \\
      &\mathit{eAppend}~(k_{ys}~(\mathit{fmap}_{\mathit{ListF}~a}~(\mathit{fmap}_m~\fold{k_{ys}})~(\mathsf{Cons}~a~\mathit{xs})))~\mathit{zs} \\
      =&\eqAnnotation{definition of $\mathit{fmap_{\mathit{ListF}~a}}$} \\
      &\mathit{eAppend}~(k_{ys}~(\mathsf{Cons}~a~(\mathit{fmap}_m~\fold{k_{ys}}~\mathit{xs})))~\mathit{zs} \\
      =&\eqAnnotation{definition of $k_{ys}$} \\
      & \mathit{eAppend}~(\mathit{return}_m~(\mathit{in}~(\mathsf{Cons}~a~(\mathit{join}_m~(\mathit{fmap_m}~\fold{k_{ys}}~\mathit{xs})))))~\mathit{zs} \\
      =&\eqAnnotation{definition of $\mathit{eAppend}$} \\
      & \mathit{eAppend}~(\mathit{return}_m~(\mathit{in}~(\mathsf{Cons}~a~(\mathit{eAppend}~\mathit{xs}~\mathit{ys}))))~\mathit{zs} \\
      =&\eqAnnotation{\autoref{eq:eappend-direct-cons}} \\
      & \mathit{return_m}~(\mathit{in}~(\mathsf{Cons}~a~(\mathit{eAppend}~(\mathit{eAppend}~\mathit{xs}~\mathit{ys})~\mathit{zs}))) \\
      =&\eqAnnotation{definition of $\mathit{eAppend}$} \\
      &\mathit{return}_m~(\mathit{in}~(\mathsf{Cons}~a~\\
      & \hspace{1cm}(\mathit{join_m}~(\mathit{fmap}_m~\fold{k_{zs}}~(\mathit{join}_m~(\mathit{fmap_m}~\fold{k_{ys}}~\mathit{xs})))))) \\
      =&\eqAnnotation{naturality of $\mathit{join_m}$ (\autoref{eq:monad-join-natural})} \\
      &\mathit{return}_m~(\mathit{in}~(\mathsf{Cons}~a~\\
      & \hspace{1cm}(\mathit{join_m}~(\mathit{join}_m~(\mathit{fmap}_m~(\mathit{fmap_m}~\fold{k_{zs}})~(\mathit{fmap}_m~\fold{k_{ys}}~\mathit{xs})))))) \\
%    \end{array}
%  \end{displaymath}
%  \begin{displaymath}
%    \begin{array}{cl}
      =&\eqAnnotation{monad law: $\mathit{join_m} \circ \mathit{join_m} = \mathit{join_m} \circ \mathit{fmap}_m~\mathit{join_m}$ (\autoref{eq:monad-join-join})} \\
      &\mathit{return}_m~(\mathit{in}~(\mathsf{Cons}~a~\\
      & \hspace{1cm}(\mathit{join_m}~(\mathit{fmap}_m~\mathit{join_m}~\\
      & \hspace{1.5cm}(\mathit{fmap}_m~(\mathit{fmap_m}~\fold{k_{zs}})~(\mathit{fmap}_m~\fold{k_{ys}}~\mathit{xs})))))) \\
      =&\eqAnnotation{$\mathit{fmap}_m$ preserves function composition (\autoref{eq:fmap-comp})} \\
      & \mathit{return}_m~(\mathit{in}~(\mathsf{Cons}~a~\\
      & \hspace{1cm}(\mathit{join_m}~(\mathit{fmap}_m~(\mathit{join_m} \circ \mathit{fmap_m}~\fold{k_{zs}} \circ \fold{k_{ys}})~\mathit{xs})))) \\
      =&\eqAnnotation{definition of $\mathit{eAppend}$} \\
      & \mathit{return}_m~(\mathit{in}~(\mathsf{Cons}~a~\\
      & \hspace{1cm}(\mathit{join_m}~(\mathit{fmap}_m~((\lambda l.~\mathit{eAppend}~l~\mathit{zs}) \circ \fold{k_{ys}})~\mathit{xs})))) \\
      =&\eqAnnotation{definition of $k_{\mathit{eAppend}~\mathit{ys}~\mathit{zs}}$} \\
      &k_{\mathit{eAppend}~\mathit{ys}~\mathit{zs}}~(\mathsf{Cons}~a~(\mathit{fmap}_m~((\lambda l.~\mathit{eAppend}~l~\mathit{zs}) \circ \fold{k_{ys}})~\mathit{xs})) \\
      =&\eqAnnotation{definition of $\mathit{fmap}_{\mathit{ListF}~a}$} \\
      &k_{\mathit{eAppend}~\mathit{ys}~\mathit{zs}}~\\
      & \hspace{1cm}(\mathit{fmap}_{\mathit{ListF}~a}~(\mathit{fmap}_m~((\lambda l.~\mathit{eAppend}~l~\mathit{zs}) \circ \fold{k_{ys}}))~(\mathsf{Cons}~a~\mathit{xs})) \mathproofbox
    \end{array}
  \end{displaymath}
\end{proof*}

We identify the following problems with this proof:
\begin{itemize}
\item We had to perform a non-trivial number of rewriting steps in
  order to get ourselves to into a position in which we can apply
  \proofprinref{pp:initial-alg}. These steps are not specific to the
  $\mathit{eAppend}$ function, and will have to be re-done whenever we
  wish to use \proofprinref{pp:initial-alg} to prove a property of a
  function on data interleaved with effects.
\item We were forced to unfold the definition of $\mathit{eAppend}$
  multiple times in order to proceed with the calculation. As we noted
  during the proof, this unfolding prevented us from applying
  \autoref{eq:eappend-direct-nil} and instead we had to perform some
  of the same calculation steps again. For the same reason, in the
  $\mathsf{Cons}$ case, we were only able to apply
  \autoref{eq:eappend-direct-cons} once, unlike in the proof of
  \thmref{thm:append-assoc} where the analogous equation was applied
  twice. We also had to expand $\mathit{eAppend}$ again in order to
  rewrite the occurrences of $\mathit{join_m}$ and $\mathit{fmap_m}$.
\end{itemize}
To some extent, it is possible to mitigate these problems without
using $f$-and-$m$-algebras.

The first problem can be addressed by noting that
$\mathit{eAppend}~\mathit{xs}~\mathit{ys} =
\mathit{extend}~\fold{k_{\mathit{ys}}}~\mathit{xs}$, where
$\mathit{extend} :: (a \to m~b) \to m~a \to m~b$ is the argument
flipped bind $(\mbind)$ operation for the monad $m$. Using the
general fact that $\mathit{extend}~f~(\mathit{extend}~g~\mathit{x}) =
\mathit{extend}~(\mathit{extend}~f \circ g)~x$ allows for a quicker
reduction of the theorem statement to \autoref{eq:fold-fusion-target}.

The second problem can be addressed by using the general \emph{fold
  fusion law} to prove \autoref{eq:fold-fusion-target}. Fold fusion is
an important consequence of \proofprinref{pp:initial-alg} that can
first be derived as an independent lemma. In the current setting, the
fold fusion law can be stated as follows:
\begin{displaymath}
  \left.
    \begin{array}{l}
      f~(k_1~\mathsf{Nil}) = k_2~\mathsf{Nil} \\
      f~(k_1~(\mathsf{Cons}~x~\mathit{xs})) = k_2~(\mathsf{Cons}~x~(\mathit{fmap_m}~f~\mathit{xs}))
    \end{array}
  \right\} \Rightarrow f \circ \fold{k_1} = \fold{k_2}
\end{displaymath}
Using fold fusion shortens the sequences of equational reasoning for
the $\mathsf{Nil}$ and $\mathsf{Cons}$ cases by a few lines at the
start and the end, but does not free us from having to unfold the
definition of $\mathit{eAppend}$ and reason using the monad
laws. Using fold fusion and the general property of $\mathit{extend}$
does save us a little effort, but does not clearly separate the pure
and effectful parts of the proof in the way that the
$f$-and-$m$-algebra proof principle in the next section will allow us
to, and still do not allow us to directly reuse the reasoning from the
proof in the pure case in \thmref{thm:append-assoc}.


% The derivation of both the fold fusion law and the proof of
% associativity of $\mathit{append}$ based on it can then be lifted to
% the effectful setting. This modularization and instantiation does give
% a shorter and more streamlined proof than the one presented above, but
% it relies on 1) the programmer's correct extraction and formulation of
% the fold fusion law as the relevant lemma from the associativity
% problem for $\mathit{eAppend}$, 2) a correct proof that the effectful
% fold fusion law holds, and 3) the correct instantiation of that law to
% the desired result for $\mathit{eAppend}$. It seems unlikely that a
% programmer not already familiar with program fusion would devise such
% a proof strategy. Another possibility is to prove the associativity of
% $\mathit{eAppend}$ by first writing an $\mathit{append}$ function for
% the ``unwrapped'' datatype $\mathit{List_{io}'}$ as a fold for that
% type, then proving associativity for this $\mathit{append}$ function
% as an instance of fold fusion for $\mathit{List_{io}'}$, and, finally,
% using the observation that the monad library function
% $\mathit{liftM2\; op}$ is associative if $\mathit{op}$ is to lift this
% proof to a proof of associativity of $\mathit{eAppend}$. This gives
% $\mathit{eAppend}$ as $\mathit{liftM2}$ applied to the
% $\mathit{append}$ function for $\mathit{List_{io}'}$. However, this
% second approach to associativity of $\mathit{eAppend}$ requires not
% only the same programmer knowledge of fold fusion as the previous
% approach, but specialised knowledge of Haskell's monad library and
% $\mathit{liftM2}$'s properties as well. Moreover, it will not be
% applicable in situations where $\mathit{liftM2}$ fails to preserve the
% relevant properties of $\mathit{op}$. So although the proof in
% \thmref{thm:direct-eappend-assoc} it is not necessarily the shortest
% possible, it is the most straightforward, natural, and therefore the
% most likely to be attempted.

We see, then, that the definition and proof that we have given in this
section -- not to mention alternative proofs akin to those discussed
above -- demonstrate that direct use of initial $f$-algebras provides
the wrong level of abstraction for dealing with datatypes that
interleave data and effects.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Separating data and effects with \texorpdfstring{$f$}{f}-and-\texorpdfstring{$m$}{m}-algebras}
\label{sec:f-and-m-algebras}

As we saw in the previous section, directly defining and proving
properties of functions on datatypes consisting of interleaved pure
and effectful information is possible, but tedious. We were not able
to build upon the definition and proof that we used in the
non-effectful case (\autoref{sec:pure-append}), and our equational
reasoning repeatedly broke layers of abstraction: we were forced to
unfold the definition $\mathit{eAppend}$ several times in the proof of
\thmref{thm:direct-eappend-assoc} in order to perform further
calculation.

To solve the problems we have identified with the direct use of
$f$-algebras, we use the concept of $f$-and-$m$-algebras, originally
introduced by Filinski and St\o{}vring \cite{filinski07inductive}, and
generalised to arbitrary functors by Atkey, Ghani, Jacobs and Johann
\cite{atkey12fibrational}. As the name may imply, $f$-and-$m$-algebras
are simultaneously $f$-algebras and $m$-algebras. A twist is that the
$m$-algebra component must be an Eilenberg-Moore
algebra. Eilenberg-Moore algebra structure for a type $a$ describes
how to incorporate the effects of the monad $m$ into values of type
$a$.

\subsection{Eilenberg-Moore algebras}
\label{sec:eilenberg-moore-algebras}

Given a monad $(m, \mathit{fmap}_m, \mathit{return}_m,
\mathit{join}_m)$ (\defref{defn:monad}), an
$m$-Eilenberg-Moore-algebra is an $m$-algebra that also interacts well
with the structure of the monad:

\begin{definition}
  An \emph{$m$-Eilenberg-Moore algebra} consists of a pair
  $(a,l)$ of a type $a$ and a function
  \begin{displaymath}
    l :: m~a \to a
  \end{displaymath}
  such that the following two diagrams commute:
  \begin{equation}
    \label{eq:em-alg-return}
    \xymatrix{
      {a} \ar[r]^{\mathit{return}_m} \ar[rd]^{\mathit{id}}
      &
      {m~a} \ar[d]^{l}
      \\
      &
      {a}
    }
  \end{equation}
  \begin{equation}
    \label{eq:em-alg-join}
    \xymatrix{
      {m~(m~a)} \ar[r]^{\mathit{join}_m} \ar[d]_{\mathit{fmap}_m~l}
      &
      {m~a} \ar[d]^{l}
      \\
      {m~a} \ar[r]^l
      &
      {a}
    }
  \end{equation}
\end{definition}

Eilenberg-Moore algebras form a key piece of the theory of monads,
especially in their application to universal algebra. For a monad that
represents an algebraic theory (e.g., abelian groups), the collection
of all Eilenberg-Moore algebras for that monad are exactly the
structures supporting that algebraic theory. Mac Lane's book
\cite{maclane98} goes into further depth on this view of
Eilenberg-Moore algebras.

In terms of computational effects, an $m$-Eilenberg-Moore-algebra $(a,
l)$ represents a way of ``performing'' the effects of the monad $m$ in
the type $a$, preserving the $\mathit{return}_m$ and $\mathit{join_m}$
of the monad structure. For example, if we let the monad $m$ be the
error monad $\mathit{ErrorM}$:
\begin{displaymath}
  \begin{array}{ll}
    \begin{array}[t]{l}
      \kw{data}~\mathit{ErrorM}~a \\
      \quad
      \begin{array}{c@{\hspace{0.5em}}l}
        = & \mathsf{Ok}~a \\
        | & \mathsf{Error}~\mathit{String}
      \end{array}
    \end{array}
    &
    \begin{array}[t]{l}
      \begin{array}{@{}l@{\hspace{0.3em}}l@{\hspace{0.3em}}l@{\hspace{0.3em}}c@{\hspace{0.3em}}l}
        \mathit{fmap_{ErrorM}}&g&(\mathsf{Ok}~a) &=& \mathsf{Ok}~(g~a) \\
        \mathit{fmap_{ErrorM}}&g&(\mathsf{Error}~\mathit{msg}) &=& \mathsf{Error}~\mathit{msg} \\
      \end{array} \\
      \\
      \mathit{return_{ErrorM}}~a = \mathsf{Ok}~a \\
      \\
      \begin{array}{@{}l@{\hspace{0.3em}}l@{\hspace{0.3em}}c@{\hspace{0.3em}}l}
        \mathit{join_{ErrorM}}&(\mathsf{Ok}~(\mathsf{Ok}~a)) &=& \mathsf{Ok}~a \\
        \mathit{join_{ErrorM}}&(\mathsf{Ok}~(\mathsf{Error}~\mathit{msg})) &=& \mathsf{Error}~\mathit{msg} \\
        \mathit{join_{ErrorM}}&(\mathsf{Error}~\mathit{msg}) &=& \mathsf{Error}~\mathit{msg}
      \end{array}
    \end{array}
  \end{array}
\end{displaymath}
then we can define an $\mathit{ErrorM}$-Eilenberg-Moore-algebra with
carrier $\mathit{IO}~a$ as follows:
\begin{displaymath}
  \begin{array}{l}
    l :: \mathit{ErrorM}~(\mathit{IO}~a) \to \mathit{IO}~a \\
    \begin{array}{@{}l@{\hspace{0.4em}}lcl}
      l&(\mathsf{Ok}~\mathit{ioa}) & = & \mathit{ioa} \\
      l&(\mathsf{Error}~\mathit{msg}) & = & \mathit{throw}~(\mathsf{ErrorCall}~\mathit{msg})
    \end{array}
  \end{array}
\end{displaymath}
The function $\mathit{throw}$ and the constructor $\mathsf{ErrorCall}$
are part of the Haskell standard $\mathit{Control.Exception}$
module. The algebra $l$ propagates normal $\mathit{IO}$ actions, and
interprets errors using the exception throwing facilities of the
Haskell $\mathit{IO}$ monad.

The general pattern of $m$-Eilenberg-Moore-algebras with carriers that
are themselves constructed from monads has been studied by Filinski
under the name ``layered monads'' \cite{filinski99representing}. The
idea is that the presence of $m$-Eilenberg-Moore-algebras of the form
$m~(m'~a) \to m'~a$, for all $a$, capture the fact that the monad $m'$
can perform all the effects that the monad $m$ can, so we can say that
$m'$ is layered over $m$.

A particularly useful class of Eilenberg-Moore algebras for a given
monad $m$ is the class of \emph{free}
$m$-Eilenberg-Moore-algebras. The \emph{free} Eilenberg-Moore algebra
for an arbitrary type $a$ is given by $(m\,a, \,\mathit{join}_m)$. In
terms of layered monads, this just states that the monad $m$ can be
layered over itself. We will make use of this construction below in
the proof of \thmref{thm:make-initial-f-and-m-alg} below.

% If we step back from considering specific monads, there are three
% generic ways of making Eilenberg-Moore algebras for a given monad $m$:
% \begin{enumerate}
% \item 
% \item Given an $m$-Eilenberg-Moore-algebra $(a, \mathit{mAlgebra}_a)$
%   and an arbitrary type $b$, we can construct the \emph{exponential}
%   $m$-Eilenberg-Moore algebra with carrier $b \to a$ and:
%   \begin{displaymath}
%     \begin{array}{l}
%       \mathit{mAlgebra}_{b \to a} :: m~(b \to a) \to (b \to a) \\
%       \mathit{mAlgebra}_{b \to a}~x~b = \mathit{mAlgebra}_a~(\mathit{fmap}_m~(\lambda f.~f~b)~x)
%     \end{array}
%   \end{displaymath}
%   % FIXME: forward ref here if we use it
% \item Given a pair of $m$-Eilenberg-Moore-algebras $(a,
%   \mathit{mAlgebra}_a)$ and $(b, \mathit{mAlgebra}_b)$, we can form
%   the \emph{product} $m$-Eilenberg-Moore-algebra with carrier $(a,b)$
%   and:
%   \begin{displaymath}
%     \begin{array}{l}
%       \mathit{mAlgebra}_{(a,b)} :: m~(a,b) \to (a,b) \\
%       \mathit{mAlgebra}_{(a,b)}~\mathit{ab} = (\mathit{mAlgebra}_a~(\mathit{fmap}_m~\mathit{fst}~\mathit{ab}), \mathit{mAlgebra}_b~(\mathit{fmap}_m~\mathit{snd}~\mathit{ab}))
%     \end{array}
%   \end{displaymath}
%   where $\mathit{fst}$ and $\mathit{snd}$ are the first and second
%   projections from the tuple type, respectively.
% \end{enumerate}

Finally in this short introduction to Eilenberg-Moore algebras, we
define homomorphisms between $m$-Eilenberg-Moore-algebras. These are
exactly the same as homomorphisms between $f$-algebras that we defined
in \autoref{sec:f-algebras}.

\begin{definition}
  An \emph{$m$-Eilenberg-Moore-algebra homomorphism}
  \begin{displaymath}
    h :: (a, l_a) \to (b, l_b)
  \end{displaymath}
  consists of a function $h :: a \to b$ such that the following
  diagram commutes:
  \begin{equation}
    \label{eq:em-alg-homomorphism}
    \xymatrix{
      {m~a} \ar[r]^{\mathit{fmap}_m~h} \ar[d]_{l_a}
      &
      {m~b} \ar[d]^{l_b}
      \\
      {a} \ar[r]^h
      &
      {b}
    }
  \end{equation}
\end{definition}

\subsection{Definition of \texorpdfstring{$f$}{f}-and-\texorpdfstring{$m$}{m}-algebras}

As we indicated above, an $f$-and-$m$-algebra consists of an
$f$-algebra and an $m$-Eilenberg-Moore-algebra with the same
carrier. Intuitively, the $f$-algebra part deals with the pure parts
of the structure, and the $m$-Eilenberg-Moore-algebra part deals with
the effectful parts. We require the extra structure of an
Eilenberg-Moore algebra in order to account for the potential merging
of the effects that are present between the layers of the inductive
datatype (through the preservation of $\mathit{join}$) and the correct
preservation of potential lack of effects (through the preservation of
$\mathit{return}$).

\begin{definition}
  An \emph{$f$-and-$m$-algebra} consists of a triple
  $(a,k,l)$ of an object $a$ and two
  functions:
  \begin{displaymath}
    \begin{array}{rcl}
      k & :: & f~a \to a \\
      l & :: & m~a \to a
    \end{array}
  \end{displaymath}
  where $l$ is an $m$-Eilenberg-Moore algebra.
\end{definition}

Homomorphisms of $f$-and-$m$-algebras are single functions that are
simultaneously $f$-algebra homomorphisms and
$m$-Eilenberg-Moore-algebra homomorphisms:

\begin{definition}
  An \emph{$f$-and-$m$-algebra homomorphism}
  \begin{displaymath}
    h :: (a, k_a, l_a) \to (b, k_b, l_b)
  \end{displaymath}
  between two $f$-and-$m$ algebras is a function $h :: a \to b$ such
  that:
  \begin{align}
    \label{eq:fm-alg-hom1}
    h \circ k_a & = k_b \circ \mathit{fmap}_f~h \\
    \label{eq:fm-alg-hom2}
    h \circ l_a & = l_b \circ \mathit{fmap}_m~h
  \end{align}
\end{definition}

Given the above definitions, the definition of initial
$f$-and-$m$-algebra is straightforward, and follows the same
structure as for initial $f$-algebras. Abstractly, an initial
$f$-and-$m$-algebra is an initial object in the category of
$f$-and-$m$-algebras and $f$-and-$m$-algebra homomorphisms. We use the
notation $\mu(f|m)$ for carriers of initial $f$-and-$m$-algebras to
indicate the interleaving of pure data (represented by $f$) and
effects (represented by $m$).

\begin{definition}\label{def:f-and-m-folds}
  An \emph{initial $f$-and-$m$-algebra} is an $f$-and-$m$-algebra
  $(\mu(f|m), \mathit{in}_f, \mathit{in}_m)$ such that for any
  $f$-and-$m$-algebra $(a, k, l)$, there exists a unique
  $f$-and-$m$-algebra homomorphism $\eFold{k}{l} :: \mu(f|m) \to
  a$.
\end{definition}

As for initial $f$-algebras, the requirement that an initial
$f$-and-$m$-algebra always has an $f$-and-$m$-algebra homomorphism to
any other $f$-and-$m$-algebra allows us to define functions on the
carriers of initial $f$-and-$m$-algebras. The uniqueness requirement
yields the following proof principle for functions defined on initial
$f$-and-$m$-algebras. It follows the same basic form as
\proofprinref{pp:initial-alg} for initial $f$-algebras, but also
includes an obligation to prove that the right-hand side of the
equation to be shown is an $m$-Eilenberg-Moore-algebra
homomorphism.

\begin{proofprinciple}[Initial $f$-and-$m$-Algebras]
  \label{pp:initial-f-m-alg}
  Suppose that $(\mu(f|m), \mathit{in}_f, \mathit{in}_m)$ is an
  initial $f$-and-$m$-algebra. 

  Let $(a, k, l)$ be an $f$-and-$m$-algebra, and let $\eFold{k}{l}$
  denote the induced function of type $\mu(f|m) \to a$. For any
  function $g :: \mu(f|m) \to a$, the equation:
  \begin{displaymath}
    \eFold{k}{l} = g
  \end{displaymath}
  holds if and only if
  \begin{equation}\label{eq:fm-falg}
    g \circ \mathit{in}_f = k \circ \mathit{fmap}_f~g
  \end{equation}
  and
  \begin{equation}\label{eq:fm-malg}
    g \circ \mathit{in}_m = l \circ \mathit{fmap}_m~g
  \end{equation}
\end{proofprinciple}

The key feature of \proofprinref{pp:initial-f-m-alg} is that it
cleanly splits the pure and effectful proof obligations. Therefore we
may use this principle to cleanly reason about programs that operate
on interleaved pure and effectful data at a high level of abstraction,
unlike the direct reasoning we carried out in
\autoref{sec:direct-eappend}. We shall see this separation in action
for our list append running example in the next section.

\begin{example}
  The $\mathit{List}~m~a$ datatype that we defined in the introduction
  can be presented as the carrier of an initial
  $(\mathit{ListF}~a)$-and-$m$-algebra. The $\mathit{in_{\mathit{ListF}~a}}$
  function is defined as follows:
  \begin{displaymath}
    \begin{array}{l}
      \mathit{in_{\mathit{ListF}~a}} :: \mathit{ListF}~a~(\mathit{List}~m~a) \to \mathit{List}~m~a \\
      \begin{array}{@{}l@{\hspace{0.3em}}c@{\hspace{0.3em}}l}
        \mathit{in_{\mathit{ListF}~a}}~\mathsf{Nil}&=&\mathsf{List}~(\mathit{return_m}~\mathsf{Nil}_m) \\
        \mathit{in_{\mathit{ListF}~a}}~(\mathsf{Cons}~a~\mathit{xs})&=&\mathsf{List}~(\mathit{return_m}~(\mathsf{Cons}_m~a~\mathit{xs})) \\
      \end{array}
    \end{array}
  \end{displaymath}
  The $\mathit{in_m}$ component is slightly complicated by the
  presence of the $\mathsf{List}$ constructor. We use Haskell's
  $\kw{do}$ notation for convenience:
  \begin{displaymath}
    \begin{array}{l}
      \mathit{in_m} :: m~(\mathit{List}~m~a) \to \mathit{List}~m~a \\
      \mathit{in_m}~\mathit{ml} = \mathsf{List}~(\kw{do}~\{ \mathsf{List}~x \leftarrow \mathit{ml}; x \})
    \end{array}
  \end{displaymath}
  (If it were not for the $\mathsf{List}$ constructor, then
  $\mathit{in_m}$ would simply be $\mathit{join_m}$.)

  Finally, we define the induced homomorphism to any other
  $(\mathit{ListF}~a)$-and-$m$-algebra as a pair of mutually recursive
  functions, following the structure of the declaration of
  $\mathit{List}~m~a$:
  \begin{displaymath}
    \begin{array}{l}
      \eFold{-}{-} :: (\mathit{ListF}~a~b \to b) \to (m~b \to b) \to \mathit{List}~m~a \to b \\
      \eFold{k}{l} = \mathit{loop} \\
      \quad\begin{array}{r@{\hspace{0.4em}}l}
        \kw{where} & \mathit{loop} :: \mathit{List}~m~a \to b \\
        & \mathit{loop}~(\mathit{List}~x) = l~(\mathit{fmap}_m~\mathit{loop'}~x) \\
        \\
        & \mathit{loop'} :: \mathit{ListF}~a~b \to b \\
        & \mathit{loop'}~\mathsf{Nil}_m = k~\mathsf{Nil} \\
        & \mathit{loop'}~(\mathsf{Cons}_m~a~\mathit{xs}) = k~(\mathsf{Cons}~a~(\mathit{loop}~\mathit{xs}))
      \end{array}
    \end{array}
  \end{displaymath}
\end{example}
We will give a general construction of initial $f$-and-$m$-algebras in
\autoref{sec:f-and-m-alg-impl} that builds on the generic definition
of initial $f$-algebras from \autoref{sec:f-algebras}. The key result
is that the existence of initial $f$-and-$m$-algebras can be reduced
to the existence of initial $(f \circ m)$-algebras: this is
\thmref{thm:make-initial-f-and-m-alg} below.

\section{List append III: lists with interleaved effects, via \texorpdfstring{$f$}{f}-and-\texorpdfstring{$m$}{m}-algebras}
\label{sec:f-and-m-append}

We now revisit the problem of defining and proving associativity for
append on lists interleaved with effects that we examined in
\autoref{sec:direct-eappend}. We use the abstraction of (initial)
$f$-and-$m$-algebras, firstly to simplify the implementation of
$\mathit{eAppend}$ from \autoref{sec:direct-eappend}, and secondly to
simplify the proof of associativity. We shall see that both the
definition and proof mirror the definition and proof from the
pure case we presented in \autoref{sec:pure-append}.

By separating the pure and effectful parts of the proof,
\proofprinref{pp:initial-f-m-alg} allows us to reuse proofs from the
pure case. Therefore, it makes sense to ask when the additional
condition (\autoref{eq:fm-malg}) that it imposes fails. We examine an
instance of this in \autoref{sec:reverse}, where a standard property
of list reverse fails to carry over to the case of lists with
interleaved effects.

\subsection{Append for lists with interleaved effects}

We define our function $\mathit{eAppend}$ against the abstract
interface of initial $(\mathit{ListF}~a)$-and-$m$-algebras that we
defined in the previous section. Hence we assume that an initial
$(\mathit{ListF}~a)$-and-$m$-algebra $(\mu(\mathit{ListF}~a|m),
\mathit{in}_{\mathit{ListF}~a}, \mathit{in}_m)$ exists, and we denote
the unique $(\mathit{ListF}~a)$-and-$m$-algebra homomorphism using the
notation $\eFold{-}{-}$. We can define the function $\mathit{eAppend}$
in terms of initial $f$-and-$m$-algebras as:
\begin{displaymath}
  \begin{array}{l}
    \mathit{eAppend} :: \mu(\mathit{ListF}~a|m) \to \mu(\mathit{ListF}~a|m) \to \mu(\mathit{ListF}~a|m) \\
    \mathit{eAppend}~\mathit{xs}~\mathit{ys} = \eFold{k}{\mathit{in}_m}~\mathit{xs} \\
    \begin{array}{r@{\hspace{0.4em}}l@{\hspace{0.5em}}c@{\hspace{0.5em}}l}
      \textbf{where} & k~\mathsf{Nil} &=& \mathit{ys} \\ &
      k~(\mathsf{Cons}~a~\mathit{xs}) &=&
      \mathit{in}_{\mathit{ListF}~a}~(\mathsf{Cons}~a~\mathit{xs})
    \end{array}
  \end{array}
\end{displaymath}
Note that, unlike the direct definition of $\mathit{eAppend}$ that we
made in \autoref{sec:direct-eappend}, this definition is almost
identical to the definition of the function $\mathit{append}$ from
\autoref{sec:pure-append}. The only differences are the additional
$m$-Eilenberg-Moore-algebra argument to $\eFold{-}{-}$ and the
different type of $\mathit{in}_{\mathit{ListF}~a}$. The fact that the
pure part of the definition (i.e.,~the function $k$) is almost
identical to the $k$ in the definition of $\mathit{append}$ is a
result of the separation of pure and effectful concerns that the
abstraction of $f$-and-$m$-algebras affords.

Just as in the case of $\mathit{append}$, we can immediately read off
two properties of $\mathit{eAppend}$. We have one property for each of
the constructors of the type constructor $\mathit{ListF}~a$:
\begin{align}
  \label{eq:eAppend-nil}
  \mathit{eAppend}~(\mathit{in}_{\mathit{ListF}~a}~\mathsf{Nil})~\mathit{ys} & = \mathit{ys} \\
  \label{eq:eAppend-cons}
  \mathit{eAppend}~(\mathit{in}_{\mathit{ListF}~a}~(\mathsf{Cons}~a~\mathit{xs}))~\mathit{ys} & = \mathit{in}_{\mathit{ListF}~a}~(\mathsf{Cons}~a~(\mathit{eAppend}~\mathit{xs}~\mathit{ys}))
\end{align}
Both of these equations follow from the fact that the
$\eFold{k}{\mathit{in}_m}$ in the definition of $\mathit{eAppend}$ is
an $f$-and-$m$-algebra homomorphism, using \autoref{eq:fm-alg-hom1}.

Again by construction, we also know that for any fixed $\mathit{ys}$,
$\lambda \mathit{xs}.~\mathit{eAppend}~\mathit{xs}~\mathit{ys}$ is an
$m$-Eilenberg-Moore-algebra homomorphism. Hence we have the following
property of $\mathit{eAppend}$ for free, from
\autoref{eq:fm-alg-hom2}. For all $x :: m~(\mu(\mathit{ListF}~a|m))$:
\begin{equation}\label{eq:eappend-em}
  \mathit{eAppend}~(\mathit{in}_m~\mathit{x})~\mathit{ys} = \mathit{in}_m~(\mathit{fmap}_m~(\lambda \mathit{xs}.~\mathit{eAppend}~\mathit{xs}~\mathit{ys})~\mathit{x})
\end{equation}
If we unfold the definition of $\mathit{in}_m$, we can see that
\autoref{eq:eappend-em} captures the fact that $\mathit{eAppend}$
always evaluates the effects placed ``before'' the first element of
its first argument:
\begin{displaymath}
  \begin{array}{ll}
     & \mathit{eAppend}~(\mathbf{do}~\{\mathit{xs} \leftarrow x; \mathit{return}_m~(\mathit{List}~\mathit{xs}) \})~\mathit{ys} \\
    =& \mathbf{do}~\{ \mathit{xs} \leftarrow x; \mathit{return}_m~(\mathit{List}~(\mathit{return}_m~(\mathit{eAppend}~\mathit{xs}~\mathit{ys}))) \}
  \end{array}
\end{displaymath}

With these three properties of $\mathit{eAppend}$ in hand we can prove
that it is associative. We use \proofprinref{pp:initial-f-m-alg},
which splits the proof into the pure and effectful parts. As we shall
see, the pure part of the proof, where the real work happens, is
identical to the proof steps we took in the proof of
\thmref{thm:append-assoc}. The effectful parts of the proof are
straightforward, following directly from the fact that
$\mathit{eAppend}$ is an $m$-Eilenberg-Moore-algebra homomorphism
(\autoref{eq:eappend-em}).

\begin{theorem}
  For all $\mathit{xs}, \mathit{ys}, \mathit{zs} :: \mu(\mathit{ListF}~a|m)$,
  \begin{displaymath}
    \mathit{eAppend}~\mathit{xs}~(\mathit{eAppend}~\mathit{ys}~\mathit{zs}) = \mathit{eAppend}~(\mathit{eAppend}~\mathit{xs}~\mathit{ys})~\mathit{zs}
  \end{displaymath}
\end{theorem}

\begin{proof*}
  The function $\mathit{eAppend}$ is defined in terms of the initial
  algebra property of $\mu(\mathit{ListF}~a|m)$, so we can apply
  \proofprinref{pp:initial-f-m-alg}. Thus we must prove
  \autoref{eq:fm-falg} and \autoref{eq:fm-malg}. Firstly, for all $x
  :: \mathit{ListF}~a~(\mu(\mathit{ListF}~a|m))$, we must show that
  \autoref{eq:fm-falg} holds, i.e. that:
  \begin{displaymath}
    \begin{array}{cl}
      &\mathit{eAppend}~(\mathit{eAppend}~(\mathit{in}_{\mathit{ListF}~a}~x)~\mathit{ys})~\mathit{zs}\\
      =&k~(\mathit{fmap}_{\mathit{ListF}~a}~(\lambda \mathit{xs}.~\mathit{eAppend}~(\mathit{eAppend}~\mathit{xs}~\mathit{ys})~\mathit{zs})~x)
    \end{array}
  \end{displaymath}
  where
  \begin{displaymath}
    \begin{array}{rcl}
      k~\mathsf{Nil} &=& \mathit{eAppend}~\mathit{ys}~\mathit{zs} \\
      k~(\mathsf{Cons}~a~\mathit{xs}) &=& \mathit{in}_{\mathit{ListF}~a}~(\mathsf{Cons}~a~\mathit{xs})
    \end{array}
  \end{displaymath}
  This equation is, up to renaming, \emph{exactly the same} as the
  equation we had to show in proof of
  \thmref{thm:append-assoc}. Therefore, we use the same reasoning
  steps to show this equation, relying on the properties of
  $\mathit{eAppend}$ captured above in \autoref{eq:eAppend-nil} and
  \autoref{eq:eAppend-cons}.

  Secondly, we must show that the right-hand side of the equation to
  be proved is an $m$-Eilenberg-Moore-algebra homomorphism, i.e., the
  \autoref{eq:fm-malg} holds:
  \begin{displaymath}
    \begin{array}{cl}
      &\mathit{eAppend}~(\mathit{eAppend}~(\mathit{in}_m~x)~\mathit{ys})~\mathit{zs} \\
      =&\mathit{in}_m~(\mathit{fmap}_m~(\lambda \mathit{xs}.~\mathit{eAppend}~(\mathit{eAppend}~\mathit{xs}~\mathit{ys})~\mathit{zs})~x)
    \end{array}
  \end{displaymath}
  This follows straightforwardly from the fact that $\mathit{eAppend}$
  is itself an $m$-Eilenberg-Moore-algebra homomorphism, as we noted
  above in \autoref{eq:eappend-em}, and that such homomorphisms are closed under composition:
  \begin{displaymath}
    \begin{array}{cl}
      & \mathit{eAppend}~(\mathit{eAppend}~(\mathit{in}_m~x)~\mathit{ys})~\mathit{zs} \\
      =&\eqAnnotationS{\autoref{eq:eappend-em}} \\
      & \mathit{eAppend}~(\mathit{in}_m~(\mathit{fmap}_m~(\lambda \mathit{xs}.~\mathit{eAppend}~\mathit{xs}~\mathit{ys})~x))~\mathit{zs} \\
      =&\eqAnnotationS{\autoref{eq:eappend-em}} \\
      & \mathit{in}_m~(\mathit{fmap}_m~(\lambda \mathit{xs}.~\mathit{eAppend}~\mathit{xs}~\mathit{zs})~(\mathit{fmap}_m~(\lambda \mathit{xs}.~\mathit{eAppend}~\mathit{xs}~\mathit{ys})~x)) \\
      =&\eqAnnotationS{$\mathit{fmap}_m$ preserves function composition (\autoref{eq:fmap-comp})} \\
      & \mathit{in}_m~(\mathit{fmap}_m~(\lambda \mathit{xs}.~\mathit{eAppend}~(\mathit{eAppend}~\mathit{xs}~\mathit{ys})~\mathit{zs})~x) \mathproofbox
    \end{array}
  \end{displaymath}
\end{proof*}
As promised, the proof that $\mathit{eAppend}$ is associative, using
\proofprinref{pp:initial-f-m-alg}, is much simpler than the direct
$f$-algebra proof we attempted in \autoref{sec:direct-eappend}. In
addition, the separation of pure and effectful parts has meant that we
were able to reuse the proof of the pure case from
\autoref{sec:pure-append}, and so need only to establish the side
condition for effects.

\subsection{Reverse for lists with interleaved effects?}
\label{sec:reverse}

Given the above example of a proof of a property of a function on pure
lists carrying over almost unchanged to lists interleaved with
effects, we might wonder if there are circumstances where this
approach fails. Clearly, it cannot be the case that all properties
true for pure lists carry over to effectful lists. One example of a
property that fails to carry over is the following property of the
reverse function:
\begin{equation}\label{eq:reverse-append}
  \mathit{reverse}~(\mathit{append}~\mathit{xs}~\mathit{ys}) = \mathit{append}~(\mathit{reverse}~\mathit{ys})~(\mathit{reverse}~\mathit{xs})
\end{equation}
Intuitively, this property cannot possibly hold for a reverse function
on lists interleaved with effects, since in order to reverse a list,
all of the effects inside it must be executed in order to reach the
last element and place it at the head of the new list. Thus the left
hand side of the equation above will execute all the effects of
$\mathit{xs}$ and then $\mathit{ys}$ in order, whereas the right hand
side will execute all the effects of $\mathit{ys}$ first, and then
$\mathit{xs}$. If the interleaved effects involve the possibility of
non-termination, as in the $\mathit{List_{lazy}}$ example in
\autoref{sec:motivate-interleaving}, then $\mathit{reverse}$ may never
get to the last element of the list.

If we try to prove this property using
\proofprinref{pp:initial-f-m-alg}, we see that we are unable to prove
\autoref{eq:fm-malg}, namely that the right-hand side must be an
Eilenberg-Moore-algebra homomorphism.

We can define a reverse function on effectful lists as follows. This
is very similar to the standard definition of (non-tail recursive)
reverse on pure lists, and makes use of the $\mathit{eAppend}$
function we defined above.
\begin{displaymath}
  \begin{array}{l}
    \mathit{eReverse} :: \mu(\mathit{ListF}~a|m) \to \mu(\mathit{ListF}~a|m) \\
    \mathit{eReverse} = \eFold{k}{\mathit{in}_m} \\
    \quad\begin{array}{r@{\hspace{0.4em}}l@{\hspace{0.5em}}c@{\hspace{0.5em}}l}
      \kw{where} & k~\mathsf{Nil} &=& \mathit{in}_{\mathit{ListF}~a}~\mathsf{Nil} \\
      & k~(\mathsf{Cons}~a~\mathit{xs}) &=& \mathit{eAppend}~\mathit{xs}~(\mathit{in}_{\mathit{ListF}~a}~(\mathsf{Cons}~a~(\mathit{in}_{\mathit{ListF}~a}~\mathsf{Nil})))
    \end{array}
  \end{array}
\end{displaymath}
Verifying the effectful analogue of \autoref{eq:reverse-append} requires a little extra step
before we can apply \proofprinref{pp:initial-f-m-alg}, because the
left-hand side of the equation is constructed from a composite of two
functions of the form $\eFold{-}{-}$. However, it is
straightforward to prove that this composite is equal to
$\eFold{\mathit{alg}}{\mathit{in}_m}$, where
\begin{displaymath}
  \begin{array}{l}
    k' :: \mathit{ListF}~a~(\mu(\mathit{ListF}~a|m)) \to \mu(\mathit{ListF}~a|m) \\
    \begin{array}{@{}l@{\hspace{0.5em}}lcl}
      k'&\mathsf{Nil} &=& \mathit{eReverse}~\mathit{ys} \\
      k'&(\mathsf{Cons}~a~\mathit{xs}) & =& \mathit{eAppend}~\mathit{xs}~(\mathit{in}_{\mathit{ListF}~a}~(\mathsf{Cons}~a~(\mathit{in}_{\mathit{ListF}~a}~\mathsf{Nil})))
    \end{array}
  \end{array}
\end{displaymath}
This same extra step is required in the case for pure datatypes as
well, so this is not where the problem with interleaved effects
lies. If we attempt to apply \proofprinref{pp:initial-f-m-alg} to the
equation:
\begin{displaymath}
  \eFold{k'}{\mathit{in}_m}~\mathit{xs} = \mathit{eAppend}~(\mathit{eReverse}~\mathit{ys})~(\mathit{eReverse}~\mathit{xs})
\end{displaymath}
Then the pure part of the proof goes through straightforwardly. We are
left with proving that the right-hand side of this equation is an
Eilenberg-Moore-algebra homomorphism in its second
argument. Certainly, $\mathit{eReverse}$ is an Eilenberg-Moore-algebra
homomorphism by its construction via the initial $f$-and-$m$-algebra
property. However, $\mathit{eAppend}$ is not an Eilenberg-Moore
algebra homomorphism in its \emph{second} argument, as the following
counterexample shows.

If we let the monad $m$ be the $\mathit{ErrorM}$ monad we defined in
\autoref{sec:eilenberg-moore-algebras}, then if $\mathit{eAppend}$
were an Eilenberg-Moore-algebra homomorphism in its second argument
the following equation would hold:
\begin{equation}\label{eq:purported-em}
  \begin{array}{cl}
    &\mathit{eAppend}~(\mathit{in}_{\mathit{ListF}~a}~(\mathsf{Cons}~a~(\mathit{in}_{\mathit{ListF}~a}~\mathsf{Nil})))~(\mathit{in}_{\mathit{ErrorM}}~(\mathsf{Error}~\texttt{"msg"}))
    \\ =&\mathit{in}_{\mathit{ErrorM}}~\\ & \hspace{1cm}(\mathit{fmap}_{\mathit{ErrorM}}~(\lambda
    ys.~\mathit{eAppend}~(\mathit{in}_{\mathit{ListF}~a}~(\mathsf{Cons}~a~(\mathit{in}_{\mathit{ListF}~a}~\mathsf{Nil}))))~\\ & \hspace{1.5cm}(\mathsf{Error}~\texttt{"msg"}))
  \end{array}
\end{equation}
However, starting from the left-hand side, we calculate as follows:
\begin{displaymath}
  \begin{array}{cl}
    & \mathit{eAppend}~(\mathit{in}_{\mathit{ListF}~a}~(\mathsf{Cons}~a~(\mathit{in}_{\mathit{ListF}~a}~\mathsf{Nil})))~(\mathit{in}_{\mathit{ErrorM}}~(\mathsf{Error}~\texttt{"msg"})) \\
    =&\eqAnnotation{\autoref{eq:eAppend-cons}} \\
    & \mathit{in}_{\mathit{ListF}~a}~(\mathit{Cons}~a~(\mathit{eAppend}~(\mathit{in}_{\mathit{ListF}~a}~\mathsf{Nil})~(\mathit{in}_{\mathit{ErrorM}}~(\mathsf{Error}~\texttt{"msg"})))) \\
    =&\eqAnnotation{\autoref{eq:eAppend-nil}} \\
    & \mathit{in}_{\mathit{ListF}~a}~(\mathit{Cons}~a~(\mathit{in}_{\mathit{ErrorM}}~(\mathsf{Error}~\texttt{"msg"})))
  \end{array}
\end{displaymath}
while the right hand side of \autoref{eq:purported-em} reduces by the
definition of $\mathit{fmap}_{\mathit{ErrorM}}$ to simply:
\begin{displaymath}
  \mathit{in}_{\mathit{ErrorM}}~(\mathsf{Error}~\texttt{"msg"})
\end{displaymath}
Thus the proof fails. This is the formal rendering of the intuition
for the failure given at the start of this subsection.

% As a consequence, the involution property:
% \begin{displaymath}
%   \mathit{eReverse}~(\mathit{eReverse}~\mathit{xs}) = \mathit{xs}
% \end{displaymath}
% does not hold in the presence of arbitrary interleaved
% effects. Intuitively, it is again clear why: the expression on the
% left pushes all the effects to the start of the list as
% $\mathit{eReverse}$ traverses the list, while the right-hand side
% leaves the effects interspersed between the elements.

\section{Generic implementation of initial \texorpdfstring{$f$}{f}-and-\texorpdfstring{$m$}{m}-algebras}
\label{sec:impl-f-and-m}

We have seen that existing datatypes such as $\mathit{List}~m~a$ can
be given the structure of initial $f$-and-$m$-algebras. In this
section, we show that, in Haskell, we can implement an initial
$f$-and-$m$-algebra for any functor $f$ and monad $m$. We build on the
generic implementation of initial $f$-algebras we presented in
\autoref{sec:example-initial-f}. The key construction is to show that
if we have an initial $(f \circ m)$-algebra, then we can construct an
initial $f$-and-$m$-algebra.


\subsection{From initial \texorpdfstring{$(f \circ m)$}{(f . m)}-algebras to initial \texorpdfstring{$f$}{f}-and-\texorpdfstring{$m$}{m}-algebras}
\label{sec:from-initial}

Initial $f$-and-$m$-algebras can be constructed from initial $(f \circ
m)$-algebras. If the type $\mu(f \circ m)$ is the carrier of an
initial $(f \circ m)$-algebra, then the initial $f$-and-$m$-algebra
that we construct has carrier $m~(\mu (f \circ m))$. One way of
looking at the proof of the following theorem is as containing all the
additional parts of the definition and proof steps we carried out in
the direct initial $f$-algebra proof of associativity in
\autoref{sec:direct-eappend} that were missing in the initial
$f$-and-$m$-algebra approach in the previous section. Thus we have
abstracted out parts that are common to all definitions and proofs
that involve interleaved data and effects.

\begin{theorem}\label{thm:make-initial-f-and-m-alg}
  Let $(f, \mathit{fmap}_f)$ be a functor, and $(m, \mathit{fmap}_m,
  \mathit{return}_m, \mathit{join}_m)$ be a monad.  If we have an
  initial $(f \circ m)$-algebra $(\mu(f \circ m),
  \mathit{in})$, then $m~(\mu(f \circ m))$ is the carrier of an
  initial $f$-and-$m$-algebra.
\end{theorem}

\begin{proof*}
  See \appendixref{sec:make-initial-fm-proof}.
\end{proof*}
%\pattynote{Redirects to intro rather than appendix. :-( }

In Atkey, Ghani, Jacobs and Johann's work \cite{atkey12fibrational},
this same result was obtained in a less elementary way by constructing
a functor $\Phi$ from the category of $(f \circ m)$-algebras to the
category of $f$-and-$m$-algebras. The functor $\Phi$ was shown to be a
left adjoint, and since left adjoints preserve initial objects, $\Phi$
maps any initial $(f \circ m)$-algebra to an initial
$f$-and-$m$-algebra.

% FIXME: mention Filinski and Stvring's construction?

\subsection{Implementation of initial \texorpdfstring{$f$}{f}-and-\texorpdfstring{$m$}{m}-algebras in Haskell}
\label{sec:f-and-m-alg-impl}

\newcommand{\fcompose}{\mathop{\mathord:\circ\mathord:}}

In light of \thmref{thm:make-initial-f-and-m-alg}, we can take the
Haskell implementation of initial $f$-algebras from
\autoref{sec:f-algebras} and apply the construction in the theorem to
construct an initial $f$-and-$m$-algebra.

The seed of our construction is the existence of an initial $(f \circ
m)$-algebra. Therefore, we need to first construct the composite
functor $f \circ m$. To express the composition of two type operators
as a new type operator, we introduce a $\kw{newtype}$, as
follows\footnote{This definition requires the GHC extension
  \texttt{-XTypeOperators}, allowing infix type constructors.}:
\begin{displaymath}
  \kw{newtype}~(f \fcompose g)~a = \mathsf{C}~\{\mathit{unC} :: f~(g~a) \}
\end{displaymath}
We define $\mathit{fmap}_{f\fcompose g}$ straightforwardly in terms of
$\mathit{fmap}_f$ and $\mathit{fmap}_g$:
\begin{displaymath}
  \mathit{fmap}_{f\fcompose g}~h~(\mathsf{C}~x) = \mathsf{C}~(\mathit{fmap}_f~(\mathit{fmap}_g~h)~x)
\end{displaymath}

\thmref{thm:make-initial-f-and-m-alg} states that if $\mu(f \circ m)$
is the carrier of an initial $(f \circ m)$-algebra, then $m~(\mu(f
\circ m))$ is the carrier of an initial
$f$-and-$m$-algebra. Therefore, we can define an implementation of an
initial $f$-and-$m$-algebra by setting $\mu (f|m)$ to be the type
$\mathit{MuFM}~f~m$, defined as:
\begin{displaymath}
  \kw{type}~\mathit{MuFM}~f~m = m~(\mathit{Mu}~(f \fcompose m))
\end{displaymath}
Unfolding the definitions of $f \fcompose m$ and $\mathit{Mu}$ shows that
the type $\mathit{MuFM}~f~m$ is, up to isomorphism, the same as the
type $\mathit{MuFM_0}~f~m$ from \autoref{sec:motivate-interleaving}
that we arrived at by generalising the $\mathit{List_{io}}$ and
$\mathit{List_{lazy}}$ examples.

The $f$-algebra and $m$-Eilenberg-Moore-algebra structure maps
$\mathit{in}_f$ and $\mathit{in}_m$ are defined
following the construction in \thmref{thm:make-initial-f-and-m-alg}:
\begin{displaymath}
  \begin{array}{l}
    \mathit{in}_f :: f~(\mathit{MuFM}~f~m) \to \mathit{MuFM}~f~m \\
    \mathit{in}_f = \mathit{return}_m \circ \mathit{in} \circ \mathsf{C} \\
    \\
    \mathit{in}_m :: m~(\mathit{MuFM}~f~m) \to \mathit{MuFM}~f~m \\
    \mathit{in}_m = \mathit{join}_m
  \end{array}
\end{displaymath}

Finally, we construct the unique $f$-and-$m$-homomorphism out of
$\mathit{MuFM}~f~m$ following the proof of
\thmref{thm:make-initial-f-and-m-alg} by building upon our
implementation of the unique homomorphisms out of the initial $(f
\fcompose m)$-algebra:
\begin{displaymath}
  \begin{array}{l}
    \eFold{-}{-} :: (f~a \to a) \to (m~a \to a) \to \mathit{MuFM}~f~m \to a \\
    \eFold{k}{l} = l \circ \mathit{fmap}_m~\fold{k \circ \mathit{fmap}_f~l \circ \mathit{unC}}
  \end{array}
\end{displaymath}
We can also implement $\eFold{-}{-}$ directly in terms of Haskell's
general recursion, just as we did for the implementation of
$\fold{-}$. This definition arises by inlining the implementation of
$\fold{-}$ into the definition of $\eFold{-}{-}$ above, and performing
some straightforward rewriting. The direct implementation of
$\eFold{-}{-}$ is as follows:
\begin{displaymath}
  \begin{array}{l}
    \eFold{-}{-} :: (f~a \to a) \to (m~a \to a) \to \mathit{MuFM}~f~m \to a \\
    \eFold{k}{l} = l \circ \mathit{fmap}_m~\mathit{loop} \\
    \quad\begin{array}{r@{\hspace{0.4em}}l@{\hspace{0.5em}}c@{\hspace{0.5em}}l}
      \kw{where} & \mathit{loop} &=& k \circ \mathit{fmap}_f~l \circ \mathit{fmap}_f~(\mathit{fmap}_m~\mathit{loop}) \circ \mathit{unC} \circ \mathit{unIn}
    \end{array}
  \end{array}
\end{displaymath}

Whichever implementation of $\eFold{-}{-}$ we choose, we note that
there is an implicit precondition that the second argument (of type
$m~a \to a$) must be an Eilenberg-Moore algebra. Unfortunately, it is
not possible to express this requirement in Haskell's type system.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Application: Streaming I/O and coproducts of free monads with arbitrary monads}
\label{sec:coproducts-with-free-monads}

In \autoref{sec:motivate-interleaving}, we motivated the consideration
of streams of interleaved data and effects by giving the
$\mathit{hGetContents}$ function a type that more precisely reflects
its actual behaviour. The Haskell community, motivated by the concerns
about lazy I/O that we listed in \autoref{sec:motivate-interleaving},
has proposed many other datatypes that capture the interleaving of
effects with pure data\footnote{For example, the \texttt{iteratees},
  \texttt{iterIO}, \texttt{conduits}, \texttt{enumerators}, and
  \texttt{pipes} Haskell libraries all make use of interleaved data
  and effects. These libraries are all available from the Hackage
  archive of Haskell libraries
  (\url{http://hackage.haskell.org/packages/hackage.html}).}, in order
to make the interleaving explicit. One of the earliest was Kiselyov's
iteratees \cite{kiselyov12iteratees}.  Iteratees are used to support
lazy I/O in languages such as Haskell, Scala, and F\# by handling
different kinds of sequential information processing in an incremental
way.

Iteratees are descriptions of functions that alternate reading from
some input with effects in some monad, eventually yielding some
output. Kiselyov captured this using the following datatype, which
follows the same pattern of mutual recursion as the
$\mathit{List}~m~a$ datatype declaration:
\begin{displaymath}
  \begin{array}{ll}
    \kw{data}~\mathit{Reader'}~m~a~b
    &
    \kw{newtype}~\mathit{Reader}~m~a~b = 
    \\
    \quad
    \begin{array}[t]{c@{\hspace{0.5em}}l}
      = & \mathsf{Input}~(\mathit{Maybe}~a \to \mathit{Reader}~m~a~b) \\
      | & \mathsf{Yield}~b
    \end{array}
    &
    \quad \mathsf{Reader}~(m~(\mathit{Reader'}~m~a~b))
  \end{array}
\end{displaymath}
A value of type $\mathit{Reader}~m~a~b$ is some effect described by
the monad $m$, yielding either a result of type $b$, or a request for
input. As Kiselyov demonstrates, the fact that values of type
$\mathit{Reader}~m~a~b$ abstract the source of the data that they read
is extremely powerful: different constructions allow values of type
$\mathit{Reader}~m~a~b$ to be chained together, or connected to actual
input/output devices, all while retaining the ability to perform
concrete effects in the monad $m$.

Kiselyov treats the $\mathit{Reader}~m~a~b$ type in isolation, and
notes that it has several useful properties, including the fact that
it is (the functor part of) a monad. In terms of $f$-and-$m$-algebras
we can see that the type $\mathit{Reader}~m~a~b$ is an instance of an
initial $f$-and-$m$-algebra, where the functor $f$ is given by:
\begin{displaymath}
  \begin{array}{l}
    \kw{data}~\mathit{ReaderF}~a~b~x = \\
    \begin{array}[t]{c@{\hspace{0.5em}}l}
      = & \mathsf{Input}~(\mathit{Maybe}~a \to x) \\
      | & \mathsf{Yield}~b
    \end{array}
  \end{array}
\end{displaymath}
With this formulation, we could use \proofprinref{pp:initial-f-m-alg}
to reason about programs involving iteratees. For example, we could
prove that $\mathit{Reader}~m~a~b$ is a monad whenever $m$
is. However, we can see iteratees as an instance of a yet more general
construction: the coproduct of a free monad with an arbitrary
  monad. Monad coproducts provide a general and canonical way of
specifying the combination of two monads
\cite{luth02composing}. Almost trivially, once we observe that
$\mathit{Reader}~m~a~b$ is the coproduct of two monads, we can
immediately deduce that it is a monad, rather than having to prove
this fact as a special case for this particular type. As we will see
below in \autoref{sec:construct-coproducts}, the coproduct of a free
monad and an arbitrary monad can be straightforwardly constructed
using initial $f$-and-$m$-algebras.

In the next subsection, we present the formal definition of the notion
of a free monad, and briefly describe the reading of free monads as
abstract interaction trees that can be interpreted in multiple
ways. In \autoref{sec:construct-free-monads}, we show that concrete
free monads can be defined using specific initial $f$-algebras. We
will be able to reuse most of this construction when constructing the
coproduct in \autoref{sec:construct-coproducts}. We present the formal
definition of monad coproduct in \autoref{sec:coproducts-of-monads},
and elaborate on the reading of free monads as interaction trees, now
interleaved with effects from some arbitrary monad. In
\autoref{sec:construct-coproducts}, we present a concrete construction
of the coproduct of a free monad with an arbitrary monad. By using
$f$-and-$m$-algebras we are able to reuse much of the core of the
definitions of the free monad structure we defined in
\autoref{sec:construct-free-monads}.

% Finally, in
% \autoref{sec:haskell-application}, we briefly examine the application
% of our results to Haskell libraries that use interleaved data and
% effects.

% reconstruct a result originally due to Hyland, Plotkin and
% Power \cite{hyland06combining} on the construction of coproducts of
% free monads with arbitrary monads. This example highlights the
% advantages that $f$-and-$m$-algebras' separation of pure data and
% effects provides, and also has theoretical and practical interest. We
% now give a brief explanation of the relevant concepts.


% Coproducts of free monads with arbitrary monads arise when considering
% certain kinds of datatypes interleaved with effects, such as the
% Iteratee type $\mathit{Reader}~m~a~b$ in
% \autoref{sec:motivate-interleaving}. This type describes computations
% that interleave the possibility of reading with performing effects in
% some arbitrary monad (often the $\mathit{IO}$ monad). The power of the
% $\mathit{Reader}~m~a~b$ type, as demonstrated by Kiselyov
% \cite{kiselyov12iteratees}, is that we are provided with considerable
% flexibility in how to interpret ``reading'', allowing for instances of
% the type $\mathit{Reader}~m~a~b$ to be chained together in interesting
% ways. The monad coproduct interface provides us with a simple,
% general, and canonical way of precisely describing exactly how
% $\mathit{Reader}~m~a~b$ is the combination of read effects with
% effects in $m$.


\subsection{Free monads}
\label{sec:free-monads}

A \emph{free monad} for a functor $(f,\, \mathit{fmap}_f)$ is a way of
extending $f$ to be a monad while, intuitively, adding no additional
constraints. A useful application of free monads is as a way of
describing effectful computations over a set of commands, where the
commands are described by the functor $f$, and no commitment is made
as to their interpretation. Swierstra and Altenkirch
\cite{swierstra07beauty} have developed this idea to provide a
straightforward way of reasoning about programs that perform
input/output. We will briefly describe this view of free monads after
we give the formal definition:

\begin{definition}\label{defn:freemonad}
  Let $(f, \mathit{fmap}_f)$ be a functor. A \emph{free monad} on
  $(f,\mathit{fmap}_f)$ is a monad
  \begin{displaymath}
    (\mathit{FreeM}~f, \mathit{fmap}_{\mathit{FreeM}~f}, \mathit{return}_{\mathit{FreeM}~f}, \mathit{join}_{\mathit{FreeM}~f})
  \end{displaymath}
  equipped with a function:
  \begin{displaymath}
    \mathit{wrap}_f :: f~(\mathit{FreeM}~f~a) \to \mathit{FreeM}~f~a
  \end{displaymath}
  that satisfies:
  \begin{align}
    \label{eq:wrap-natural}
    \mathit{wrap}_f \circ \mathit{fmap}_f~(\mathit{fmap}_{\mathit{FreeM}~f}~g) &= \mathit{fmap}_{\mathit{FreeM}~f}~g \circ \mathit{wrap}_f \\
    \label{eq:wrap-join}
    \mathit{wrap}_f \circ \mathit{fmap}_f~\mathit{join}_{\mathit{FreeM}~f} &= \mathit{join}_{\mathit{FreeM}~f} \circ \mathit{wrap}_f
  \end{align}
  and such that for every monad $(m, \mathit{fmap}_m,
  \mathit{return_m}, \mathit{join}_m)$ and $g :: f~a \to m~a$, such
  that $g$ is natural:
  \begin{displaymath}
    g \circ \mathit{fmap}_f~k = \mathit{fmap}_m~k \circ g
  \end{displaymath}
  there is a unique monad morphism $\fmext{g} :: \mathit{FreeM}~f~a
  \to m~a$ such that:
  \begin{displaymath}
    \mathit{join}_m \circ \mathit{fmap}_m~\fmext{g} \circ g = \fmext{g} \circ \mathit{wrap}_f
  \end{displaymath}
\end{definition}

An alternative but equivalent definition of free monad, which is
slightly more standard from a categorical point of view, has the type
of $\mathit{wrap}_f$ as $f~a \to \mathit{FreeM}~f~a$. We choose the
form in \defref{defn:freemonad} because it is more convenient for
programming.

The following lemma is an immediate consequence of the definition of
free monad, and can be taken as another alternative definition in
terms of isomorphisms of collections of morphisms. It will be useful
when we come to define the coproduct of free monads with arbitrary
monads in terms of $f$-and-$m$-algebras in
\autoref{sec:construct-coproducts} below.

\begin{lemma}
  If $(\mathit{FreeM}~f, \mathit{fmap}_{\mathit{FreeM}~f},
  \mathit{return}_{\mathit{FreeM}~f},
  \mathit{join}_{\mathit{FreeM}~f})$ is a free monad for a functor
  $(f, \,\mathit{fmap}_f)$, then the operation $\fmext{-} :: (\forall
  a.~f~a \to m~a) \to (\forall a.~\mathit{FreeM}~f~a \to m~a)$ is a
  bijection between natural transformations and monad morphisms. The
  inverse operation can be defined as follows:
  \begin{displaymath}
    \begin{array}{l}
      \fmext{-}^{-1} :: (\forall a.~\mathit{FreeM}~f~a \to m~a) \to (\forall a.~f~a \to m~a) \\
      \fmext{h}^{-1} = h \circ \mathit{wrap}_f \circ \mathit{fmap}_f~\mathit{return}_{\mathit{FreeM}~f}
    \end{array}
  \end{displaymath}
\end{lemma}

One way of explaining the free monad abstraction is in terms of
expressions with variables, and substitution. Under this reading, the
functor $(f,\mathit{fmap}_f)$ describes the constructors that can be
used to make expressions, and a value of type $\mathit{FreeM}~f~a$ is
an expression comprised of the constructors from $f$ and variables
from $a$. The $\mathit{join}_{\mathit{FreeM}~f}$ part of the monad
structure provides substitution of expressions into other expressions,
and the extension $\fmext{g}$ allows us to interpret a whole
expression if we can interpret all the constructors.

Another reading, which is more in line with our general theme of
computational effects, is in terms of ``interaction trees''. We think
of the functor $(f, \mathit{fmap}_f)$ as describing a collection of
possible commands that can be issued by a program. For example, the
functor $(\mathit{ReaderF}~a, \mathit{fmap}_{\mathit{ReaderF}~a})$,
that we define now, describes a single command of reading a value from
some input. The $\mathit{ReaderF}~a$ functor is defined as follows:
\begin{displaymath}
  \begin{array}{@{}l@{\hspace{2em}}l}
    \begin{array}{@{}l}
      \kw{data}~\mathit{ReaderF}~a~x \\
      \quad
      \begin{array}{c@{\hspace{0.3em}}l}
        = & \mathsf{Read}~(a \to x)
      \end{array}
    \end{array}
    &
    \begin{array}{@{}l}
      \mathit{fmap}_{\mathit{ReaderF}~a} :: (x \to y) \to \mathit{ReaderF}~a~x \to \mathit{ReaderF}~a~y \\
      \mathit{fmap}_{\mathit{ReaderF}~a}~g~(\mathsf{Read}~k) = \mathsf{Read}~(g \circ k)
    \end{array}
  \end{array}
\end{displaymath}
We think of values of type $\mathit{FreeM}~(\mathit{ReaderF}~a)~b$ as
trees of read commands, eventually yielding a value of type
$b$. We use the $\mathit{wrap}_{\mathit{ReaderF}~a}$ part of the free
monad interface to define a primitive read operation:
\begin{displaymath}
  \begin{array}{@{}l}
    \mathit{read} :: \mathit{FreeM}~(\mathit{ReaderF}~a)~a \\
    \mathit{read} = \mathit{wrap}_{\mathit{ReaderF}~a}~(\mathsf{Read}~\mathit{return}_{\mathit{FreeM}~(\mathit{ReaderF}~a)})
  \end{array}
\end{displaymath}
Every free monad is a monad, so we can use Haskell's $\kw{do}$
notation to sequence individual commands. For example, here is a
simple program that reads two strings from some input, and returns
them as a pair in the opposite order.
\begin{displaymath}
  \begin{array}{@{}l}
  \mathit{swapRead} :: \mathit{FreeM}~(\mathit{ReaderF}~\mathit{String})~(\mathit{String},\mathit{String}) \\
  \mathit{swapRead} = \kw{do}~\{ s_1 \leftarrow \mathit{read}; s_2 \leftarrow \mathit{read}; \mathit{return}~(s_2,s_1) \}
\end{array}
\end{displaymath}

The free monad interface gives us considerable flexibility in how we
actually interpret the $\mathit{read}$ commands. For example, we can
interpret each $\mathit{read}$ command as reading a line from the
terminal by defining a transformation from
$\mathit{ReaderF}~\mathit{String}$ to $\mathit{IO}$, using the
standard Haskell function $\mathit{getLine}$ to do the actual reading:
\begin{displaymath}
  \begin{array}{@{}l}
    \mathit{useGetLine} :: \mathit{ReaderF}~\mathit{String}~a \to \mathit{IO}~a \\
    \mathit{useGetLine}~(\mathsf{Read}~k) = \kw{do}~\{ s \leftarrow \mathit{getLine}; \mathit{return}~(k~s) \}
  \end{array}
\end{displaymath}
The free monad interface now provides a way to extend this
interpretation of individual commands to trees of commmands:
\begin{displaymath}
  \fmext{\mathit{useGetLine}} :: \mathit{FreeM}~(\mathit{ReaderF}~\mathit{String})~a \to \mathit{IO}~a
\end{displaymath}
Applying $\fmext{\mathit{useGetLine}}$ to $\mathit{swapRead}$ results
in the following interaction, where the second and third lines are
entered by the user, and the final line is printed by the Haskell
implementation:
\begin{displaymath}
  \begin{array}{@{}l}
    >~\fmext{\mathit{useGetLine}}~\mathit{swapRead} \\
    \texttt{"free"} \\
    \texttt{"monad"} \\
    (\texttt{"monad"}, \texttt{"free"})
  \end{array}
\end{displaymath}

The free monad interface provides us with a powerful way of giving
multiple interpretations to effectful commands. Moreover, it is easy
to extend the language of commands simply by extending the functor
$f$. Swierstra \cite{swierstra08data} demonstrates a convenient method
in Haskell for dealing with modular construction of functors for
describing commands in free monads. However, explicitly naming every
additional command that we wish to be able to perform can be
tedious. Sometimes, we simply want access to effects in a known monad
$m$. For example, we may know that we want to execute concrete
$\mathit{IO}$ actions as well as abstract read operations. One
possible way of accomplishing this is to ensure that there is an
additional constructor to the functor $f$ that describes an additional
``abstract command'' of performing an effect in the chosen monad. For
example, we could extend the $\mathit{ReaderF}~a$ functor like so to
add the possibility of effects in a monad $m$:
\begin{displaymath}
  \kw{data}~\mathit{ReaderMF}~m~a~x = \mathsf{Read}~(a \to x) \mathrel| \mathsf{Act}~(m~x)
\end{displaymath}
This approach has the disadvantage that the effects of the monad $m$
must now be handled by the interpretation of the other abstract
commands. For example, we would have to add another case to the
$\mathit{useGetLine}$ function to handle the $\mathsf{Act}$
case. Thus, we would be forced to combine the interpretation of the
pure data representing abstract commands with the interpretation of
concrete effects. As we have observed in the case of list append in
\autoref{sec:direct-eappend}, the mingling of such concerns can lead
to unnecessarily complicated reasoning. Fortunately, a conceptually
simpler solution is available: we take the coproduct of the free monad
for the functor $f$ that describes our abstract effects with the monad
$m$ that describes our concrete effects. We define the coproduct of
two monads in \autoref{sec:coproducts-of-monads} below, and
demonstrate how monad coproducts cleanly combine abstract effects with
concrete effects. Before that, in the next section, we demonstrate how
to construct free monads from initial $f$-algebras.

\subsection{Constructing free monads, via \texorpdfstring{$f$}{f}-algebras}
\label{sec:construct-free-monads}

\begin{figure}
  Let $(f,\mathit{fmap}_f)$ be a functor, and define:
  \begin{displaymath}
    \begin{array}{l}
      \kw{data}~\mathit{FreeMF}~f~a~x \\
      \quad\begin{array}{cl}
        = & \mathsf{Var}~a \\
        | & \mathsf{Term}~(f~x)
      \end{array} \\
      \\
      \mathit{fmap}_{\mathit{FreeMF}} :: (x \to y) \to \mathit{FreeMF}~f~a~x \to \mathit{FreeMF}~f~a~y \\
      \begin{array}{@{}l@{\hspace{0.3em}}l@{\hspace{0.3em}}l@{\hspace{0.3em}}c@{\hspace{0.3em}}l}
        \mathit{fmap}_{\mathit{FreeMF}} & g & (\mathsf{Var}~a) & = & \mathsf{Var}~a \\
        \mathit{fmap}_{\mathit{FreeMF}} & g & (\mathsf{Term}~\mathit{fx}) & = & \mathsf{Term}~(\mathit{fmap}_f~g~\mathit{fx})
      \end{array}
    \end{array}
  \end{displaymath}

  \bigskip

  Free monads:
  \begin{displaymath}
    \begin{array}{@{}l}
      \kw{type}~\mathit{FreeM}~f~a = \mu(\mathit{FreeMF~f~a}) \\
      \\
      \begin{array}{@{}l}
        \mathit{fmap}_{\mathit{FreeM}~f} :: (a \to b) \to \mathit{FreeM}~f~a \to \mathit{FreeM}~f~b \\
        \mathit{fmap}_{\mathit{FreeM}~f}~g = \fold{k} \\
        \quad\begin{array}{r@{\hspace{0.4em}}l@{\hspace{0.5em}}c@{\hspace{0.5em}}l}
          \kw{where} & k~(\mathsf{Var}~a) &=& \mathit{in}~(\mathsf{Var}~(g~a)) \\
          & k~(\mathsf{Term}~x) &=& \mathit{in}~(\mathsf{Term}~x)
        \end{array}
      \end{array} \\
      \\
      \begin{array}{@{}l}
        \mathit{return}_{\mathit{FreeM}~f} :: a \to \mathit{FreeM}~f~a \\
        \mathit{return}_{\mathit{FreeM}~f}~a = \mathit{in}~(\mathsf{Var}~a)
      \end{array} \\
      \\
      \begin{array}{@{}l}
        \mathit{join}_{\mathit{FreeM}~f} :: \mathit{FreeM}~f~(\mathit{FreeM}~f~a) \to \mathit{FreeM}~f~b \\
        \mathit{join}_{\mathit{FreeM}~f} = \fold{j} \\
        \quad\begin{array}{r@{\hspace{0.4em}}l@{\hspace{0.5em}}c@{\hspace{0.5em}}l}
          \kw{where} & j~(\mathsf{Var}~x) &=& x \\
          & j~(\mathsf{Term}~x) &=& \mathit{in}~(\mathsf{Term}~x)
        \end{array}
      \end{array} \\
      \\
      \begin{array}{@{}l}
        \mathit{wrap}_f :: f~(\mathit{FreeM}~f~a) \to \mathit{FreeM}~f~a \\
        \mathit{wrap}_f~x = \mathit{in}~(\mathsf{Term}~x)
      \end{array} \\
      \\
      \begin{array}{@{}l}
        \fmext{-} :: (f~a \to m~a) \to \mathit{FreeM}~f~a \to m~a \\
        \fmext{g} = \fold{e} \\
        \quad\begin{array}{r@{\hspace{0.4em}}l@{\hspace{0.5em}}c@{\hspace{0.5em}}l}
          \kw{where} & e~(\mathsf{Var}~a) &=& \mathit{return}_m~a \\
          & e~(\mathsf{Term}~x) &=& \mathit{join}_m~(g~x)
        \end{array}
      \end{array}
    \end{array}
  \end{displaymath}
  
  \caption{Constructing free monads via $f$-algebras}
\label{fig:construct-free-monads}
\end{figure}

\autoref{fig:construct-free-monads} demonstrates how the free monad
interface we defined in the previous section may be implemented in
terms of initial $(\mathit{FreeMF}~f~a)$-algebras, where the functor
$\mathit{FreeMF}~f~a$ is also defined in
\autoref{fig:construct-free-monads}. The key idea is that a value of
type $\mathit{FreeM}~f~a$ is constructed from layers of ``terms''
described by the functor $f$, represented by $\mathsf{Term}$
constructor, and terminated by ``variables'', represented by the
$\mathsf{Var}$ constructor.

The definition of the free monad structure is relatively
straightforward, using the functions induced by the initial algebra
property of $\mu (\mathit{FreeMF}~f~a)$. Each of the properties
required of free monads is proved by making use of
\proofprinref{pp:initial-alg}. When we construct the coproduct of a
free monad with an arbitrary monad in
\autoref{sec:construct-coproducts} we will be able to reuse many of
the definitions in \autoref{fig:construct-free-monads}.

\subsection{Coproducts of monads}
\label{sec:coproducts-of-monads}

\newcommand{\cprd}[2]{#1\mathord{+}#2}

\emph{Monad coproducts} provide a canonical way of describing the
combination of two monads to form another monad. We can think of the
coproduct of two monads as the ``least commitment'' combination. The
coproduct of two monads is able to describe any effects that its
constituents describes, but imposes no interaction between them. The
coproduct of two arbitrary monads is not always guaranteed to exist,
but is known to exist in certain special cases. For example, monad
coproducts are guaranteed to exist when the monads in question are
ideal monads \cite{ghani04coproducts}, or when working in the category
of Sets \cite{adamek12coproducts}, or if the monads are constructed
from algebraic theories \cite{hyland06combining}. One particular
special case is when one of the constituent monads is \emph{free}, as
we shall see in \autoref{sec:construct-coproducts}, below.

Formally, ``least commitment'' is realised as the existence of a
\emph{unique} monad morphism out of a coproduct for every way of
interpreting its constituent parts. Coproducts of monads are precisely
coproducts in the category of monads and monad morphisms. The
following definition sets out the precise conditions:
\begin{definition}\label{defn:coproducts}
  Let $(m_1, \mathit{fmap}_{m_1}, \mathit{return}_{m_1}, \mathit{join}_{m_1})$ and $(m_2,
  \mathit{fmap}_{m_1}, \mathit{return}_{m_2}, \mathit{join}_{m_2})$ be a pair of monads. A \emph{coproduct} of these two monads is a
  monad $(\cprd{m_1}{m_2}, \mathit{fmap}_{\cprd{m_1}{m_2}}, \mathit{return}_{\cprd{m_1}{m_2}},
  \mathit{join}_{\cprd{m_1}{m_2}})$ along with a pair of monad morphisms:
  \begin{displaymath}
    \begin{array}{rcl}
      \mathit{inj}_1 & :: & m_1~a \to (\cprd{m_1}{m_2})~a \\
      \mathit{inj}_2 & :: & m_1~a \to (\cprd{m_1}{m_2})~a
    \end{array}
  \end{displaymath}
  and the property that for any monad $(m,\mathit{fmap}_m,
  \mathit{return}_m, \mathit{join}_m)$ and pair of monad morphisms
  $g_1 : m_1~a \to m~a$ and $g_2 : m_2~a \to m~a$ there is a
  \emph{unique} monad morphism $[g_1,g_2] : (\cprd{m_1}{m_2})~a \to
  m~a$ such that
  \begin{displaymath}
    \begin{array}{rcl}
      {}[g_1,g_2] \circ \mathit{inj}_1 & = & g_1 \\
      {}[g_1,g_2] \circ \mathit{inj}_2 & = & g_2
    \end{array}
  \end{displaymath}
\end{definition}

In \autoref{sec:free-monads}, we demonstrated how the free monad over
a functor describing read commands allowed us to provide multiple
interpretations of ``reading''. 
% We now show how we can use the
% coproduct interface to describe computations that interleave abstract
% read operations with concrete $\mathit{IO}$ effects. 
The monad coproduct
$\cprd{(\mathit{FreeM}~(\mathit{ReaderF}~\mathit{String}))}{\mathit{IO}}$
freely combines the abstract read commands described by the functor
$\mathit{ReaderF}~\mathit{String}$ with the concrete input/output
actions of the $\mathit{IO}$ monad. We view
$\cprd{(\mathit{FreeM}~(\mathit{ReaderF}~\mathit{String}))}{\mathit{IO}}$
as the modular reconstruction of the Iteratee monad
$\mathit{Reader}~m~a$ we presented in
\autoref{sec:motivate-interleaving}.

The following example extends the $\mathit{swapRead}$ example from
\autoref{sec:free-monads} to perform an input/output effect as well as
two abstract read effects.  The $\mathit{inj_1}$ and $\mathit{inj_2}$
components of the coproduct monad interface allow us to lift effectful
computations from the free monad and the $\mathit{IO}$ monad respectively:
\begin{displaymath}
  \begin{array}{@{}l}
    \mathit{swapRead2} :: (\cprd{(\mathit{FreeM}~(\mathit{ReaderF}~\mathit{String}))}{\mathit{IO}})~() \\
    \mathit{swapRead2} = \kw{do}~
    \begin{array}[t]{@{}l}
      s_1 \leftarrow \mathit{inj_1}~\mathit{read} \\
      s_2 \leftarrow \mathit{inj_1}~\mathit{read} \\
      \mathit{inj_2}~(\mathit{putStrLn}~(\texttt{"("} \dplus s_2 \dplus \texttt{","} \dplus s_1 \dplus \texttt{")"}))
    \end{array}
  \end{array}
\end{displaymath}
This program executes two abstract read commands to read a pair of
strings, and then executes a concrete $\mathit{IO}$ action to print
the two strings in reverse order to the terminal.

%FIXME: like Scheme's with-input-from-file

We can provide an interpretation for the abstract $\mathit{read}$
operations by combining the coproduct interface with the free monad
interface. For example, to interpret the read commands as reading from
the terminal, we use the $\mathit{useGetLine}$ interpretation from
\autoref{sec:free-monads}:
\begin{displaymath}
  [\fmext{\mathit{useGetLine}}, \mathit{id}] :: (\cprd{(\mathit{FreeM}~(\mathit{ReaderF}~\mathit{String}))}{\mathit{IO}})~a \to \mathit{IO}~a
\end{displaymath}

Alternatively, we can interpret the abstract $\mathit{read}$ commands
as reading from a file handle. The function $\mathit{useFileHandle}$
describes how to execute single reads on a file handle as an $\mathit{IO}$ action:
\begin{displaymath}
  \begin{array}{@{}l}
    \mathit{useFileHandle} :: \mathit{Handle} \to \mathit{ReaderF}~\mathit{String}~a \to \mathit{IO}~a \\
    \mathit{useFileHandle}~h~(\mathsf{Read}~k) = \kw{do}~\{ s \leftarrow \mathit{hGetLine}~h; \mathit{return}~(k~s) \}
  \end{array}
\end{displaymath}
Again, we can combine the free monad and monad coproduct interfaces to
extend this interpretation of individual abstract $\mathit{read}$
commands to all trees of $\mathit{read}$ commands interleaved with
arbitrary $\mathit{IO}$ actions:
\begin{displaymath}
  \lambda h.~[\fmext{\mathit{useFileHandle}~h}, \mathit{id}] :: \mathit{Handle} \to (\cprd{(\mathit{FreeM}~(\mathit{ReaderF}~\mathit{String}))}{\mathit{IO}})~a \to \mathit{IO}~a
\end{displaymath}

\subsection{Constructing coproducts with free monads via \texorpdfstring{$f$}{f}-and-\texorpdfstring{$m$}{m}-algebras}
\label{sec:construct-coproducts}

\begin{figure}
  \centering
  \begin{displaymath}
    \begin{array}{@{}l}
      \kw{type}~(\cprd{(\mathit{FreeM}~f)}{m})~a = \mu(\mathit{FreeMF}~f~a|m) \\
      \\
      \begin{array}{@{}l}
        \mathit{fmap}_{\cprd{(\mathit{FreeM}~f)}{m}} :: (a \to b) \to (\cprd{(\mathit{FreeM}~f)}{m})~a \to (\cprd{(\mathit{FreeM}~f)}{m})~b \\
        \mathit{fmap}_{\cprd{(\mathit{FreeM}~f)}{m}}~g = \eFold{k}{\mathit{in}_m} \\
        \quad\begin{array}{r@{\hspace{0.4em}}l@{\hspace{0.5em}}c@{\hspace{0.5em}}l}
          \kw{where} & k~(\mathsf{Var}~a) &=& \mathit{in}_{\mathit{FreeMF}~f~b}~(\mathsf{Var}~(g~a)) \\
          & k~(\mathsf{Term}~x) &=& \mathit{in}_{\mathit{FreeMF}~f~b}~(\mathsf{Term}~x)
        \end{array}
      \end{array} \\
      \\
      \begin{array}{@{}l}
        \mathit{return}_{\cprd{(\mathit{FreeM}~f)}{m}} :: a \to (\cprd{(\mathit{FreeM}~f)}{m})~a \\
        \mathit{return}_{\cprd{(\mathit{FreeM}~f)}{m}}~a = \mathit{in}_{\mathit{FreeMF}~f~a}~(\mathsf{Var}~a)
      \end{array} \\
      \\
      \begin{array}{@{}l}
        \mathit{join}_{\cprd{(\mathit{FreeM}~f)}{m}} :: (\cprd{(\mathit{FreeM}~f)}{m})~((\cprd{(\mathit{FreeM}~f)}{m})~a) \to (\cprd{(\mathit{FreeM}~f)}{m})~a \\
        \mathit{join}_{\cprd{(\mathit{FreeM}~f)}{m}} = \eFold{j}{\mathit{in}_m} \\
        \quad\begin{array}{r@{\hspace{0.4em}}l@{\hspace{0.5em}}c@{\hspace{0.5em}}l}
          \kw{where} & j~(\mathsf{Var}~x) &=& x \\
          & j~(\mathsf{Term}~x) &=& \mathit{in}~(\mathsf{Term}~x)
        \end{array}
      \end{array} \\
      \\
      \mathit{inj_1} :: \mathit{FreeM}~f~a \to (\cprd{(\mathit{FreeM}~f)}{m})~a \\
      \mathit{inj_1} = \fmext{\mathit{in}_{\mathit{FreeMF}~f~a} \circ \mathsf{Term}} \\
      \\
      \mathit{inj_2} :: m~a \to (\cprd{(\mathit{FreeM}~f)}{m})~a \\
      \mathit{inj_2} = \mathit{in}_m \circ \mathit{fmap}_m~\mathit{return}_{\cprd{(\mathit{FreeM}~f)}{m}}\\
      \\
      \begin{array}{@{}l}
        {}[-,-] :: (\forall a.~\mathit{FreeM}~f~a \to m'~a) \to (\forall a.~m~a \to m'~a) \to (\cprd{(\mathit{FreeM}~f)}{m})~a \to m'~a \\
        {}[g_1,g_2] = \eFold{c}{\mathit{join}_{m'} \circ g_2} \\
        \quad\begin{array}{r@{\hspace{0.4em}}l@{\hspace{0.5em}}c@{\hspace{0.5em}}l}
          \kw{where} & c~(\mathsf{Var}~a) &=& \mathit{return}_{m'}~a \\
          & c~(\mathsf{Term}~x) &=& \mathit{join}_{m'}~(\fmext{g_1}^{-1}~x)
        \end{array}
      \end{array}
    \end{array}
  \end{displaymath}
  \caption{Construction of coproducts with free monads via $f$-and-$m$-algebras}
  \label{fig:construct-coproduct}
\end{figure}

\autoref{fig:construct-coproduct} demonstrates the construction of the
coproduct of a free monad with an arbitrary monad $m$ in terms of
initial $f$-and-$m$-algebras. We program against the abstract
interface of initial $f$-and-$m$-algebras, rather relying on any
particular implementation.

% If we take the implementation that we
% presented in \autoref{sec:f-and-m-alg-impl}, then the definition of
% $(\cprd{(\mathit{FreeM}~f)}{m})~a$ unfolds to be
% $m~(\mathit{Mu}~(\mathit{FreeMF}~f~a \fcompose m))$, which is exactly
% the Haskell translation of the construction that Hyland \emph{et al.}
% present.

The definitions of the basic monad structure -- $\mathit{fmap}$,
$\mathit{return}$ and $\mathit{join}$ -- are almost identical to the
corresponding definitions for the free monad in
\autoref{fig:construct-free-monads}. This demonstrates the same
feature of the use of $f$-and-$m$-algebras that we saw when defining
the effectful list append in \autoref{sec:f-and-m-append}: the clean
separation of pure and effectful concerns allows us to reuse much of
the work we performed in the non-effectful case. The proofs that these
definitions actually form a monad carry over just as they did for the
list append example.

For the monad coproduct structure, we use the pure and effectful parts
of the initial $(\mathit{FreeMF}~f~a)$-and-$m$-algebra stucture --
$\mathit{in}_{\mathit{FreeMF}~f~a}$ and $\mathit{in}_m$ -- for the
first and second injections $\mathit{inj}_1$ and $\mathit{inj}_2$
respectively. Since $\mathit{in}_{\mathit{FreeMF}~f~a}$ injects an
single abstract command from $f$ into the coproduct, we use the free
monad structure to inject all the commands into the coproduct.

In order for the use of the $(\mathit{FreeMF}~f~a)$-and-$m$-algebra
initiality to construct a function on $\mu(\mathit{FreeMF}~f~a|m)$ in
the definition of $[-,-]$ to be valid, we must check that the second
component of $\eFold{c}{\mathit{join_{m'}} \circ g_2}$ is actually an
$m$-Eilenberg-Moore-algebra. For the first law
(\autoref{eq:em-alg-return}), we reason as follows:
\begin{displaymath}
  \begin{array}{cl}
    & \mathit{join}_{m'} \circ g_2 \circ \mathit{return}_m \\
    =&\eqAnnotation{$g_2$ is a monad morphism (\autoref{eq:monad-mor-return})} \\
    & \mathit{join}_{m'} \circ \mathit{return}_{m'} \\
    =&\eqAnnotation{monad law: $\mathit{join}_{m'} \circ \mathit{return}_{m'} = \mathit{id}$ (\autoref{eq:monad-join-return})} \\
    & \mathit{id}
  \end{array}
\end{displaymath}
The second law (\autoref{eq:em-alg-join}) is also straightforward:
\begin{displaymath}
  \begin{array}{cl}
    & \mathit{join}_{m'} \circ g_2 \circ \mathit{join}_m \\
    =&\eqAnnotation{$g_2$ is a monad morphism (\autoref{eq:monad-mor-join})} \\
    & \mathit{join}_{m'} \circ \mathit{join}_{m'} \circ g_2 \circ \mathit{fmap}_m~g_2 \\
    =&\eqAnnotation{monad law: $\mathit{join}_{m'} \circ \mathit{join}_{m'} = \mathit{join}_{m'} \circ \mathit{fmap}_m'~\mathit{join}_{m'}$ (\autoref{eq:monad-join-join})} \\
    & \mathit{join}_{m'} \circ \mathit{fmap}_{m'}~\mathit{join}_{m'} \circ g_2 \circ \mathit{fmap}_m~g_2 \\
    =&\eqAnnotation{naturality of $g_2$} \\
    & \mathit{join}_{m'} \circ g_2 \circ \mathit{fmap}_{m}~\mathit{join}_{m'} \circ \mathit{fmap}_m~g_2 \\
    =&\eqAnnotation{$\mathit{fmap}_m$ preserves function composition (\autoref{eq:fmap-comp})} \\
    & \mathit{join}_{m'} \circ g_2 \circ \mathit{fmap}_{m}~(\mathit{join}_{m'} \circ g_2)
  \end{array}
\end{displaymath}
The proof that $[g_1,g_2]$ satisfies the conditions specified in
\defref{defn:coproducts} is remarkably similar to the proof that
$\fmext{g}$ satisfies the required properties for the free monad
specification. This is another testament to the power of
$f$-and-$m$-algebras.

We emphasise that the result we have presented here is not new; Hyland
\emph{et al.} have already demonstrated, albeit with a different proof
technique, that the construction we give below in
\autoref{sec:construct-coproducts} actually defines the monad
coproduct. A special case of this result, where the free monad part of
the construction is the free monad over the identity functor, has also
been previously presented by Pir{\'o}g and Gibbons
\cite{pirog12tracing}. Our contribution is to show that the use of
$f$-and-$m$-algebras simplifies and elucidates the definitions
involved.

\section{Conclusions}
\label{sec:conclusions}

We have presented a generalisation of Filinski and St\o{}vring's
$f$-and-$m$-algebras to arbitrary categories, and seen how they
simplify defining and reasoning about functions that manipulate
interleaved data and effects. The key observation is that initial
$f$-and-$m$-algebras are the analogue for the effectful setting of
initial $f$-algebras in the pure setting. As such, they support the
extension of the standard definitional and proof principles to the
effectful setting. This allows the implicit interleaving of data with
effects, such as I/O and non-termination as, to be made explicit and
properly reflected in functions' types. Because they separate pure and
effectful concerns, $f$-and-$m$-algebras support the direct transfer
of definitions and proofs --- as illustrated with our running example
of list append --- from the pure setting to the effectful setting. We
have further shown how programming with initial $f$-and-$m$-algebras
can be made practical by giving a generic construction of them in
terms of $(f \circ m)$-algebras. Finally, we have argued that other
datatypes that interleave data and effects in languages such as
Haskell, Scala, and F\# can be expressed as coproducts of free monads
with arbitrary monads, and can thus be straightforwardly constructed
using initial $f$-and-$m$-algebras.

\subsection{Related Work}

The earliest attempt to incorporate effects into the initial algebra
methodology appears to be Sheard's~\cite{she93a,she93b} use of
compile-time reflection to give direct constructions of monadic
$\mathit{map}$ and $\mathit{fold}$ functions.
Fokkinga~\cite{fokkinga94monadic} and later
Pardo~\cite{pardo04combining}, generalised Sheard's constructions to
the general categorical setting, giving a generic recursion combinator
for effectful recursive computations that has type
\begin{displaymath}
  \fold{-}_m : (f~a \to m~a) \to \mu f \to m~a
\end{displaymath}
and whose definition requires the existence of a \emph{distributive
  law} $d :: f~(m~a) \to m~(f~a)$ describing how effects percolate
through pure data in a uniform way.  Fokkinga and Pardo both defined
such distributive laws by induction over a grammar of regular
functors, and then used them, together with liftings of functors to
Kleisli categories~\cite{bw90,mul93}, to define monadic
$\mathit{fold}$s for regular datatypes. The result was an effectful
structural recursion scheme over pure regular data in which all
effects are pushed to the ``outside'' to monadically wrap a pure
result.

In fact, as both Fokkinga and Pardo show, the existence of a
distributive law for just the binary product functor is all that is
actually required to ensure that distributive laws exist for all
regular functors. However, such laws need not exist for all monads;
there is no distributive law for binary products for the state monad,
for example. In any case, the assumption that distributive laws exists
is too strong for the purposes of this article: we are concerned here
with structural recursion over \emph{effectful} data, in which data
and effects are interleaved, rather than just monadically wrapped
data.

Although Fokkinga and Pardo work in the same effectful setting, Pardo
transfers more origami programming ideas from the pure setting to the
effectful one than Fokkinga does. In addition to defining the
aforementioned monadic $\mathit{fold}s$ (catamorphisms), Pardo
dualises them to give monadic $\mathit{unfold}$s (anamorphisms) for
structuring corecursive programs with monadic effects. He also defines
monadic hylomorphisms to support even more general ways of structuring
monadic computations and combining their results. Interestingly,
monadic hylomorphisms do achieve some interleaving of recursive calls
to effectful computations with other computations, but the
computations they structure must still consume pure data.  Pardo also
develops rules for fusing monadic programs structured using the
monadic constructs he defines.

Meijer and Juering~\cite{mj95} further extend ideas of origami
programming to the effectful setting by developing a number of monadic
fusion rules. Among these is a new short cut fusion rule for
eliminating (pure) intermediate structures of type $fa$ for regular
functors $f$ in a monadic context $m$.  J\"urgensen~\cite{jue02} and
Voigtl\"ander \cite{voi08} also define monadic fusion rules based on
the uniqueness of the map from a free monad to any other monad.  Like
the aforementioned recursion schemes, many methods based on initial
algebras for restricted classes of (pure) datatypes are in fact
generalisable to arbitrary inductive types. For example, Ghani and
Johann~\cite{gj09} give a short cut fusion rule that can eliminate
data structures of any (pure) inductive type in any monadic context.

The work of Filinski and St\o{}vring \cite{filinski07inductive} is
undoubtedly the most closely related to ours. As we do in this
article, they give $\mathit{fold}s$ for datatypes with proper
interleaving of effects. They do so first considering the case of lazy
datatypes, and then generalising to datatypes that interleave monadic
effects other than nontermination with pure data. To first model the
way laziness interleaves the possibility of nontermination at any
point in the production of a data structure, and then to model more
general interleavings of effects, Filinski and St\o{}vring work in the
specific category Cpo, and 
%of $\omega$-cpos and (total) continuous functions. They also work 
with a specific grammar of what might be called ``effectful regular
functors'' that allow effects in recursive positions. Their principle
of {\em definition by rigid induction} amounts to the derivation of
$\mathit{fold}$s for minimal invariants for monads in Cpo. A minimal
invariant is a special case --- in the specific setting of Cpo for the
lifting monad $m$ and an effectful regular functor $f$ --- of the
carrier of the initial $f$-and-$m$-algebra, and Filinski and
St\o{}vring's $\mathit{fold}$s are special cases of our
$\mathit{fold}$s from \defref{def:f-and-m-folds}. Such monadic
$\mathit{fold}$s differ from those of Fokkinga, Pardo, and Meijer and
Juering in that they derive from initiality in the category of
$f$-and-$m$-algebras, rather than from initiality of algebras in the
Kleisli category for $m$ under the auspices of a distributive law for
$f$ and $m$.  Because initial $f$-and-$m$-algebras properly interleave
effectful computations with the construction of pure data, so that
effects are actually an integral part of the data being processed
rather than just wrapping it, more general recursive patterns of
effectful computation are possible.

Given the well-known relationship between $\mathit{fold}$s and
induction, it is perhaps surprising that the papers preceding Filinski
and St\o{}vring's do not derive induction rules or other proof
principles for effectful datatypes. Filinski and St\o{}vring do,
however, give a principle of {\em proof by rigid induction} for such
datatypes that is a variant of those of both Lehmann and
Smyth~\cite{ls81} and Crole and Pitts~\cite{cp92}. It supports the
same kind of inductive reasoning, again in the specific category and
for the specific functors with which they work, that we show initial
$f$-and-$m$-algebras to support more generally.  \pattynote{We don't
  explicitly give proof principles (i.e., actually identify the proof
  principles we illustrate), so it's hard to draw the same parallel
  for them as for definitional principles. Be careful here.} The
results reported in this article thus extend both the definitional
principles of Filinksi and St\o{}vring for structuring recursion over
effectful datatypes, and their proof principles for reasoning about
computations over such datatypes, to the general category-theoretic
setting and to arbitrary functors.  Filinski and St\o{}vring also give
fusion rules for effectful streams (although not for arbitrary
effectful datatypes), and illustrate the extension of relational
reasoning to effectful datatypes. We consider neither fusion rules nor
relational reasoning here.  Nevertheless, we see that this article
generalises previous extensions of the initial algebra methodology to
the effectful setting in three ways: it handles arbitrary functors,
rather than special classes of functors; it handles actual
interleaving of effects and data, rather than just the wrapping of
pure results in effectful contexts; and it gives proof principles for
reasoning about interleaved effectful computations, rather than just
constructs for structuring those computations.

% \pattynote{Perhaps leave the following paragraph in the introduction?
%   If so, then skip to last paragraph, which appears below after it.}

\subsection{Future Work}

The monadic induction schemes introduced by Filinski and St\o{}vring,
and generalised here, \pattynote{Careful! Induction schemes not
  explicitly given} give one way to reason about effect-interleaved
data. The ``fast and loose'' reasoning advocated by Danielsson {\em et
  al.}~\cite{dan06} is another.  Using a logical relations style
relation to relate total and non-total semantics of programs,
Danielsson {\em et al.}  show that programmers can reason about
programs as though they were written in a total language and expect
the results to carry over to non-total languages.  It would be useful
from both practical and theoretical viewpoints to know if this kind of
``morally correct'' reasoning can be extended to effects other than
just nontermination.

\pattynote{Add other future work?}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{jfp}
\bibliography{paper}

\appendix

\section{Proof of \thmref{thm:make-initial-f-and-m-alg}}
\label{sec:make-initial-fm-proof}

\begin{proof*}
  The $f$-algebra and $m$-Eilenberg-Moore-algebra structure are
  constructed from the $(f \circ m)$-algebra structure map
  $\mathit{in}$ and the structure of the monad $m$.  For the
  $f$-algebra component, we use the composite:
  \begin{displaymath}
    \mathit{in}_f = \mathit{return}_m \circ \mathit{in} :: f~(m~(\mu(f \circ m))) \to m~(\mu(f \circ m))
  \end{displaymath}
  The $m$-Eilenberg-Moore-algebra component is straightforward, using
  the free Eilenberg-Moore-algebra construction from
  \autoref{sec:eilenberg-moore-algebras}:
  \begin{displaymath}
    \mathit{in}_m = \mathit{join}_m :: m~(m~(\mu(f \circ m))) \to m~(\mu(f \circ m))
  \end{displaymath}
  Since we have used the free Eilenberg-Moore-algebra construction, we
  are automatically guaranteed that we have an
  $m$-Eilenberg-Moore-algebra.

  Now let us assume we are given an $f$-and-$m$-algebra $(a,k,l)$. We
  construct, and prove unique, an $f$-and-$m$-algebra homomorphism $h$
  from the algebra $(m~(\mu(f \circ m)), \mathit{in}_f,
  \mathit{in}_m)$ to the algebra with carrier $a$ using the initiality
  of $\mu(f \circ m)$:
  \begin{displaymath}
    h = l \circ \mathit{fmap}_m~\fold{k \circ \mathit{fmap}_f~l} :: m~(\mu(f \circ m)) \to a
  \end{displaymath}
  Close inspection of $h$ reveals that it has the same structure as
  the definition of $\mathit{eAppend}$ in terms of initial
  $f$-algebras we gave at the start of \autoref{sec:direct-eappend},
  where $l = \mathit{join}_m$. Therefore, as we noted in the
  introduction to \autoref{sec:from-initial}, the construction we are
  building here abstracts out the common parts of proofs and
  definitions on effectful datatypes.

  To complete our proof, we now need to demonstrate that $h$ is an
  $f$-and-$m$-algebra homomorphism, and that it is the unique such. We
  split this task into three steps:
  \begin{enumerate}
  \item The function $h$ is an $f$-algebra homomorphism. We reason as
    follows:
    \begin{displaymath}
      \begin{array}{cl}
        & h \circ \mathit{in}_f \\
        =&\eqAnnotation{definitions of $h$ and $\mathit{in_f}$} \\
         &l \circ \mathit{fmap}_m~\fold{k \circ \mathit{fmap}_f~l} \circ \mathit{return}_m \circ \mathit{in} \\
        =&\eqAnnotation{naturality of $\mathit{return}_m$ (\autoref{eq:monad-return-natural})} \\
         &l \circ \mathit{return}_m \circ \fold{k \circ \mathit{fmap}_f~l} \circ \mathit{in} \\
        =&\eqAnnotation{$l$ is an $m$-Eilenberg-Moore-algebra (\autoref{eq:em-alg-return})} \\
         &\fold{k \circ \mathit{fmap}_f~l} \circ \mathit{in} \\
        =&\eqAnnotation{$\fold{-}$ is an $(f \circ m)$-algebra homomorphism (\autoref{eq:falgebra-homomorphism})} \\
         &k \circ \mathit{fmap}_f~l \circ \mathit{fmap}_f~(\mathit{fmap}_m~\fold{k \circ \mathit{fmap}_f~l}) \\
%    \end{array}
%  \end{displaymath}
%  \begin{displaymath}
%    \begin{array}{cl}
        =&\eqAnnotation{$\mathit{fmap}_f$ preserves function composition (\autoref{eq:fmap-comp})} \\
         &k \circ \mathit{fmap}_f~(l \circ \mathit{fmap_m}~\fold{k \circ \mathit{fmap}_f~l}) \\
        =&\eqAnnotation{definition of $h$} \\
         & k \circ \mathit{fmap}_f~h
      \end{array}
    \end{displaymath}
  \item The function $h$ is an $m$-Eilenberg-Moore-algebra
    homomorphism, as shown by the following steps:
    \begin{displaymath}
      \begin{array}{cl}
        & h \circ \mathit{in}_m \\
        =&\eqAnnotation{definitions of $h$ and $\mathit{in}_m$} \\
         &l \circ \mathit{fmap}_m~\fold{k \circ \mathit{fmap}_f~l} \circ \mathit{join}_m \\
        =&\eqAnnotation{naturality of $\mathit{join}_m$ (\autoref{eq:monad-join-natural})} \\
         &l \circ \mathit{join}_m \circ \mathit{fmap}_m~(\mathit{fmap}_m~\fold{k \circ \mathit{fmap}_f~l}) \\
        =&\eqAnnotation{$l$ is an Eilenberg-Moore algebra (\autoref{eq:em-alg-join})} \\
         &l \circ \mathit{fmap}_m~k \circ \mathit{fmap}_m~(\mathit{fmap}_m~\fold{k \circ \mathit{fmap}_f~l}) \\
        =&\eqAnnotation{$\mathit{fmap}_m$ preserves function composition (\autoref{eq:fmap-comp})} \\
         &l \circ \mathit{fmap}_m~(l \circ \mathit{fmap}_m~\fold{k \circ \mathit{fmap}_f~l}) \\
        =&\eqAnnotation{definition of $h$} \\
         &l \circ \mathit{fmap}_m~h
      \end{array}
    \end{displaymath}
  \item The function $h$ is the unique such $f$-and-$m$-algebra. Let
    us assume that there exists another $f$-and-$m$-algebra
    homomorphism $h' :: m~(\mu(f \circ m)) \to a$. We aim to show that
    $h = h'$. We first observe that the following function defined by composition:
    \begin{displaymath}
      h' \circ \mathit{return}_m :: \mu(f \circ m) \to a
    \end{displaymath}
    is an $(f \circ m)$-algebra homomorphism from $(\mu(f \circ m),
    \mathit{in})$ to $(a, k \circ \mathit{fmap}_f~l)$, as verified by
    the following steps:
    \begin{displaymath}
      \begin{array}{cl}
        & h' \circ \mathit{return}_m \circ \mathit{in} \\
        =&\eqAnnotationS{definition of $\mathit{in}_f$} \\
        & h' \circ \mathit{in}_f \\
        =&\eqAnnotationS{$h'$ is an $f$-and-$m$-algebra homomorphism} \\
        & k \circ \mathit{fmap}_f~h' \\
        =&\eqAnnotationS{monad law: $\mathit{join}_m \circ \mathit{fmap}_m~\mathit{return}_m = \mathit{id}$ (\autoref{eq:monad-join-fmap-return})} \\
        & k \circ \mathit{fmap}_f~(h' \circ \mathit{join}_m \circ \mathit{fmap}_m~\mathit{return}_m) \\
        =&\eqAnnotationS{$h'$ is an $m$-Eilenberg-Moore-algebra homomorphism (\autoref{eq:em-alg-homomorphism})} \\
        & k \circ \mathit{fmap}_f~(l \circ \mathit{fmap}_m~h' \circ \mathit{fmap}_m~\mathit{return}_m) \\
        =&\eqAnnotationS{$\mathit{fmap}_f$ preserves function composition (\autoref{eq:fmap-comp})} \\
        & k \circ \mathit{fmap}_f~l \circ \mathit{fmap}_f~(\mathit{fmap}_m~(h' \circ \mathit{return}_m))
      \end{array}
    \end{displaymath}
    Thus, by the uniqueness of $(f \circ m)$-algebra homomorphisms out
    of $\mu(f \circ m)$, we have proved that 
    \begin{equation}\label{eq:h'-prop}
      h' \circ \mathit{return}_m = \fold{k \circ \mathit{fmap}_f~l}
    \end{equation}
    We now use this equation to prove that $h=h'$ by the following
    steps:
    \begin{displaymath}
      \begin{array}{cl}
        & h \\
        =&\eqAnnotationS{definition of $h$} \\
        &l \circ \mathit{fmap}_m~\fold{k \circ \mathit{fmap}_f~l} \\
        =&\eqAnnotationS{\autoref{eq:h'-prop}} \\
        &l \circ \mathit{fmap}_m~(h' \circ \mathit{return}_m) \\
        =&\eqAnnotationS{$\mathit{fmap}_m$ preserves function composition (\autoref{eq:fmap-comp})} \\
        &l \circ \mathit{fmap}_m~{h'} \circ \mathit{fmap}_m~\mathit{return}_m \\
        =&\eqAnnotationS{$h'$ is an $m$-Eilenberg-Moore-algebra homomorphism (\autoref{eq:em-alg-homomorphism})} \\
        &h' \circ \mathit{join}_m \circ \mathit{fmap}_m~\mathit{return}_m \\
        =&\eqAnnotationS{monad law: $\mathit{join}_m \circ \mathit{fmap}_m~\mathit{return}_m = \mathit{id}$} \\
        &h'
      \end{array}
    \end{displaymath}
    Thus $h$ is the unique $f$-and-$m$-algebra homomorphism from
    $m~(\mu (f \circ m))$ to $a$. \mathproofbox
  \end{enumerate}
\end{proof*}


\end{document}
