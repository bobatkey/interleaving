\documentclass{jfp1}

\usepackage[usenames]{color}
\usepackage{hyperref}
\hypersetup{colorlinks=true,
            pdftitle=Interleaving data and effects,
            pdfauthor={Robert Atkey, Patricia Johann, Neil Ghani, and Bart Jacobs},
            pdfkeywords={inductive types, initial algebas, effects, monads, eilenberg-moore algebras, interleaved data and effects}}
\def\sectionautorefname{Section}
\def\subsectionautorefname{Section}

\usepackage{stmaryrd}

\newcommand{\fold}[1]{\llparenthesis #1 \rrparenthesis}
\newcommand{\eFold}[2]{\llparenthesis #1|#2 \rrparenthesis}
\newcommand{\construct}{\mathsf{in}}
\newcommand{\mbind}{\mathrel{>\kern-0.45em>\kern-0.45em=}}

\newcommand{\eqAnnotation}[1]{\hspace{2cm}\left\{\textrm{#1}\right\}}

\newcommand{\cat}[1]{\mathcal{#1}}

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{proofprinciple}{Proof Principle}

\newcommand{\proofprinref}[1]{\hyperref[#1]{Proof Principle \ref*{#1}}}
\newcommand{\thmref}[1]{\hyperref[#1]{Theorem \ref*{#1}}}
\newcommand{\defref}[1]{\hyperref[#1]{Definition \ref*{#1}}}

\newcommand{\kw}[1]{\textbf{#1}}

\title{Interleaving data and effects}

\author[R. Atkey, P. Johann, N. Ghani, B. Jacobs]
       {ROBERT ATKEY, PATRICIA JOHANN, NEIL GHANI \\
         University of Strathclyde, Glasgow, G1 1XH, UK\\
         \vspace{0.3cm}
         BART JACOBS \\
         Radboud University, Nijmegen, Netherlands}
%         \email{bart@cs.ru.nl}}

%         \email{$\{$robert.atkey,patricia.johann,neil.ghani$\}$@strath.ac.uk}

       
\begin{document}

\label{firstpage}

\maketitle

\begin{abstract}
  The study of programming with and reasoning about inductive
  data-types such as lists and trees has benefited from the simple
  categorical principle of initial $f$-algebras. From this simple
  principle, definitional and proof principles can be easily derived,
  and the technique has been expanded to a whole methodology of
  structured functional programming, usually called origami
  programming.

  In this article, we examine the setting of inductive data
  interleaved with computational effects. For example lists
  interleaved with the possibility of errors, or with input/output
  actions. We demonstrate that the abstraction of initial
  $f$-and-$m$-algebras, originally due to Filinski and St\o{}vring, is
  the appropriate way to reason about such interleaved pure and
  effectful data.
\end{abstract}

\section{Introduction}

In this article, we present a tutorial-style introduction to the use
of $f$-and-$m$-algebras for programming with and reasoning about pure
data interleaved with effects. We start by motivating the possibility
of interleaving data and effects with two examples.

\subsection{List processing with the possibility of errors}
\label{sec:list-proc-with-errors}

\begin{figure}[t]
  \centering
  \begin{displaymath}
    \begin{array}{l}
      \begin{array}{l@{\hspace{0.2em}}c@{\hspace{0.2em}}l}
        \kw{data}~\mathit{Token} &=& \mathsf{TokTT} \mathrel| \mathsf{TokFF} \mathrel| \mathsf{TokNot} \mathrel| \mathsf{TokConj} \mathrel| \mathsf{TokLParen} \mathrel| \mathsf{TokRParen} \\
        & & \kw{deriving}~\mathit{Show}
      \end{array}\\
      \\
      \begin{array}{l}
        \mathit{lexString} :: \mathit{String} \to [\mathit{Token}] \\
        \begin{array}{@{}l@{\hspace{0.5em}}lcl}
          \mathit{lexString}&[~]&=&[~]\\ 
          \mathit{lexString}&('\ ':\mathit{ss})&=&\mathit{lexString}~\mathit{ss}\\
          \mathit{lexString}&('\texttt{\textbackslash{}n}':\mathit{ss})&=&\mathit{lexString}~\mathit{ss} \\
          \mathit{lexString}&('\texttt{t}':{} '\texttt{t}':\mathit{ss})&=&\mathsf{TokTT} : \mathit{lexString}~\mathit{ss} \\
          \mathit{lexString}&('\texttt{f}':{} '\texttt{f}':\mathit{ss})&=&\mathsf{TokFF} : \mathit{lexString}~\mathit{ss} \\
          \mathit{lexString}&('\texttt{\textasciitilde}':\mathit{ss})&=&\mathsf{TokNot} : \mathit{lexString}~\mathit{ss} \\
          \mathit{lexString}&('\texttt{\&}':{} '\texttt{\&}':\mathit{ss})&=&\mathsf{TokConj} : \mathit{lexString}~\mathit{ss} \\
          \mathit{lexString}&('\texttt{(}':\mathit{ss})&=&\mathsf{TokLParen} : \mathit{lexString}~\mathit{ss} \\
          \mathit{lexString}&('\texttt{)}':\mathit{ss})&=&\mathsf{TokRParen} : \mathit{lexString}~\mathit{ss} \\
        \end{array}
      \end{array}
    \end{array}
  \end{displaymath}
  \caption{A simple lexer implementation in Haskell}
\label{fig:simple-lexer-1}
\end{figure}

Let us assume that we want to write a lexer for a simple
language. Recall that a lexer turns some textual input, represented as
a string of characters, into a list of tokens that is usually then
passed on to a parser. In \autoref{fig:simple-lexer-1} we have
followed a simple approach afforded by most functional programming
languages, making use of pattern matching to interrogate the input to
determine which tokens to produce. If we load the definitions
$\mathit{Token}$ and $\mathit{lexString}$ into an interactive Haskell
implementation, we can convert strings of characters into lists of
$\mathit{Token}$s. For example:
\begin{displaymath}
  \begin{array}{l}
    >~\mathit{lexString}~\texttt{"\textasciitilde{}tt \&\& (ff \&\& tt)"} \\
    {}[\mathsf{TokNot}, \mathsf{TokTT}, \mathsf{TokConj}, \mathsf{TokLParen}, \mathsf{TokFF}, \mathsf{TokConj}, \mathsf{TokTT}, \mathsf{TokRParen}]
  \end{array}
\end{displaymath}
Our implementation is appealingly simple. However, it is too
simple. When applied to invalid input (i.e.~input that does not
contain valid tokens for our little language), the function
$\mathit{lexString}$ simply crashes:
\begin{displaymath}
  \begin{array}{l}
    >~\mathit{lexString}~\texttt{"(tt \&\& 1)"} \\
    {}[\mathsf{TokLParen}, \mathsf{TokTT}, \mathsf{TokConj}, \\
    \quad\texttt{*** Exception: ...: Non-exhaustive patterns in function }\mathit{lexString}
  \end{array}
\end{displaymath}
Obviously, this is not ideal. A user of our language will want to know
exactly what it was about their input that caused the problem, and
also presumably does not care that we named the lexing function
$\mathit{lexString}$.

The error message produced by the Haskell implementation does tell us
one thing that is wrong with our function though: we have not
explicitly handled the case of characters that do not form valid
tokens. We can fix this by adding an catch-all case to the pattern
match that uses the $\mathit{error}$ function to report the erroneous
input by throwing an exception:
\begin{displaymath}
  \mathit{lexString}~(c : \mathit{ss}) = \mathit{error}~(\texttt{"Unrecognised character: "} \dplus [c])
\end{displaymath}
Using a function like $\mathit{error}$ to throw an exception is not
without its costs however. Throwing exceptions in what is otherwise
purely functional code eliminates the possibility of using
straightforward techniques to reason about such code, especially in
the presence of Haskell's ``imprecise'' exception semantics. Moreover,
exceptions can only be caught in the $\mathit{IO}$ monad, constraining
our choices in how the organise code.

The problem here is that we have stated that the $\mathit{lexString}$
function ought to return a list of $\mathit{Token}$ values, but the
standard list data-type does not admit the possibility of errors. A
list either contains an element, or stops.

One way to proceed is to define different lexing function that uses a
monad to report errors. We could use the following definition of a
simple error monad:
\begin{displaymath}
  \begin{array}{ll}
    \begin{array}[t]{l}
      \kw{data}~\mathit{ErrorM}~a \\
      \quad
      \begin{array}{c@{\hspace{0.5em}}l}
        = & \mathsf{Ok}~a \\
        | & \mathsf{Error}~\mathit{String}
      \end{array}
    \end{array}
    &
    \begin{array}[t]{l}
      \kw{instance}~\mathit{Monad}~\mathit{ErrorM}~\kw{where} \\
      \quad \mathit{return} = \mathsf{Ok} \\
      \quad
      \begin{array}{@{}l@{\hspace{0.4em}}c@{\hspace{0.4em}}l@{\hspace{0.4em}}c@{\hspace{0.4em}}l}
        (\mathsf{Ok}~a) &\mbind &f &=& f~a \\
        (\mathsf{Error}~\mathit{msg}) &\mbind& f& =& \mathsf{Error}~\mathit{msg}
      \end{array}
    \end{array}
  \end{array}
\end{displaymath} % FIXME: define return and bind?
and define a function $\mathit{lexStringM}$ with type:
\begin{displaymath}
  \mathit{lexStringM} :: \mathit{String} \to \mathit{ErrorM}~[\mathit{Token}]
\end{displaymath}
and the following clause for unrecognised characters:
\begin{displaymath}
  \mathit{lexStringM}~(c :: \mathit{ss}) = \mathsf{Error}~(\texttt{"Unrecognised character: "} \dplus [c])
\end{displaymath}
This solves our problem in that we can now write $\mathit{lexStringM}$
using only the purely functional features of Haskell, and still be
able to accurately report lexing errors. However, we have lost a
desirable feature of the old $\mathit{lexString}$ function: the fact
that it will produce a stream of tokens up to the error. The monadic
$\mathit{lexStringM}$ function generates no tokens if the input
contains unrecognised characters:
\begin{displaymath}
  \begin{array}{l}
    >~\mathit{lexStringM}~\texttt{"(tt \&\& 1)"} \\
    \mathsf{Error}~\texttt{"Unrecognised character: 1"}
  \end{array}
\end{displaymath}

To regain the old behaviour, we must \emph{interleave} the possibility
of error with the list of tokens. The following data-type declaration
defines a type of lists interleaved with the possibility of error:
\begin{displaymath}
  \begin{array}{l}
    \kw{data}~\mathit{List_{err}}~a \\
    \quad
    \begin{array}{c@{\hspace{0.5em}}l}
      = & \mathsf{Nil}_{\mathit{err}} \\
      | & \mathsf{Cons}_{\mathit{err}}~a~(\mathit{List_{err}}~a) \\
      | & \mathsf{Err}_{\mathit{err}}~\mathit{String}
    \end{array}
  \end{array}
\end{displaymath}
We can now define a new lexing function $\mathit{lexString_{err}}$ with the type
\begin{displaymath}
  \mathit{lexString_{err}} :: \mathit{String} \to \mathit{List_{err}}~\mathit{Token}
\end{displaymath}
and the following clause for unrecognised characters:
\begin{displaymath}
  \mathit{lexString_{err}}~(c :: \mathit{ss}) = \mathsf{Err}~(\texttt{"Unrecognised character: "} \dplus [c])
\end{displaymath}
The new function $\mathit{lexString_{err}}$ combines the advantages of
the previous two definitions: it produces all the tokens up to an
error, and it reports errors using only pure functional Haskell code
(we have dropped the $\ _{err}$ subscript in the output for brevity):
\begin{displaymath}
  \begin{array}{l}
    >~\mathit{lexString_{err}}~\texttt{"(tt \&\& 1)"} \\
    \mathsf{Cons}~\mathsf{TokLParen}~(\mathsf{Cons}~\mathsf{TokTT}~(\mathsf{Cons}~\mathsf{TokConj}~(\mathsf{Err}~(\texttt{"Unrecognised character: 1"}))))
  \end{array}
\end{displaymath}
Functions that consume the output of $\mathit{lexString_{err}}$ can
now process all the tokens up to any error, and also handle any lexing
errors in a purely functional way.

However, the new data-type $\mathit{List_{err}}$ that we had to define
presents a problem in itself. We now have a new list-like type, but
none of the extensive Haskell library of functions for dealing with
lists carries over to this new type. Moreover, even if we define
standard list functions like append or reverse on
$\mathit{List_{err}}$, we will have to verify all the standard
properties of these functions again (assuming that they hold).

In this article, we present a method for lifting definitions and
proofs from pure data-types such as lists to data-types that interleave
pure data and effects, making use of the concept of
$f$-and-$m$-algebras, which we define in
\autoref{sec:f-and-m-algebras}.


% For example, like list append...
%   \begin{displaymath}
%     \begin{array}{l}
%       \mathit{append_{err}} :: \mathit{List_{err}}~a \to \mathit{List_{err}}~a \to \mathit{List_{err}}~a \\
%       \begin{array}{@{}l@{\hspace{0.5em}}l@{\hspace{0.5em}}lcl}
%         \mathit{append_{err}}&\mathsf{Nil}_{\mathit{err}}&\mathit{ys} &=& \mathit{ys} \\
%         \mathit{append_{err}}&(\mathsf{Cons}_{\mathit{err}}~x~\mathit{xs})&\mathit{ys} &=& \mathsf{Cons}_{\mathit{err}}~x~(\mathit{append_{err}}~\mathit{xs}~\mathit{ys}) \\
%         \mathit{append_{err}}&(\mathsf{Err}_{\mathit{err}}~\mathit{msg})&\mathit{ys}&=&\mathsf{Err}_{\mathit{err}}~\mathit{msg}
%       \end{array}
%     \end{array}
%   \end{displaymath}
%   Frustratingly, this is very close to the normal list append
%   function, except for the additional case to handle the
%   $\mathsf{Err}_{\mathit{err}}$ constructor. ...

% \begin{displaymath}
%   \begin{array}{l}
%     \kw{data}~\mathit{List}~a \\
%     \quad
%     \begin{array}{c@{\hspace{0.5em}}l}
%       = & \mathsf{Nil} \\
%       | & \mathsf{Cons}~a~(\mathit{List}~a)
%     \end{array}
%   \end{array}
% \end{displaymath}

\subsection{List processing with the possibility of input/output}

In the lexing example, we used lists interleaved with errors to deal
with functions that output a stream of values, but that may encounter
an error as they perform their processing. A further refinement of the
$\mathit{lexString}$ function is for it to deal with input that is
generated on demand by performing some input/output interaction with
the outside world. The standard Haskell way of achieving such
interleaving is to use a function such as $\mathit{hGetContents} ::
\mathit{Handle} \to \mathit{IO}~\mathit{String}$ that presents the
input stream from a file handle as a lazy list of characters that are
read on demand. The function $\mathit{hGetContents}$ has two problems:
it interleaves the non-pure reading of the input with pure code that
operates on the list of characters, and it gives no way of controlling
the process. For example there is no way of deterministically
controlling when the handle is closed. Kiselyov
\cite{kiselyov12iteratees} goes into further depth on the problems
with the interleaved input/output approach.

A different approach is to explicitly represent lists interleaved with
the $\mathit{IO}$ actions that are used to generate them. This
approach has been used in several recently developed Haskell libraries
such as \texttt{conduits} and \texttt{pipes} for dealing safely with
interleaved input/output and pure data.

We can declare a type of lists interleaved with input/output actions
as a pair of mutually recursive data-types:
\begin{displaymath}
  \begin{array}{ll}
    \kw{data}~\mathit{List'_{io}}~a
    &
    \kw{newtype}~\mathit{List_{io}}~a = 
    \\
    \quad
    \begin{array}[t]{c@{\hspace{0.5em}}l}
      = & \mathsf{Nil}_{\mathit{io}} \\
      | & \mathsf{Cons}_{\mathit{io}}~a~(\mathit{List'_{IO}}~a) \\
    \end{array}
    &
    \quad \mathsf{List}_{\mathit{io}}~(\mathit{IO}~(\mathit{List'_{io}}~a))
  \end{array}
\end{displaymath}
An example function that uses $\mathit{IO}$ actions to generate a list
of values is the following function that reads a fixed number of
characters from the standard input:
\begin{displaymath}
  \begin{array}{l}
    \mathit{getChars} :: \mathit{Int} \to \mathit{List}_{io}~\mathit{Char} \\
    \begin{array}{@{}l@{\hspace{0.4em}}l@{\hspace{0.4em}}c@{\hspace{0.4em}}l}
      \mathit{getChars} & 0 & = & \mathsf{List}~(\mathit{return}~\mathsf{Nil}_{\mathit{io}}) \\
      \mathit{getChars} & n & = & \mathsf{List}~(
      \begin{array}[t]{@{}l@{\hspace{0.4em}}l}
        \kw{do} & c \leftarrow \mathit{getChar} \\
        & \mathit{return}~(\mathsf{Cons}_{\mathit{io}}~c~(\mathit{getChars}~(n-1))))
      \end{array}
    \end{array}
  \end{array}
\end{displaymath}
Note that the interleaving between effects and pure data has been made
explicit in the definition of $\mathit{getChars}$, the uses of the
standard Haskell library function $\mathit{getChar}$ to read a
character are interleaved with the uses of the constructor
$\mathsf{Cons}_{\mathit{io}}$.

% FIXME: Now talk about readers (a.k.a.~iteratees):
% \begin{displaymath}
%   \begin{array}{ll}
%     \kw{data}~\mathit{Reader'_{io}}~a~b
%     &
%     \kw{newtype}~\mathit{Reader_{io}}~a~b = 
%     \\
%     \quad
%     \begin{array}[t]{c@{\hspace{0.5em}}l}
%       = & \mathsf{Input}_{\mathit{io}}~(\mathit{Maybe}~a \to \mathit{Reader_{IO}}~a~b) \\
%       | & \mathsf{Yield}_{\mathit{io}}~b
%     \end{array}
%     &
%     \quad \mathsf{Reader}_{\mathit{io}}~(\mathit{IO}~(\mathit{Reader'_{io}}~a~b))
%   \end{array}
% \end{displaymath}

% Define run?
% \begin{displaymath}
%   \mathit{run} :: \mathit{List_{io}}~a \to \mathit{Reader_{io}}~a~b \to \mathit{IO}~b
% \end{displaymath}

\subsection{Common generalisation: interleaved data and effects}

In the previous two sections we have defined two data-types
$\mathit{List_{err}}$ and $\mathit{List_{io}}$ that represent lists of
pure data interleaved with monadic actions. A first obvious step is to
combine these into a single data-types, by abstracting out the monad
component:
\begin{displaymath}
  \begin{array}{ll}
    \kw{data}~\mathit{List'}~m~a
    &
    \kw{newtype}~\mathit{List}~m~a = 
    \\
    \quad
    \begin{array}[t]{c@{\hspace{0.5em}}l}
      = & \mathsf{Nil} \\
      | & \mathsf{Cons}~a~(\mathit{List}~m~a) \\
    \end{array}
    &
    \quad \mathsf{List}~(m~(\mathit{List'}~m~a))
  \end{array}
\end{displaymath}
Thus we obtain $\mathit{List_{err}}$ as
$\mathit{List}~\mathit{ErrorM}$ and $\mathit{List_{io}}$ as
$\mathit{List}~\mathit{IO}$.

This common generalisation is nice, but it still leaves us with the
problem that we identified above: how do we write programs in a
structured way on data-types that interleave data and effects, and how
do we reason about them. In this article, we present a solution to
both these problems in terms of $f$-and-$m$-algebras. The basic idea
is that the $f$ part of an $f$-and-$m$-algebra deals with the pure
data, while the $m$ part is a monad that deals with the effects. By
separating the two concepts as much as possible, we are able to
program and reason in a much cleaner way than if we worked directly on
the mutually defined data-type descriptions above.

The concept of $f$-and-$m$-algebra is originally due to Filinski and
St\o{}vring \cite{filinski07inductive}, and was generalised by the
present authors \cite{atkey12fibrational}. 

The remainder of this article is structured as follows:

\begin{enumerate}
\item In \autoref{sec:background}, we present previous work in the
  literature on integrating effectful computation with pure data. Our
  starting point is Filinski and St\o{}vring's work on induction
  principles for interleaved data-types, and the categorical
  generalisation by the current authors. We also recall the basic
  definitions of functor and monad in this section.
\item We recall the basic definition of (initial) $f$-algebras, the standard way
  of programming and reasoning in a structured way on inductive
  data-types, in \autoref{sec:f-algebras}. In this section we introduce
  the simple problem of defining and proving associative the list
  append function. For pure lists, this property is obviously very
  well known, but it becomes much more difficult to prove when moving
  to the more complex setting of interleaved data and effects.x
\item To motivate the use of $f$-and-$m$-algebras, in
  \autoref{sec:direct-eappend} we attempt to define and prove
  associative the append function for lists with interleaved effects
  directly from the relevant initial algebra property. This turns out
  to be very difficult and loses the direct simplicity of the proof in
  the pure case.
\item In \autoref{sec:f-and-m-algebras}, we present the definition of
  $f$-and-$m$-algebras and show how this concept simplifies both
  programming and reasoning about interleaved data and effects. We
  reprove the associativity property for list append, showing that the
  use of $f$-and-$m$-algebras allows us to reuse much of the proof
  from \autoref{sec:f-algebras}, leaving us just with the effectful
  parts.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Background}
\label{sec:background}

\subsection{Related Work}

As we mentioned above, the concept of $f$-and-$m$-algebra was
originally introduced by Filinski and St\o{}vring
\cite{filinski07inductive} in the setting of inductive reasoning about
interleaved data and effects in the category of $\mathit{DCPO}$s. The
current authors \cite{atkey12fibrational} generalised Filinski and
St\o{}vring's work to a general categorical setting based on
fibrations that allows for arbitrary categories, as well as more
freedom in the choice of predicates that can be used (for example,
Kripke predicates). Filinski and St\o{}vring used their framework to
show that a backtracking search procedure could be optimised for speed
by switching a list-based representation of results with a
continuation-passing-style representation. Their setting was a
call-by-value programming language with arbitrary effects, so the
search process would be interleaved with effects, necessitating a
method for reasoning about interleaved data and effects.

Pir{\'o}g and Gibbons \cite{pirog12tracing} have also recently
investigated the interleaving of data and effects with an aim towards
defining a general notion of ``tracer'' for an arbitrary monad. By
interleaving a single pure constructor with an arbitrary monad, they
derive a way to ``checkpoint'' monadic computations. Pir{\'o}g and
Gibbons demonstrate several uses of their definition, notably for
partial, non-deterministic and probabilistic computations.

%Hyland \emph{et al.},

The combination of monadic effects and inductive data types has
previously been studied by Fokkinga \cite{fokkinga94monadic} and Pardo
\cite{pardo04combining}. They use distributive laws $\lambda : f \circ
m \rightarrow m \circ f$ relating functors describing data types to
monads modelling effects. Given a distributive law, it can be shown
that $\mu f$ is the carrier of an initial algebra in the Kleisli
category of $m$. From this, a theory of effectful structural recursion
over \emph{pure} data is derived. By contrast, in this paper we have
explored computation and reasoning with \emph{effectful} data, where
data and effects are interleaved.

We have also already mentioned the work of Kiselyov
\cite{kiselyov12iteratees} on interleaving input/output and pure data
for the purposes of controlling streaming input/output in Haskell
programs. Kiselyov's work has lead to several Haskell libraries,
including \texttt{enumerators}, \texttt{conduits} and
\texttt{pipes}. While at core these libraries are based on the
interleaving of effects with pure data, their current designs go
beyond the framework we present in this article due to real-world
concerns with exception handling, timely and deterministic resource
control and chunking of input for efficiency.

\subsection{Functors and Monads}

We now recall the basic definitions of functor and monad in a Haskell
setting. We will make use of the basic definitions of the polymorphic
identity function $\mathit{id} = \lambda x.~x$ and function
composition $g \circ h = \lambda x.~g~(h~x)$.

\begin{definition}\label{defn:functor}
  A functor is a pair of a type operator $f$ and a higher-order
  function $\mathit{fmap}_f$ of type:
  \begin{displaymath}
    \mathit{fmap}_f :: (a \to b) \to f~a \to f~b
  \end{displaymath}
  such that $\mathit{fmap}_f$ preserves the identity function and
  composition:
  \begin{displaymath}
    \begin{array}{rcl}
      \mathit{fmap}_f~\mathit{id} & = & \mathit{id} \\
      \mathit{fmap}_f~(g \circ h) & = & \mathit{fmap}_f~g \circ \mathit{fmap}_f~h
    \end{array}
  \end{displaymath}
\end{definition}

In Haskell, the fact that a type operator $f$ has an associated
$\mathit{fmap}_f$ is usually expressed by declaring that $f$ is a
member of the $\mathit{Functor}$ typeclass:
\begin{displaymath}
  \begin{array}{l}
    \kw{class}~\mathit{Functor}~f~\kw{where} \\
    \quad \mathit{fmap} :: (a \to b) \to f~a \to f~b
  \end{array}
\end{displaymath}
The use of a type class to represent functors allows the programmer
just use $\mathit{fmap}$ and let the type checker infer which $f$'s
associated $\mathit{fmap}$ was intended. For this article, we shall
always use a subscript on $\mathit{fmap}$ to indicate which type
operator is intended, in an attempt to reduce confusion.

We will make heavy use of monads in this article to represent
effectful computation. We have opted to use the ``categorical''
definition of monad in terms of a $\mathit{join}$ (or multiplication)
operation, rather than the Kleisli-triple presentation with a bind
operation ($\mbind$) that is more standard in Haskell programming. For
our purposes, the categorical definition is more convenient for
equational reasoning. Standard references such as the lecture notes by
Benton, Hughes and Moggi \cite{benton00monads} discuss the
translations between the two presentations.

\begin{definition}
  A monad is a quadruple of a type constructor $m$, and three
  functions:
  \begin{displaymath}
    \begin{array}{rcl}
      \mathit{fmap}_m   & :: & (a \to b) \to m~a \to m~b \\
      \mathit{return}_m & :: & a \to m~a \\
      \mathit{join}_m   & :: & m~(m~a) \to m~a
    \end{array}
  \end{displaymath}
  such that the pair $(m, \mathit{fmap}_m)$ is a functor
  (\defref{defn:functor}), and the following properties are satisfied:
  \begin{displaymath}
    \begin{array}{rcl}
      \mathit{join}_m \circ \mathit{return}_m & = & \mathit{id} \\
      \mathit{join}_m \circ \mathit{fmap}_m~\mathit{return}_m & = & \mathit{id} \\
      \mathit{join}_m \circ \mathit{fmap}_m~\mathit{join}_m & = & \mathit{join}_m \circ \mathit{join}_m
    \end{array}
  \end{displaymath}
  and also the naturality laws:
  \begin{displaymath}
    \begin{array}{rcl}
      \mathit{return}_m \circ f & = & \mathit{fmap}_m~f \circ \mathit{return}_m \\
      \mathit{join}_m \circ \mathit{fmap}_m~(\mathit{fmap}_m~f) & = & \mathit{fmap}_m~f \circ \mathit{join}_m
    \end{array}
  \end{displaymath}
\end{definition}

As for functors, monads in Haskell are usually represented in terms of
the $\textit{Monad}$ type class. Again, for this article, we will
always use subscripts on $\mathit{return}_m$ and $\mathit{join}_m$ to
disambiguate which monad is being referred to, instead of leaving it
to the reader to infer.

% Mention that all monads will be strong, due to the internalness of
% $\mathit{fmap}$.

% Let $m$ be a monad. For the purposes of this article, we assume that
% the data for a monad consists of the following three items:
% such that $\mathit{fmap}_m$ satisfies the functor laws, and the
% following laws are also satisfied:

% FIXME: remark on the alternative presentation of a monad as a Kleisli
% triple. And do notation.

% \begin{displaymath}
%   \begin{array}{rcl}
%     \mathit{return}_m & :: & a \to m~a \\
%     \mbind_m & :: & m~a \to (a \to m~b) \to m~b
%   \end{array}
% \end{displaymath}
% such that
% \begin{displaymath}
%   \begin{array}{rcl}
%     (\mathit{return}_m~a) \mbind_m g & = & g~a \\
%     c \mbind_m \mathit{return}_m & = & c \\
%     c \mbind_m (\lambda x.~g_1~x \mbind_m g_2) & = & (c \mbind_m g_1) \mbind_m g_2
%   \end{array}
% \end{displaymath}

% If we have a monad in Kleisli-triple form, then it is possible to
% define $\mathit{fmap}_m$ and $\mathit{join}_m$ in terms of
% $\mathit{return}_m$ and $\mbind_m$:
% \begin{displaymath}
%   \begin{array}{rcl}
%     \mathit{fmap}_m~f~c &=& c \mbind_m (\mathit{return}_m \circ f) \\
%     \mathit{join}_m~c & = & c \mbind_m \mathit{id}
%   \end{array}
% \end{displaymath}
% Conversely, if we have a monad in the form $(m, \mathit{fmap}_m,
% \mathit{return}_m, \mathit{join}_m)$, then it is possible to define a
% $\mbind_m$ operation:
% \begin{displaymath}
%   c \mbind_m g = \mathit{join}_m~(\mathit{fmap}_m~g~c)
% \end{displaymath}
% The reader is invited to check that these two constructions satisfy
% the required properties we have listed above, and that they are
% mutually inverse constructions.

% Finally in this short recap of monads, we recall the definition of a
% \emph{monad morphism} between a pair of monads. Monad morphisms
% represent structure preserving maps between monads. For our purposes,
% it will be most useful to present this definition in terms of the
% Kleisli-triple presentation of monads.

% \begin{definition}[Monad morphism]
%   Let $(m_1, \mathit{return}_{m_1}, \mbind_{m_1})$ and $(m_2,
%   \mathit{return}_{m_2}, \mbind_{m_2})$ be a pair of monads in
%   Kleisli-triple form. A \emph{monad morphism} between them is a
%   polymorphic function:
%   \begin{displaymath}
%     f :: m_1~a \to m_2~a
%   \end{displaymath}
%   that satifies the following two properties:
%   \begin{displaymath}
%     \begin{array}{rcl}
%       f \circ \mathit{return}_{m_1} & = & \mathit{return}_{m_2} \\
%       f~(c \mbind_{m_1} g) & = & f~c \mbind_{m_2} (f \circ g)
%     \end{array}
%   \end{displaymath}
% \end{definition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Pure inductive data types via $f$-algebras}
\label{sec:f-algebras}

The standard way to handle structured programming with inductive data
structures is to make use of the categorical concepts of $f$-algebra
and initial $f$-algebra. Initial $f$-algebras allow us to define
recursive programs on inductive data structures, and also give us a
proof principle for reasoning about them
(\proofprinref{pp:initial-alg}, below). In this section, we first
recall the basic definitions of $f$-algebra, initial $f$-algebra and
the proof principle. We then make use of this proof principle to prove
a simple property of an append function on lists. We will revisit this
example below in \autoref{sec:direct-eappend} and
\autoref{sec:f-and-m-append}, in the setting of lists interleaved with
effects.

\subsection{Algebras and the initial algebra proof principle}
\label{sec:f-algebras-detail}

Let $(f, \mathit{fmap}_f)$ be a functor. The general idea of using
$f$-algebras to reason about inductive data structures is that the
functor $f$ represents a single ``layer'' in the data structure. For
example, the following definition of a functor $\mathit{ListF}~a$,
where $a$ is an arbitrary type, represents individual layers of a list
data-type. The initial $(\mathit{ListF}~a)$-algebra, which we define
below, represents complete lists.

\begin{displaymath}
  \begin{array}{l@{\hspace{3em}}l}
    \begin{array}{l}
      \kw{data}~\mathit{ListF}~a~x \\
      \quad
      \begin{array}{cl}
        = & \mathsf{Nil} \\
        | & \mathsf{Cons}~a~x
      \end{array}
    \end{array}
    &
    \begin{array}{l}
      \mathit{fmap}_{\mathit{ListF}~a} :: (x \to y) \to \mathit{ListF}~a~x \to \mathit{ListF}~a~y \\
      \begin{array}{@{}l@{\hspace{0.3em}}l@{\hspace{0.3em}}lcl}
      \mathit{fmap}_{\mathit{ListF}~a}&f&\mathsf{Nil} &=& \mathsf{Nil} \\
      \mathit{fmap}_{\mathit{ListF}~a}&f&(\mathsf{Cons}~a~x) &=& \mathsf{Cons}~a~(f~x)
    \end{array}
    \end{array}
  \end{array}
\end{displaymath}

An $f$-algebra for a given functor $f$ is a way of describing an
action for each layer in an inductive data structure. Formally,
$f$-algebras are defined as follows:
\begin{definition}
  An $f$-algebra is a pair of
  a type $a$ and a function $\mathit{fAlgebra} :: f~a \to a$.
\end{definition}
Given a pair of $f$-algebras, there is also the concept of a
homomorphism between them:
\begin{definition}
  Given a pair of $f$-algebras $(a,\mathit{fAlgebra}_a)$ and $(b,
  \mathit{fAlgebra}_b)$, an $f$-algebra homomorphism between them is a
  function $h :: a \to b$ such that
  \begin{displaymath}
    h \circ \mathit{fAlgebra}_a = \mathit{fAlgebra}_b \circ \mathit{fmap}_f~h
  \end{displaymath}
\end{definition}

\begin{definition}
  An \emph{initial} $f$-algebra is an $f$-algebra $(\mu f,
  \mathit{construct})$ such that for any other $f$-algebra $(a,
  \mathit{fAlgebra}_a)$, there exists a \emph{unique} $f$-algebra
  homomorphism $h :: \mu f \to a$.
\end{definition}

The requirement that an initial $f$-algebra always has an $f$-algebra
homomorphism to any other $f$-algebra allows us to define functions on
the carriers of initial $f$-algebras. We shall see an example of this
in the next section. The uniqueness requirement yields the following
proof principle for functions defined in this way on initial
$f$-algebras.

\begin{proofprinciple}[Initial $f$-Algebras]\label{pp:initial-alg}
  Let $(a, \mathit{fAlgebra})$ be an $f$-algebra, and $g :: \mu~f \to
  a$ be a function. To prove an equation
  \begin{displaymath}
    \fold{\mathit{fAlgebra}} = g,
  \end{displaymath}
  it suffices to show that
  \begin{displaymath}
    g \circ \mathit{construct} = \mathit{fAlgebra} \circ \mathit{fmap}_f~g.
  \end{displaymath}
\end{proofprinciple}

\subsection{Defining, and proving the associativity of, list append}
\label{sec:pure-append}

We now use an initial $(\mathit{ListF}~a)$-algebra and
\proofprinref{pp:initial-alg} to define and prove associative the
append function on pure lists. This definition and proof are standard
and have appeared many times in the literature. We present them here
in some detail in order to use them as a reference when we cover the
more complicated case of lists interleaved with effects from an
arbitrary monad in \autoref{sec:direct-eappend} and
\autoref{sec:f-and-m-algebras}, below.

We program and reason against the abstract interface of initial
algebras. Hence we assume that an initial $(\mathit{ListF}~a)$-algebra
$(\mu(\mathit{ListF}~a), \mathit{construct})$ exists, and we write
$\fold{-}$ for the unique homomorphism induced by initiality. We now
define $\mathit{append}$:
\begin{displaymath}
  \begin{array}{l}
    \mathit{append} :: \mu(\mathit{ListF}~a) \to \mu(\mathit{ListF}~a) \to \mu(\mathit{ListF}~a) \\
    \mathit{append}~\mathit{xs}~\mathit{ys} = \fold{\mathit{fAlgebra}}~\mathit{xs} \\
    \quad\begin{array}{r@{\hspace{0.4em}}l@{\hspace{0.5em}}c@{\hspace{0.5em}}l}
      \kw{where} & \mathit{fAlgebra}~\mathsf{Nil} &=& \mathit{ys} \\
                     & \mathit{fAlgebra}~(\mathsf{Cons}~a~\mathit{xs}) &=& \mathit{construct}~(\mathsf{Cons}~a~\mathit{xs})
    \end{array}
  \end{array}
\end{displaymath}

Immediately from the definition of $\mathit{append}$ we know that it
is a $(\mathit{ListF}~a)$-algebra homomorphism. This entails the
following two equational properties of $\mathit{append}$ that tell us
how it operates on the constructors $\mathsf{Nil}$ and $\mathsf{Cons}$:
\begin{displaymath}
  \begin{array}{rcl}
    \mathit{append}~(\mathit{construct}~\mathsf{Nil})~\mathit{ys} & = & \mathit{ys} \\
    \mathit{append}~(\mathit{construct}~(\mathsf{Cons}~a~\mathit{xs}))~\mathit{ys} & = & \mathit{construct}~(\mathsf{Cons}~a~(\mathit{append}~\mathit{xs}~\mathit{ys}))
  \end{array}
\end{displaymath}
We now make use of these properties, and
\proofprinref{pp:initial-alg}, to prove associativity:

\begin{theorem}\label{thm:append-assoc}
  For all $\mathit{xs}, \mathit{ys}, \mathit{zs} :: \mu(\mathit{ListF}~a)$,
  \begin{displaymath}
    \mathit{append}~\mathit{xs}~(\mathit{append}~\mathit{ys}~\mathit{zs}) = \mathit{append}~(\mathit{append}~\mathit{xs}~\mathit{ys})~\mathit{zs}
  \end{displaymath}
\end{theorem}

\begin{proof*}
  The function $\mathit{append}$ is defined in terms of the initial
  algebra property of $\mu(\mathit{ListF}~a)$, so are able to
  make use of \proofprinref{pp:initial-alg}. Thus we need to prove
  that for all $x ::
  \mathit{ListF}~a~(\mu(\mathit{ListF}~a))$,
  \begin{displaymath}
    \begin{array}{cl}
      &\mathit{append}~(\mathit{append}~(\mathit{construct}~x)~\mathit{ys})~\mathit{zs}\\
      =&\mathit{fAlgebra}~(\mathit{fmap}_{\mathit{ListF}~a}~(\lambda \mathit{xs}.~\mathit{append}~(\mathit{append}~\mathit{xs}~\mathit{ys})~\mathit{zs})~x)
    \end{array}
  \end{displaymath}
  where
  \begin{displaymath}
    \begin{array}{rcl}
      \mathit{fAlgebra}~\mathsf{Nil} &=& \mathit{append}~\mathit{ys}~\mathit{zs} \\
      \mathit{fAlgebra}~(\mathsf{Cons}~a~\mathit{xs}) &=& \mathit{construct}~(\mathsf{Cons}~a~\mathit{xs})
    \end{array}
  \end{displaymath}
  There are two cases to consider, depending on whether $x =
  \mathsf{Nil}$ or $x = \mathsf{Cons}~a~\mathit{xs}$. In the first
  case, we reason as follows:
  \begin{displaymath}
    \begin{array}{cl}
      & \mathit{append}~(\mathit{append}~(\mathit{construct}~\mathsf{Nil})~\mathit{ys})~\mathit{zs}\\
      =&\eqAnnotation{first property of $\mathit{append}$} \\
      & \mathit{append}~\mathit{ys}~\mathit{zs} \\
      =&\eqAnnotation{definition of $\mathit{fAlgebra}$} \\
      & \mathit{fAlgebra}~\mathsf{Nil} \\
      =&\eqAnnotation{definition of $\mathit{fmap}_{\mathit{ListF}~a}$} \\
      & \mathit{fAlgebra}~(\mathit{fmap}_{\mathit{ListF}~a}~(\lambda \mathit{xs}.~\mathit{append}~(\mathit{append}~\mathit{xs}~\mathit{ys})~\mathit{zs})~\mathsf{Nil})
    \end{array}
  \end{displaymath}
  The other possibility is that $x = \mathsf{Cons}~a~\mathit{xs}$, and
  we reason as follows:
  \begin{displaymath}
    \begin{array}{cl}
      & \mathit{append}~(\mathit{append}~(\mathit{construct}~(\mathsf{Cons}~a~\mathit{xs}))~\mathit{ys})~\mathit{zs}\\
      =&\eqAnnotation{second property of $\mathit{append}$} \\
      & \mathit{append}~(\mathit{construct}~(\mathsf{Cons}~a~(\mathit{append}~\mathit{xs}~\mathit{ys})))~\mathit{zs}\\
      =&\eqAnnotation{second property of $\mathit{append}$} \\
      & \mathit{construct}~(\mathsf{Cons}~a~(\mathit{append}~(\mathit{append}~\mathit{xs}~\mathit{ys})~\mathit{zs}))\\
      =&\eqAnnotation{definition of $\mathit{fAlgebra}$} \\
      & \mathit{fAlgebra}~(\mathsf{Cons}~a~(\mathit{append}~(\mathit{append}~\mathit{xs}~\mathit{ys})~\mathit{zs}))\\
      =&\eqAnnotation{definition of $\mathit{fmap}_{\mathit{ListF}~a}$} \\
      & \mathit{fAlgebra}~(\mathit{fmap}_{\mathit{ListF}~a}~(\lambda \mathit{xs}.~\mathit{append}~(\mathit{append}~\mathit{xs}~\mathit{ys})~\mathit{zs})~(\mathsf{Cons}~a~\mathit{xs})) \mathproofbox\\
    \end{array}
  \end{displaymath}
\end{proof*}

Thus the proof that $\mathit{append}$ is associative is relatively
straightforward, using \proofprinref{pp:initial-alg}. We shall see
below, in \autoref{sec:direct-eappend}, that attempting to use
\proofprinref{pp:initial-alg} again to reason about lists interleaved
with effects leads to a much more complicated proof. We then make use
of $f$-and-$m$-algebras in \autoref{sec:f-and-m-algebras} to prove the
same property for lists interleaved with effects, and still be able to
reuse the core of the above proof.

\subsection{Implementing initial $f$-algebras in Haskell}
\label{sec:initial-f-alg-impl}

% FIXME: need some references

In \autoref{sec:f-algebras}, we presented initial $f$-algebras as an
abstract interface, without giving a concrete Haskell
implementation. The construction of initial $f$-algebras for arbitrary
functors $f$ is standard, but we represent the construction here in
order to set up the implementation of initial $f$-and-$m$-algebras in
\autoref{sec:f-and-m-alg-impl}.

We first define the carrier of the initial $f$-algebra as a recursive
data-type as follows:
\begin{displaymath}
  \kw{data}~\mathit{Mu}~f = \mathsf{In}~\{ \mathit{unIn} :: f~(\mathit{Mu}~f) \}
\end{displaymath}
We have used Haskell's record definition syntax to implicitly define a
function $\mathit{unIn} :: \mathit{Mu}~f \to f~(\mathit{Mu}~f)$ that
is the inverse of the value constructor $\mathsf{In}$.  The
$f$-algebra structure map is defined directly in terms of the value
constructor $\mathsf{In}$:
\begin{displaymath}
  \begin{array}{l}
    \mathit{construct} :: f~(\mathit{Mu}~f) \to \mathit{Mu}~f \\
    \mathit{construct} = \mathsf{In}
  \end{array}
\end{displaymath}
and the $f$-algebra homomorphisms out of $\mathit{Mu}~f$ are defined
in terms of the functor structure $\mathit{fmap}_f$ and Haskell's
general recursion.
\begin{displaymath}
  \begin{array}{l}
    \fold{-} :: \mathit{Functor}~f \Rightarrow (f~a \to a) \to \mathit{Mu}~f \to a \\
    \fold{\mathit{fAlgebra}} = \mathit{fAlgebra} \circ \mathit{fmap}_f~\fold{\mathit{fAlgebra}} \circ \mathit{unIn}
  \end{array}
\end{displaymath}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Direct reasoning about interleaved data and effects}
\label{sec:direct-eappend}

In order to motivate the abstraction of $f$-and-$m$-algebras in the
next section, we now attempt to define and prove associativity for
lists interleaved with effects directly using initial $f$-algebras and
\proofprinref{pp:initial-alg}. As we shall see, at the level of
abstraction of initial $(\mathit{ListF}~a \circ m)$-algebras, both the
definition and proof are forced to mix the pure and effectful
concerns, and it is difficult to see the relationship with the
relatively straightforward proof of \thmref{thm:append-assoc}.

As above, we program and reason against the abstract interface of
initial algebras. Hence we assume that an initial $(\mathit{ListF}~a
\circ m)$-algebra $(\mu(\mathit{ListF}~a \circ m),
\mathit{construct})$ exists, and we write $\fold{-}$ for the unique
homomorphism induced by initiality. We now define $\mathit{eAppend}$:
\begin{displaymath}
  \begin{array}{l}
    \mathit{eAppend} :: m~(\mu (\mathit{ListF}~a \circ m)) \to m~(\mu (\mathit{ListF}~a \circ m)) \to m~(\mu (\mathit{ListF}~a \circ m)) \\
    \mathit{eAppend}~\mathit{xs}~\mathit{ys} = \mathit{join}_m~(\mathit{fmap}_m~\fold{\mathit{fAlgebra}}~\mathit{xs}) \\
    \begin{array}{r@{\hspace{0.4em}}l@{\hspace{0.5em}}c@{\hspace{0.5em}}l}
      \textbf{where} & \mathit{fAlgebra}~\mathsf{Nil} &=& \mathit{ys} \\
                     & \mathit{fAlgebra}~(\mathsf{Cons}~a~\mathit{xs}) &=& \mathit{return}_m~(\mathit{construct}~(\mathsf{Cons}~a~(\mathit{join}_m~\mathit{xs})))
    \end{array}
  \end{array}
\end{displaymath}
Note that this definition bears a slight resemblance to the definition
of $\mathit{append}$ above, but we have had to insert additional
constructs to deal with the management of effects. Thus we have had to
intermingle the effectful parts of the definition with the pure
parts. This is a result of the fact that the initial $f$-algebra
abstraction is unaware of the presence of effects.

We now prove that the function $\mathit{eAppend}$ is associative,
using \proofprinref{pp:initial-alg}.

\begin{theorem}\label{thm:direct-eappend-assoc}
  For all $\mathit{xs}, \mathit{ys}, \mathit{zs} :: m~(\mu (f \circ m))$,
  \begin{displaymath}
    \mathit{eAppend}~\mathit{xs}~(\mathit{eAppend}~\mathit{ys}~\mathit{zs}) = \mathit{eAppend}~(\mathit{eAppend}~\mathit{xs}~\mathit{ys})~\mathit{zs}
  \end{displaymath}
\end{theorem}

\begin{proof*}
  We will eventually be able to use \proofprinref{pp:initial-alg}, but
  first we must rearrange both sides of the equation to be of a
  suitable form. For this proof, we adopt the notation
  $\mathit{fAlgebra}_l$ to denote an instance of the
  $\mathit{fAlgebra}$ function defined in the body of
  $\mathit{eAppend}$ with the free variable $\mathit{ys}$ replaced by
  $l$.

  The left hand side of the equation to be proved is equal to:
  \begin{displaymath}
    \begin{array}{cl}
       &\mathit{eAppend}~\mathit{xs}~(\mathit{eAppend}~\mathit{ys}~\mathit{zs}) \\
       =&\eqAnnotation{definition of $\mathit{eAppend}$} \\
       &\mathit{join_m}~(\mathit{fmap_m}~\fold{\mathit{fAlgebra}_{\mathit{eAppend}~\mathit{ys}~\mathit{zs}}}~\mathit{xs})
    \end{array}
  \end{displaymath}
  The right hand side of the equation requires a little more work:
  \begin{displaymath}
    \begin{array}{cl}
      &\mathit{eAppend}~(\mathit{eAppend}~\mathit{xs}~\mathit{ys})~\mathit{zs} \\
      =&\eqAnnotation{definition of $\mathit{eAppend}$} \\
      &\mathit{eAppend}~(\mathit{join}_m~(\mathit{fmap}_m~\fold{\mathit{fAlgebra}_{\mathit{ys}}}~\mathit{xs}))~\mathit{zs} \\
      =&\eqAnnotation{definition of $\mathit{eAppend}$} \\
      &\mathit{join}_m~(\mathit{fmap}_m~\fold{\mathit{fAlgebra_{zs}}}~(\mathit{join}_m~(\mathit{fmap}_m~\fold{\mathit{fAlgebra}_{\mathit{ys}}}~\mathit{xs}))) \\
      =&\eqAnnotation{naturality of $\mathit{join_m}$} \\
      &\mathit{join}_m~(\mathit{join}_m~(\mathit{fmap}_m~(\mathit{fmap}_m~\fold{\mathit{fAlgebra_{zs}}})~(\mathit{fmap}_m~\fold{\mathit{fAlgebra_{ys}}}~\mathit{xs}))) \\
      =&\eqAnnotation{monad law: $\mathit{join_m} \circ \mathit{join_m} = \mathit{join_m} \circ \mathit{fmap_m}~\mathit{join_m}$} \\
      &\mathit{join_m}~(\mathit{fmap_m}~\mathit{join_m}~(\mathit{fmap}_m~(\mathit{fmap}_m~\fold{\mathit{fAlgebra_{zs}}})~(\mathit{fmap}_m~\fold{\mathit{fAlgebra_{ys}}}~\mathit{xs}))) \\
      =&\eqAnnotation{$\mathit{fmap_m}$ preserves composition} \\
      &\mathit{join_m}~(\mathit{fmap_m}~(\mathit{join_m} \circ \mathit{fmap}_m~\fold{\mathit{fAlgebra_{zs}}} \circ \fold{\mathit{fAlgebra_{ys}}})~\mathit{xs})
    \end{array}
  \end{displaymath}
  Looking at the final lines of these two chains of equations, we see
  that the problem reduces to proving the following equation:
  \begin{displaymath}
    \fold{\mathit{fAlgebra}_{\mathit{eAppend}~\mathit{ys}~\mathit{zs}}} = \mathit{join_m} \circ \mathit{fmap}_m~\fold{\mathit{fAlgebra_{zs}}} \circ \fold{\mathit{fAlgebra_{ys}}}
  \end{displaymath}
  To prove this equation, we use \proofprinref{pp:initial-alg}, which
  reduces the problem to proving the following equation, for all $x ::
  f~(m~(\mu(f \circ m)))$:
  \begin{displaymath}
    \begin{array}{cl}
      &\mathit{join_m}~(\mathit{fmap}_m~\fold{\mathit{fAlgebra_{zs}}}~(\fold{\mathit{fAlgebra_{ys}}}~(\mathit{construct}~x))) \\
      =& \mathit{fAlgebra}_{\mathit{eAppend}~\mathit{xs}~\mathit{ys}}~(\mathit{fmap}_{\mathit{ListF}~a}~(\mathit{fmap}_m~(\mathit{join_m} \circ \mathit{fmap}_m~\fold{\mathit{fAlgebra_{zs}}} \circ \fold{\mathit{fAlgebra_{ys}}}))~x)
    \end{array}
  \end{displaymath}
  There are two cases to consider, depending on whether $x =
  \mathsf{Nil}$ or $x = \mathsf{Cons}~a~\mathit{xs}$. In the first
  case, we reason as follows:
  \begin{displaymath}
    \begin{array}{cl}
      &\mathit{join_m}~(\mathit{fmap}_m~\fold{\mathit{fAlgebra_{zs}}}~(\fold{\mathit{fAlgebra_{ys}}}~(\mathit{construct}~\mathsf{Nil}))) \\
      =&\eqAnnotation{$\fold{\mathit{fAlgebra_{ys}}}$ is a $(\mathit{ListF}~a)$-algebra homomorphism} \\
      &\mathit{join_m}~(\mathit{fmap_m}~\fold{\mathit{fAlgebra_{zs}}}~(\mathit{fAlgebra_{ys}}~(\mathit{fmap}_{\mathit{ListF}~a}~(\mathit{fmap}_m~\fold{\mathit{fAlgebra_{ys}}})~\mathsf{Nil}))) \\
      =&\eqAnnotation{definition of $\mathit{fmap_{\mathit{ListF}~a}}$} \\
      &\mathit{join_m}~(\mathit{fmap_m}~\fold{\mathit{fAlgebra_{zs}}}~(\mathit{fAlgebra_{ys}}~\mathsf{Nil})) \\
      =&\eqAnnotation{definition of $\mathit{fAlgebra_{ys}}$} \\
      &\mathit{join_m}~(\mathit{fmap_m}~\fold{\mathit{fAlgebra_{zs}}}~\mathit{ys}) \\
      =&\eqAnnotation{definition of $\mathit{eAppend}$} \\
      &\mathit{eAppend}~\mathit{ys}~\mathit{zs} \\
      =&\eqAnnotation{definition of $\mathit{fAlgebra}_{\mathit{eAppend}~\mathit{ys}~\mathit{zs}}$} \\
      &\mathit{fAlgebra}_{\mathit{eAppend}~\mathit{ys}~\mathit{zs}}~\mathsf{Nil} \\
      =&\eqAnnotation{definition of $\mathit{fmap}_{\mathit{ListF}~a}$} \\
      &\mathit{fAlgebra}_{\mathit{eAppend}~\mathit{xs}~\mathit{ys}}~(\mathit{fmap}_{\mathit{ListF}~a}~(\mathit{fmap}_m~(\mathit{join_m} \circ \mathit{fmap}_m~\fold{\mathit{fAlgebra_{zs}}} \circ \fold{\mathit{fAlgebra_{ys}}}))~\mathsf{Nil})
    \end{array}
  \end{displaymath}
  In the second case, when $x = \mathsf{Cons}~a~\mathit{xs}$, we
  reason using the following steps:
  \begin{displaymath}
    \begin{array}{cl}
      &\mathit{join_m}~(\mathit{fmap}_m~\fold{\mathit{fAlgebra_{zs}}}~(\fold{\mathit{fAlgebra_{ys}}}~(\mathit{construct}~(\mathsf{Cons}~a~\mathit{xs})))) \\
      =&\eqAnnotation{$\fold{\mathit{fAlgebra_{ys}}}$ is a $(\mathit{ListF}~a)$-algebra homomorphism} \\
      &\mathit{join_m}~(\mathit{fmap}_m~\fold{\mathit{fAlgebra_{zs}}}~(\mathit{fAlgebra_{ys}}~(\mathit{fmap}_{\mathit{ListF}~a}~(\mathit{fmap}_m~\fold{\mathit{fAlgebra_{ys}}})~(\mathsf{Cons}~a~\mathit{xs})))) \\
      =&\eqAnnotation{definition of $\mathit{fmap_{\mathit{ListF}~a}}$} \\
      &\mathit{join_m}~(\mathit{fmap}_m~\fold{\mathit{fAlgebra_{zs}}}~(\mathit{fAlgebra_{ys}}~(\mathsf{Cons}~a~(\mathit{fmap}_m~\fold{\mathit{fAlgebra_{ys}}}~\mathit{xs})))) \\
      =&\eqAnnotation{definition of $\mathit{fAlgebra_{ys}}$} \\
      &\mathit{join_m}~(\mathit{fmap}_m~\fold{\mathit{fAlgebra_{zs}}}~(\mathit{return}_m~(\mathit{construct}~(\mathsf{Cons}~a~(\mathit{join}_m~(\mathit{fmap_m}~\fold{\mathit{fAlgebra_{ys}}}~\mathit{xs})))))) \\
      =&\eqAnnotation{naturality of $\mathit{return_m}$} \\
      &\mathit{join_m}~(\mathit{return}_m~(\fold{\mathit{fAlgebra_{zs}}}~(\mathit{construct}~(\mathsf{Cons}~a~(\mathit{join}_m~(\mathit{fmap_m}~\fold{\mathit{fAlgebra_{ys}}}~\mathit{xs})))))) \\
      =&\eqAnnotation{monad law: $\mathit{join_m} \circ \mathit{return_m} = \mathit{id}$} \\
      &\fold{\mathit{fAlgebra_{zs}}}~(\mathit{construct}~(\mathsf{Cons}~a~(\mathit{join}_m~(\mathit{fmap_m}~\fold{\mathit{fAlgebra_{ys}}}~\mathit{xs})))) \\
      =&\eqAnnotation{$\fold{\mathit{fAlgebra_{zs}}}$ is a $(\mathit{ListF}~a)$-algebra homomorphism} \\
      &\mathit{fAlgebra_{zs}}~(\mathit{fmap}_{\mathit{ListF}~a}~(\mathit{fmap}_m~\fold{\mathit{fAlgebra_{zs}}})~(\mathsf{Cons}~a~(\mathit{join}_m~(\mathit{fmap_m}~\fold{\mathit{fAlgebra_{ys}}}~\mathit{xs})))) \\
      =&\eqAnnotation{definition of $\mathit{fmap}_{\mathit{ListF}~a}$} \\
      &\mathit{fAlgebra_{zs}}~(\mathsf{Cons}~a~(\mathit{fmap}_m~\fold{\mathit{fAlgebra_{zs}}}~(\mathit{join}_m~(\mathit{fmap_m}~\fold{\mathit{fAlgebra_{ys}}}~\mathit{xs})))) \\
      =&\eqAnnotation{definition of $\mathit{fAlgebra_{zs}}$} \\
      &\mathit{return}_m~(\mathit{construct}~(\mathsf{Cons}~a~(\mathit{join_m}~(\mathit{fmap}_m~\fold{\mathit{fAlgebra_{zs}}}~(\mathit{join}_m~(\mathit{fmap_m}~\fold{\mathit{fAlgebra_{ys}}}~\mathit{xs})))))) \\
      =&\eqAnnotation{naturality of $\mathit{join_m}$} \\
      &\mathit{return}_m~(\mathit{construct}~(\mathsf{Cons}~a~(\mathit{join_m}~(\mathit{join}_m~(\mathit{fmap}_m~(\mathit{fmap_m}~\fold{\mathit{fAlgebra_{zs}}})~(\mathit{fmap}_m~\fold{\mathit{fAlgebra_{ys}}}~\mathit{xs})))))) \\
      =&\eqAnnotation{monad law: $\mathit{join_m} \circ \mathit{join_m} = \mathit{join_m} \circ \mathit{fmap}_m~\mathit{join_m}$} \\
      &\mathit{return}_m~(\mathit{construct}~(\mathsf{Cons}~a~(\mathit{join_m}~(\mathit{fmap}_m~\mathit{join_m}~(\mathit{fmap}_m~(\mathit{fmap_m}~\fold{\mathit{fAlgebra_{zs}}})~(\mathit{fmap}_m~\fold{\mathit{fAlgebra_{ys}}}~\mathit{xs})))))) \\
      =&\eqAnnotation{$\mathit{fmap}_m$ preserves function composition} \\
      &\mathit{return}_m~(\mathit{construct}~(\mathsf{Cons}~a~(\mathit{join_m}~(\mathit{fmap}_m~(\mathit{join_m} \circ \mathit{fmap_m}~\fold{\mathit{fAlgebra_{zs}}} \circ \fold{\mathit{fAlgebra_{ys}}})~\mathit{xs})))) \\
      =&\eqAnnotation{definition of $\mathit{fAlgebra}_{\mathit{eAppend}~\mathit{ys}~\mathit{zs}}$} \\
      &\mathit{fAlgebra}_{\mathit{eAppend}~\mathit{ys}~\mathit{zs}}~(\mathsf{Cons}~a~(\mathit{fmap}_m~(\mathit{join_m} \circ \mathit{fmap_m}~\fold{\mathit{fAlgebra_{zs}}} \circ \fold{\mathit{fAlgebra_{ys}}})~\mathit{xs})) \\
      =&\eqAnnotation{definition of $\mathit{fmap}_{\mathit{ListF}~a}$} \\
      &\mathit{fAlgebra}_{\mathit{eAppend}~\mathit{ys}~\mathit{zs}}~(\mathit{fmap}_{\mathit{ListF}~a}~(\mathit{fmap}_m~(\mathit{join_m} \circ \mathit{fmap_m}~\fold{\mathit{fAlgebra_{zs}}} \circ \fold{\mathit{fAlgebra_{ys}}}))~(\mathsf{Cons}~a~\mathit{xs})) \mathproofbox
    \end{array}
  \end{displaymath}
\end{proof*}

We have been able to complete the proof, but the intermingling of the
effectful and pure parts has obscured the basic structure of the
proof. We shall see in the next section how to raise our level of
abstraction to cleanly separate the pure and effectful parts of our
definitions and proofs by considering the notion of
$f$-and-$m$-algebras.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Separating data and effects with $f$-and-$m$-algebras}
\label{sec:f-and-m-algebras}

As we saw in the previous section, directly defining and proving
properties of functions on data-types consisting of interleaved pure
and effectful information is possible, but tedious. We were not able
to build upon the definition and proof that we used in the
non-effectful case (\autoref{sec:pure-append}), and our equational
reasoning repeatedly broke through several layers of abstraction: we
were forced to unfold the definition $\mathit{eAppend}$ several times
in the proof of \thmref{thm:direct-eappend-assoc} in order to perform
further rewriting steps.

In this section, we define the concept of $f$-and-$m$-algebras,
originally introduced by Filinski and St\o{}vring
\cite{filinski07inductive}, and generalised to arbitrary functors $f$
by the current authors \cite{atkey12fibrational}. As the name may
imply, $f$-and-$m$-algebras are simultaneously $f$-algebras and
$m$-algebras. A twist is that the $m$-algebra component must be an
Eilenberg-Moore algebra, another concept from category theory that we
now recall.

\subsection{Eilenberg-Moore algebras}
\label{sec:eilenberg-moore-algebras}

Given a monad $(m, \mathit{fmap}_m, \mathit{return}_m,
\mathit{join}_m)$, an $m$-Eilenberg-Moore-algebra is an $m$-algebra
that also interacts well with the structure of the monad:

\begin{definition}
  An $m$-Eilenberg-Moore algebra consists of a pair of an object $a$
  and a function
  \begin{displaymath}
    \mathit{mAlgebra} :: m~a \to a
  \end{displaymath}
  such that the following two equations are satisfied:
  \begin{displaymath}
    \begin{array}{rcl}
      \mathit{mAlgebra} \circ \mathit{return}_m & = & \mathit{id} \\
      \mathit{mAlgebra} \circ \mathit{join}_m & = & \mathit{mAlgebra} \circ \mathit{fmap}_m~\mathit{mAlgebra} 
    \end{array}
  \end{displaymath}
\end{definition}

Eilenberg-Moore algebras for monads form a key piece of the abstract
categorical theory of monads, especially their application to
universal algebra. For a monad that represents an algebraic theory
(e.g.~abelian groups), the collection of all Eilenberg-Moore algebras
for that monad are exactly the structures supporting that algebraic
theory. Mac Lane \cite{maclane98} contains more information on this
view of Eilenberg-Moore algebras.

In terms of computational effects, an $m$-Eilenberg-Moore-algebra $(a,
\mathit{mAlgebra})$ represents a way of ``performing'' the effects of
the monad $m$ in the type $a$. For example, if we let the monad $m$ be
the error monad $\mathit{ErrorM}$ we defined in
\autoref{sec:list-proc-with-errors}, then we can define an
$\mathit{ErrorM}$-Eilenberg-Moore-algebra with carrier $\mathit{IO}~a$
as follows:
\begin{displaymath}
  \begin{array}{l}
    \mathit{mAlgebra} :: \mathit{ErrorM}~(\mathit{IO}~a) \to \mathit{IO}~a \\
    \begin{array}{@{}l@{\hspace{0.4em}}lcl}
      \mathit{mAlgebra}&(\mathsf{Ok}~\mathit{ioa}) & = & \mathit{ioa} \\
      \mathit{mAlgebra}&(\mathsf{Error}~\mathit{msg}) & = & \mathit{throw}~(\mathsf{ErrorCall}~\mathit{msg})
    \end{array}
  \end{array}
\end{displaymath}
Thus this $\mathit{mAlgebra}$ passes normal $\mathit{IO}$ actions on,
and interprets errors by using the exception throwing facilities of
the Haskell $\mathit{IO}$ monad.

The general pattern of $m$-Eilenberg-Moore-algebras with carriers that
are themselves constructed from monads has been extensively studied by
Filinski under the name ``layered monads''
\cite{filinski99representing}. The idea is that the presence of
$m$-Eilenberg-Moore-algebras of the form $m~(m'~a) \to m'~a$, for all
$a$, capture the fact that the monad $m'$ can perform all the effects
that the monad $m$ can, so we can say that $m'$ is layered over $m$.

If we step back from considering specific monads, there are three
generic ways of making Eilenberg-Moore algebras for a given monad $m$:
\begin{enumerate}
\item The \emph{free} Eilenberg-Moore algebra for an arbitrary type
  $a$ is given by $(m~a, \mathit{join}_m)$. In terms of layered
  monads, this just states that the monad $m$ can be layered over
  itself. We will make use of this construction below in the proof of
  \thmref{thm:make-initial-f-and-m-alg} below.
\item Given an $m$-Eilenberg-Moore-algebra $(a, \mathit{mAlgebra}_a)$
  and an arbitrary type $b$, we can construct the \emph{exponential}
  $m$-Eilenberg-Moore algebra with carrier $b \to a$ and:
  \begin{displaymath}
    \begin{array}{l}
      \mathit{mAlgebra}_{b \to a} :: m~(b \to a) \to (b \to a) \\
      \mathit{mAlgebra}_{b \to a}~x~b = \mathit{mAlgebra}_a~(\mathit{fmap}_m~(\lambda f.~f~b)~x)
    \end{array}
  \end{displaymath}
  % FIXME: forward ref here if we use it
\item Given a pair of $m$-Eilenberg-Moore-algebras $(a,
  \mathit{mAlgebra}_a)$ and $(b, \mathit{mAlgebra}_b)$, we can form
  the \emph{product} $m$-Eilenberg-Moore-algebra with carrier $(a,b)$
  and:
  \begin{displaymath}
    \begin{array}{l}
      \mathit{mAlgebra}_{(a,b)} :: m~(a,b) \to (a,b) \\
      \mathit{mAlgebra}_{(a,b)}~\mathit{ab} = (\mathit{mAlgebra}_a~(\mathit{fmap}_m~\mathit{fst}~\mathit{ab}), \mathit{mAlgebra}_b~(\mathit{fmap}_m~\mathit{snd}~\mathit{ab}))
    \end{array}
  \end{displaymath}
  where $\mathit{fst}$ and $\mathit{snd}$ are the first and second
  projections from the tuple type, respectively.
\end{enumerate}

Finally in this short introduction to Eilenberg-Moore algebras, we
define homomorphisms between $m$-Eilenberg-Moore-algebras. These are
exactly the same as homomorphisms between $f$-algebras that we defined
in \autoref{sec:f-algebras}.

\begin{definition}
  An $m$-Eilenberg-Moore-algebra homomorphism
  \begin{displaymath}
    h :: (a, \mathit{mAlgebra}_a) \to (b, \mathit{mAlgebra}_b)
  \end{displaymath}
  consists of a function $h :: a \to b$ such that:
  \begin{displaymath}
    h \circ \mathit{mAlgebra}_a = \mathit{mAlgebra}_b \circ \mathit{fmap}_m~h
  \end{displaymath}
\end{definition}

\subsection{Definition of $f$-and-$m$-algebras}

As we indicated above, an $f$-and-$m$-algebra is consists of an
$f$-algebra and an $m$-Eilenberg-Moore-algebra with the same
carrier. Intuitively, the $f$-algebra part deals with the pure parts
of the structure, and the $m$-Eilenberg-Moore-algebra part deals with
the effectful parts. We require the extra structure of an
\emph{Eilenberg-Moore} algebra in order to deal with the additional
structure present in the monad $m$, as opposed to the plain functor
$f$.

\begin{definition}
  An $f$-and-$m$-algebra consists of a triple of an object $a$ and two
  functions:
  \begin{displaymath}
    \programmath
    \begin{array}{rcl}
      \mathit{fAlgebra} & :: & f~a \to a \\
      \mathit{mAlgebra} & :: & m~a \to a
    \end{array}
  \end{displaymath}
  where $\mathit{mAlgebra}$ is an $m$-Eilenberg-Moore algebra.
\end{definition}

Homomorphisms of $f$-and-$m$-algebras are single functions that are
simultaneously $f$-algebra homomorphisms and
$m$-Eilenberg-Moore-algebra homomorphisms:

\begin{definition}
  An $f$-and-$m$-algebra homomorphism
  \begin{displaymath}
    h :: (a, \mathit{fAlgebra}_a, \mathit{mAlgebra}_a) \to (b, \mathit{fAlgebra}_b, \mathit{mAlgebra}_b)
  \end{displaymath}
  between two $f$-and-$m$ algebras is nothing more than a function
  \begin{displaymath}
    h :: a \to b
  \end{displaymath}
  that satisfies the following two equations:
  \begin{displaymath}
    \begin{array}{rcl}
      h \circ \mathit{fAlgebra}_a & = & \mathit{fAlgebra}_b \circ \mathit{fmap}_f~h \\
      h \circ \mathit{mAlgebra}_a & = & \mathit{mAlgebra}_b \circ \mathit{fmap}_m~h
    \end{array}
  \end{displaymath}
\end{definition}

Given the above definitions, the definition of initial
$f$-and-$m$-algebras is straightforward, and follows the same
structure as for initial $f$-algebras. Abstractly, an initial
$f$-and-$m$-algebra is an initial object in the category of
$f$-and-$m$-algebras and $f$-and-$m$-algebra homomorphisms.

\begin{definition}
  An \emph{initial} $f$-and-$m$-algebra is an $f$-and-$m$-algebra
  $(\mu(f|m), \mathit{construct}_f, \mathit{construct}_m)$ such that
  for any other $f$-and-$m$-algebra $(a, \mathit{fAlgebra}_a,
  \mathit{mAlgebra}_a)$, there exists a \emph{unique}
  $f$-and-$m$-algebra homomorphism
  \begin{displaymath}
    \eFold{\mathit{fAlgebra}_a}{\mathit{mAlgebra}_a} :: \mu(f|m) \to a
  \end{displaymath}
\end{definition}

As for initial $f$-algebras, the requirement that an initial
$f$-and-$m$-algebra always has an $f$-and-$m$-algebra homomorphism to
any other $f$-and-$m$-algebra allows us to define functions on the
carriers of initial $f$-and-$m$-algebras. We shall see examples of
this below. The uniqueness requirement yields the following proof
principle for functions defined on initial $f$-and-$m$-algebras. It
follows the same basic form as \proofprinref{pp:initial-alg} for
initial $f$-algebras, but also includes an obligation to show that the
right-hand side of the equation to be shown is also an
$m$-Eilenberg-Moore-algebra homomorphism.

\begin{proofprinciple}[Initial $f$-and-$m$-Algebras]
  \label{pp:initial-f-m-alg}
  Suppose that $(\mu(f|m), \mathit{construct}_f,
  \mathit{construct}_m)$ is an initial $f$-and-$m$-algebra.

  Let $(a, \mathit{fAlgebra}, \mathit{mAlgebra})$ be some
  $f$-and-$m$-algebra, and let
  $\eFold{\mathit{fAlgebra}}{\mathit{mAlgebra}}$ denote the induced
  function of type $\mu(f,m) \to a$. For any function $g :: \mu(f,m)
  \to a$, we can prove the equation:
  \begin{displaymath}
    \eFold{\mathit{fAlgebra}}{\mathit{mAlgebra}} = g
  \end{displaymath}
  by demonstrating that
  \begin{displaymath}
    g \circ \mathit{construct}_f = \mathit{fAlgebra} \circ \mathit{fmap}_f
  \end{displaymath}
  and
  \begin{displaymath}
    g \circ \mathit{construct}_m = \mathit{mAlgebra} \circ \mathit{fmap}_m
  \end{displaymath}
\end{proofprinciple}

The key feature of \proofprinref{pp:initial-f-m-alg} is that it
cleanly splits the pure and effectful proof obligations. Therefore we
may use this principle to cleanly reason about programs that operate
on interleaved pure and effectful data at a high level of abstraction,
unlike the direct reasoning we attempted in
\autoref{sec:direct-eappend}. We shall see an example of this in the
next section.

\subsection{Associativity of append for interleaved effectful lists}
\label{sec:f-and-m-append}

We now revisit the problem of defining and proving associativity for
append on lists interleaved with effects that we examined in
\autoref{sec:direct-eappend}. We use the abstraction of (initial)
$f$-and-$m$-algebras, first to simplify the implementation of
$\mathit{eAppend}$, and second to simplify the proof of
associativity. In both cases, we shall see that the definition and
proof build directly upon the non-effectful case we presented in
\autoref{sec:pure-append}.

We define our function $\mathit{eAppend}$ against the abstract
interface of initial $(\mathit{ListF}~a)$-and-$m$-algebras that we
defined in the previous section. Hence we assume that an initial
$(\mathit{ListF}~a)$-and-$m$-algebra $(\mu(\mathit{ListF}~a|m),
\mathit{construct}_{\mathit{ListF}~a}, \mathit{construct}_m)$ exists,
and we denote the unique $(\mathit{ListF}~a)$-and-$m$-algebra
homomorphisms using the notation $\eFold{-}{-}$. We now define the
function $\mathit{eAppend}$:
\begin{displaymath}
  \begin{array}{l}
    \mathit{eAppend} :: \mu(\mathit{ListF}~a|m) \to \mu(\mathit{ListF}~a|m) \to \mu(\mathit{ListF}~a|m) \\
    \mathit{eAppend}~\mathit{xs}~\mathit{ys} = \eFold{\mathit{fAlgebra}}{\mathit{construct}_m}~\mathit{xs} \\
    \begin{array}{r@{\hspace{0.4em}}l@{\hspace{0.5em}}c@{\hspace{0.5em}}l}
      \textbf{where} & \mathit{fAlgebra}~\mathsf{Nil} &=& \mathit{ys} \\
                     & \mathit{fAlgebra}~(\mathsf{Cons}~a~\mathit{xs}) &=& \mathit{construct}_{\mathit{ListF}~a}~(\mathsf{Cons}~a~\mathit{xs})      
    \end{array}
  \end{array}
\end{displaymath}
Note that, unlike the direct definition of $\mathit{eAppend}$ that we
made in \autoref{sec:direct-eappend}, this definition is almost
identical to the definition of the function $\mathit{append}$ from
\autoref{sec:pure-append}, except for the additional
$m$-Eilenberg-Moore-algebra argument to $\eFold{-}{-}$ and the
different type of $\mathit{construct}_{\mathit{ListF}~a}$. The fact
that the ``pure'' part of definition (i.e.~the function
$\mathit{fAlgebra}$) is almost identical is a result of the separation
of pure and effectful concerns that the abstraction of
$f$-and-$m$-algebras affords.

Just as in the case of $\mathit{append}$, we can immediately read off
two properties of $\mathit{eAppend}$ from the fact that it is a
$(\mathit{ListF}~a)$-algebra homomorphism by construction. We have a
property for each of the constructors of the type constructor
$\mathit{ListF}~a$:
\begin{displaymath}
  \begin{array}{rcl}
    \mathit{eAppend}~(\mathit{construct}_{\mathit{ListF}~a}~\mathsf{Nil})~\mathit{ys} & = & \mathit{ys} \\
    \mathit{eAppend}~(\mathit{construct}_{\mathit{ListF}~a}~(\mathsf{Cons}~a~\mathit{xs}))~\mathit{ys} & = & \mathit{construct}_{\mathit{ListF}~a}~(\mathsf{Cons}~a~(\mathit{eAppend}~\mathit{xs}~\mathit{ys}))
  \end{array}
\end{displaymath}
In addition, we also know that $\mathit{eAppend}$ is an
$m$-Eilenberg-Moore-algebra homomorphism, again by construction. Hence
we have the following property of $\mathit{eAppend}$, for free. For
all $x :: m~(\mu(\mathit{ListF}~a|m))$:
\begin{displaymath}
  \mathit{eAppend}~(\mathit{construct}_m~\mathit{x})~\mathit{ys} = \mathit{construct}_m~(\mathit{fmap}_m~(\lambda \mathit{xs}.~\mathit{eAppend}~\mathit{xs}~\mathit{ys})~\mathit{x})
\end{displaymath}

We now make use of these three properties of $\mathit{eAppend}$ to
prove that it is associative. We use
\proofprinref{pp:initial-f-m-alg}, which splits the proof into the
pure and effectful parts. As we shall see, the pure part of the proof,
where the real work happens, is identical to the proof steps we took
in the proof of \thmref{thm:append-assoc}. The effectful parts of
the proof are straightforward, following directly from the fact that
$\mathit{eAppend}$ is an $m$-Eilenberg-Moore-algebra homomorphism.

\begin{theorem}
  For all $\mathit{xs}, \mathit{ys}, \mathit{zs} :: \mu(\mathit{ListF}~a|m)$,
  \begin{displaymath}
    \mathit{eAppend}~\mathit{xs}~(\mathit{eAppend}~\mathit{ys}~\mathit{zs}) = \mathit{eAppend}~(\mathit{eAppend}~\mathit{xs}~\mathit{ys})~\mathit{zs}
  \end{displaymath}
\end{theorem}

\begin{proof}
  The function $\mathit{eAppend}$ is defined in terms of the initial
  algebra property of $\mu(\mathit{ListF}~a|m)$, so we can apply
  \proofprinref{pp:initial-f-m-alg}. Thus we have two equations to
  prove. Firstly, for all $x ::
  \mathit{ListF}~a~(\mu(\mathit{ListF}~a|m))$, we must show that:
  \begin{displaymath}
    \begin{array}{cl}
      &\mathit{eAppend}~(\mathit{eAppend}~(\mathit{construct}_{\mathit{ListF}~a}~x)~\mathit{ys})~\mathit{zs}\\
      =&\mathit{fAlgebra}~(\mathit{fmap}_{\mathit{ListF}~a}~(\lambda \mathit{xs}.~\mathit{eAppend}~(\mathit{eAppend}~\mathit{xs}~\mathit{ys})~\mathit{zs})~x)
    \end{array}
  \end{displaymath}
  where
  \begin{displaymath}
    \begin{array}{rcl}
      \mathit{fAlgebra}~\mathsf{Nil} &=& \mathit{eAppend}~\mathit{ys}~\mathit{zs} \\
      \mathit{fAlgebra}~(\mathsf{Cons}~a~\mathit{xs}) &=& \mathit{construct}_{\mathit{ListF}~a}~(\mathsf{Cons}~a~\mathit{xs})
    \end{array}
  \end{displaymath}
  This equation is, up to renaming, \emph{exactly the same} as the
  equation we had to show in proof of
  \thmref{thm:append-assoc}. Therefore, we use the same reasoning
  steps to show this equation, relying on the first two properties of
  $\mathit{eAppend}$ that we stated above.

  Secondly, we must show that the right-hand side of the equation to
  be proved is an $m$-Eilenberg-Moore-algebra homomorphism. That is,
  we must show:
  \begin{displaymath}
    \begin{array}{cl}
      &\mathit{eAppend}~(\mathit{eAppend}~(\mathit{construct}_m~x)~\mathit{ys})~\mathit{zs} \\
      =&\mathit{construct}_m~(\mathit{fmap}_m~(\lambda \mathit{xs}.~\mathit{eAppend}~(\mathit{eAppend}~\mathit{xs}~\mathit{ys})~\mathit{zs})~x)
    \end{array}
  \end{displaymath}
  This follows straightforwardly from the fact that $\mathit{eAppend}$
  is itself an $m$-Eilenberg-Moore-algebra homomorphism, as we noted
  above, and that such homomorphisms are closed under composition:
  \begin{displaymath}
    \begin{array}{cl}
      & \mathit{eAppend}~(\mathit{eAppend}~(\mathit{construct}_m~x)~\mathit{ys})~\mathit{zs} \\
      =&\eqAnnotation{$\mathit{eAppend}$ is an $m$-Eilenberg-Moore-algebra homomorphism} \\
      & \mathit{eAppend}~(\mathit{construct}_m~(\mathit{fmap}_m~(\lambda \mathit{xs}.~\mathit{eAppend}~\mathit{xs}~\mathit{ys})~x))~\mathit{zs} \\
      =&\eqAnnotation{$\mathit{eAppend}$ is an $m$-Eilenberg-Moore-algebra homomorphism} \\
      & \mathit{construct_m}~(\mathit{fmap}_m~(\lambda \mathit{xs}.~\mathit{eAppend}~\mathit{xs}~\mathit{zs})~(\mathit{fmap}_m~(\lambda \mathit{xs}.~\mathit{eAppend}~\mathit{xs}~\mathit{ys})~x)) \\
      =&\eqAnnotation{$\mathit{fmap}_m$ preserves function composition} \\
      & \mathit{construct_m}~(\mathit{fmap}_m~(\lambda \mathit{xs}.~\mathit{eAppend}~(\mathit{eAppend}~\mathit{xs}~\mathit{ys})~\mathit{zs})~x)
    \end{array}
  \end{displaymath}
  Thus we have shown that $\mathit{eAppend}$ is associative.
\end{proof}

\subsection{Effectful list reverse?}

Given the above example of a proof of a property of a function on pure
lists carrying over almost unchanged to lists interleaved with
effects, one might wonder where this approach fails. Clearly, it
cannot be the case that all properties true for pure lists carry over
to effectful lists. One such example is the following property of the
reverse function:
\begin{displaymath}
  \mathit{reverse}~(\mathit{append}~\mathit{xs}~\mathit{ys}) = \mathit{append}~(\mathit{reverse}~\mathit{ys})~(\mathit{reverse}~\mathit{xs})
\end{displaymath}
Intuitively, this property cannot possibly hold for a reverse function
on lists interleaved with effects, since in order to reverse a list,
all of the effects inside it must be executed in order to reach the
last element and place it at the head of the new list. Thus the left
hand side of the equation above will execute all the effects of
$\mathit{xs}$ and then $\mathit{ys}$ in order, whereas the right hand
side will execute all the effects of $\mathit{ys}$ first, and then
$\mathit{xs}$. If we try to prove this property using
\proofprinref{pp:initial-f-m-alg}, we will see that we will be unable
to prove the second requirement: that the right-hand side must be an
Eilenberg-Moore-algebra homomorphism.

We can define a reverse function on effectful lists as follows. This
is very similar to the standard definition of (non-tail recursive)
reverse on pure lists, and makes use of the $\mathit{eAppend}$
function we defined above.
\begin{displaymath}
  \begin{array}{l}
    \mathit{eReverse} :: \mu(\mathit{ListF}~a|m) \to \mu(\mathit{ListF}~a|m) \\
    \mathit{eReverse} = \eFold{\mathit{fAlgebra}}{\mathit{construct}_m} \\
    \quad\begin{array}{r@{\hspace{0.4em}}l@{\hspace{0.5em}}c@{\hspace{0.5em}}l}
      \kw{where} & \mathit{fAlgebra}~\mathsf{Nil} &=& \mathit{construct}_{\mathit{ListF}~a}~\mathsf{Nil} \\
      & \mathit{fAlgebra}~(\mathsf{Cons}~a~\mathit{xs}) &=& \mathit{eAppend}~\mathit{xs}~(\mathit{construct}~(\mathsf{Cons}~a~(\mathit{construct}~\mathsf{Nil})))
    \end{array}
  \end{array}
\end{displaymath}
The ``proof'' of the property above requires a little extra step
before we can apply \proofprinref{pp:initial-f-m-alg}, because the
left-hand side of the equation is constructed from a composite of two
functions of the form $\eFold{-}{-}$. However, it is straightforward
to prove that this composite is equal to
$\eFold{\mathit{alg}}{\mathit{construct}_m}$, where
\begin{displaymath}
  \begin{array}{l}
    \mathit{alg} :: \mathit{ListF}~a~(\mu(\mathit{ListF}~a|m)) \to \mu(\mathit{ListF}~a|m) \\
    \begin{array}{@{}l@{\hspace{0.5em}}lcl}
      \mathit{alg}&\mathsf{Nil} &=& \mathit{eReverse}~\mathit{ys} \\
      \mathit{alg}&(\mathsf{Cons}~a~\mathit{xs}) & =& \mathit{eAppend}~\mathit{xs}~(\mathit{construct}~(\mathsf{Cons}~a~(\mathit{construct}~\mathsf{Nil})))
    \end{array}
  \end{array}
\end{displaymath}
Therefore, we attempt to apply \proofprinref{pp:initial-f-m-alg} to the equation
\begin{displaymath}
  \eFold{\mathit{alg}}{\mathit{construct}_m}~\mathit{xs} = \mathit{eAppend}~(\mathit{eReverse}~\mathit{ys})~(\mathit{eReverse}~\mathit{xs})
\end{displaymath}
The pure part of the proof goes through straightforwardly. We are left
with proving that the right hand side of this equation is an
Eilenberg-Moore algebra homomorphism in its second
argument. Certainly, $\mathit{eReverse}$ is an Eilenberg-Moore algebra
homomorphism by its construction via the initial $f$-and-$m$-algebra
property. However, $\mathit{eAppend}$ is not an Eilenberg-Moore
algebra homomorphism in its \emph{second} argument. Thus the proof
fails.

As a consequence, the involution property:
\begin{displaymath}
  \mathit{eReverse}~(\mathit{eReverse}~\mathit{xs}) = \mathit{xs}
\end{displaymath}
does not hold in the presence of arbitrary interleaved
effects. Intuitively, it is again clear why: the expression on the
left pushes all the effects to the start of the list as
$\mathit{eReverse}$ traverses the list, while the right-hand side
leaves the effects interspersed between the elements.


% \subsection{When not to use $f$-and-$m$-algebras}

% Printing out an annotated list

\subsection{From initial $(f \circ m)$-algebras to initial $f$-and-$m$-algebras}

We now show that initial $f$-and-$m$-algebras can be constructed from
initial $(f \circ m)$-algebras. If the type $\mu(f \circ m)$ the
carrier of an initial $(f \circ m)$-algebra, then the initial
$f$-and-$m$-algebra that we construct has carrier $m~(\mu (f \circ
m))$. This is exactly following the general construction of inductive
data-types with interleaved data and effects that we discussed in the
introduction.

\begin{theorem}\label{thm:make-initial-f-and-m-alg}
  Let $(f, \mathit{fmap}_f)$ be a functor, and $(m, \mathit{fmap}_m,
  \mathit{return}_m, \mathit{join}_m)$ be a monad.  If we have an
  initial $(f \circ m)$-algebra $(\mu(f \circ m),
  \mathit{construct})$, then $m~(\mu(f \circ m))$ is the carrier of an
  initial $f$-and-$m$-algebra.
\end{theorem}

\begin{proof*}
  As we mentioned above, the carrier of the $f$-and-$m$-algebra is
  $m~(\mu(f \circ m))$. The $f$-algebra and
  $m$-Eilenberg-Moore-algebra structure are constructed from the $(f
  \circ m)$-algebra structure map $\mathit{construct}$, and the
  structure of the monad $m$.  For the $f$-algebra component, we use
  the composite:
  \begin{displaymath}
    \mathit{construct}_f = \mathit{return}_m \circ \mathit{construct} :: f~(m~(\mu(f \circ m))) \to m~(\mu(f \circ m))
  \end{displaymath}
  The $m$-Eilenberg-Moore-algebra component is straightforward, using
  the free Eilenberg-Moore-algebra construction from
  \autoref{sec:eilenberg-moore-algebras}:
  \begin{displaymath}
    \mathit{construct}_m = \mathit{join}_m :: m~(m~(\mu(f \circ m))) \to m~(\mu(f \circ m))
  \end{displaymath}
  Since we have used the free Eilenberg-Moore-algebra construction, we
  are automatically guaranteed that we have an
  $m$-Eilenberg-Moore-algebra.

  Now let us assume we are given an $f$-and-$m$-algebra $(a,
  \mathit{fAlgebra}_a, \mathit{mAlgebra}_a)$. We construct, and prove
  unique, an $f$-and-$m$-algebra homomorphism $h$ from the algebra
  $(m~(\mu(f \circ m)), \mathit{construct}_f, \mathit{construct}_m)$
  to the algebra with carrier $a$ using the initiality of $\mu(f \circ
  m)$:
  \begin{displaymath}
    h = \mathit{mAlgebra}_a \circ \mathit{fmap}_m~\fold{\mathit{fAlgebra}_a \circ \mathit{fmap}_f~\mathit{mAlgebra}_a} :: m~(\mu(f \circ m)) \to a
  \end{displaymath}
  To complete our proof, we now need to demonstrate that $h$ is an
  $f$-and-$m$-algebra homomorphism, and that it is the unique such. We
  split this task into three steps:
  \begin{enumerate}
  \item The function $h$ is an $f$-algebra homomorphism. We reason as
    follows:
    \begin{displaymath}
      \begin{array}{cl}
        & h \circ \mathit{construct}_f \\
        =&\eqAnnotation{definitions of $h$ and $\mathit{construct_f}$} \\
         &\mathit{mAlgebra}_a \circ \mathit{fmap}_m~\fold{\mathit{fAlgebra}_a \circ \mathit{fmap}_f~\mathit{mAlgebra}_a} \circ \mathit{return}_m \circ \mathit{construct} \\
        =&\eqAnnotation{naturality of $\mathit{return}_m$} \\
         &\mathit{mAlgebra}_a \circ \mathit{return}_m \circ \fold{\mathit{fAlgebra}_a \circ \mathit{fmap}_f~\mathit{mAlgebra}_a} \circ \mathit{construct} \\
        =&\eqAnnotation{$\mathit{mAlgebra}_a$ is an Eilenberg-Moore-algebra} \\
         &\fold{\mathit{fAlgebra}_a \circ \mathit{fmap}_f~\mathit{mAlgebra}_a} \circ \mathit{construct} \\
        =&\eqAnnotation{$\fold{-}$ is an $(f \circ m)$-algebra homomorphism} \\
         &\mathit{fAlgebra}_a \circ \mathit{fmap}_f~\mathit{mAlgebra}_a \circ \mathit{fmap}_f~\fold{\mathit{fAlgebra}_a \circ \mathit{fmap}_f~\mathit{mAlgebra}_a} \\
        =&\eqAnnotation{$\mathit{fmap}_f$ preserves function composition} \\
         &\mathit{fAlgebra}_a \circ \mathit{fmap}_f~(\mathit{mAlgebra}_a \circ \fold{\mathit{fAlgebra}_a \circ \mathit{fmap}_f~\mathit{mAlgebra}_a}) \\
        =&\eqAnnotation{definition of $h$} \\
         & \mathit{fAlgebra}_a \circ \mathit{fmap}_f~h
      \end{array}
    \end{displaymath}
  \item The function $h$ is an $m$-Eilenberg-Moore-algebra
    homomorphism, as shown by the following steps:
    \begin{displaymath}
      \begin{array}{cl}
        & h \circ \mathit{construct_m} \\
        =&\eqAnnotation{definitions of $h$ and $\mathit{construct}_m$} \\
         &\mathit{mAlgebra}_a \circ \mathit{fmap}_m~\fold{\mathit{fAlgebra}_a \circ \mathit{fmap}_f~\mathit{mAlgebra}_a} \circ \mathit{join}_m \\
        =&\eqAnnotation{naturality of $\mathit{join}_m$} \\
         &\mathit{mAlgebra}_a \circ \mathit{join}_m \circ \mathit{fmap}_m~(\mathit{fmap}_m~\fold{\mathit{fAlgebra}_a \circ \mathit{fmap}_f~\mathit{mAlgebra}_a}) \\
        =&\eqAnnotation{$\mathit{mAlgebra}_a$ is an Eilenberg-Moore algebra} \\
         &\mathit{mAlgebra}_a \circ \mathit{fmap}_m~\mathit{mAlgebra}_a \circ \mathit{fmap}_m~(\mathit{fmap}_m~\fold{\mathit{fAlgebra}_a \circ \mathit{fmap}_f~\mathit{mAlgebra}_a}) \\
        =&\eqAnnotation{$\mathit{fmap}_m$ preserves function composition} \\
         &\mathit{mAlgebra}_a \circ \mathit{fmap}_m~(\mathit{mAlgebra}_a \circ \mathit{fmap}_m~\fold{\mathit{fAlgebra}_a \circ \mathit{fmap}_f~\mathit{mAlgebra}_a}) \\
        =&\eqAnnotation{definition of $h$} \\
         &\mathit{mAlgebra}_a \circ \mathit{fmap}_m~h
      \end{array}
    \end{displaymath}
  \item The function $h$ is the unique such $f$-and-$m$-algebra. Let
    us assume that there exists another $f$-and-$m$-algebra
    homomorphism $h' :: m~(\mu(f \circ m)) \to a$. We aim to show that
    $h = h'$. We first observe that the function:
    \begin{displaymath}
      h' \circ \mathit{return}_m :: \mu(f \circ m) \to a
    \end{displaymath}
    obtained by composition is an $(f \circ m)$-algebra homomorphism
    from $(\mu(f \circ m), \mathit{construct})$ to $(a, \mathit{fAlgebra}_a
    \circ \mathit{fmap}_f~\mathit{mAlgebra}_a)$, as verified by the
    following steps:
    \begin{displaymath}
      \begin{array}{cl}
        & h' \circ \mathit{return}_m \circ \mathit{construct} \\
        =&\eqAnnotation{definition of $\mathit{fAlgebra}$} \\
        & h' \circ \mathit{construct}_f \\
        =&\eqAnnotation{$h'$ is an $f$-and-$m$-algebra homomorphism} \\
        & \mathit{fAlgebra}_a \circ \mathit{fmap}_f~h' \\
        =&\eqAnnotation{monad law: $\mathit{join}_m \circ \mathit{return}_m = \mathit{id}$} \\
        & \mathit{fAlgebra}_a \circ \mathit{fmap}_f~(h' \circ \mathit{join}_m \circ \mathit{return}_m) \\
        =&\eqAnnotation{$h'$ is an $m$-Eilenberg-Moore-algebra homomorphism} \\
        & \mathit{fAlgebra}_a \circ \mathit{fmap}_f~(\mathit{mAlgebra}_a \circ h' \circ \mathit{return}_m) \\
        =&\eqAnnotation{$\mathit{fmap}_f$ preserves function composition} \\
        & \mathit{fAlgebra}_a \circ \mathit{fmap}_f~\mathit{mAlgebra}_a \circ \mathit{fmap}_f~(h' \circ \mathit{return}_m)
      \end{array}
    \end{displaymath}
    Thus, by the uniqueness of $(f \circ m)$-algebra homomorphisms out
    of $\mu(f \circ m)$, we have proved that $h' \circ
    \mathit{return}_m = \fold{\mathit{fAlgebra}_a \circ
      \mathit{fmap}_f~\mathit{mAlgebra}_a}$. We now use this equation
    to prove that $h=h'$ by the following steps:
    \begin{displaymath}
      \begin{array}{cl}
        & h \\
        =&\eqAnnotation{definition of $h$} \\
        &\mathit{mAlgebra}_m \circ \mathit{fmap}_m~\fold{\mathit{fAlgebra}_a \circ \mathit{fmap}_f~\mathit{mAlgebra}_a} \\
        =&\eqAnnotation{$h' \circ \mathit{return}_m$ is an $(f \circ m)$-algebra homomorphism} \\
        &\mathit{mAlgebra}_a \circ \mathit{fmap}_m~(h' \circ \mathit{return}_m) \\
        =&\eqAnnotation{$\mathit{fmap}_m$ preserves function composition} \\
        &\mathit{mAlgebra}_a \circ \mathit{fmap}_m~{h'} \circ \mathit{fmap}_m~\mathit{return}_m \\
        =&\eqAnnotation{$h'$ is an $m$-Eilenberg-Moore-algebra homomorphism} \\
        &h' \circ \mathit{join}_m \circ \mathit{fmap}_m~\mathit{return}_m \\
        =&\eqAnnotation{monad law: $\mathit{join}_m \circ \mathit{fmap}_m~\mathit{return}_m = \mathit{id}$} \\
        &h'
      \end{array}
    \end{displaymath}
    Thus $h$ is the unique $f$-and-$m$-algebra homomorphism from
    $m~(\mu (f \circ m))$ to $a$. \mathproofbox
  \end{enumerate}
\end{proof*}

In the current authors' previous work \cite{atkey12fibrational}, this
same result was obtained in a less elementary way by constructing a
functor $\Phi$ from the category of $(f \circ m)$-algebras to the
category of $f$-and-$m$-algebras. The functor $\Phi$ was shown to be a
left adjoint, and since left adjoints preserve initial objects, $\Phi$
maps any initial $(f \circ m)$-algebra to an initial
$f$-and-$m$-algebra.

% FIXME: mention Filinski and Stvring's construction?

\subsection{Implementation of initial $f$-and-$m$-algebras in Haskell}
\label{sec:f-and-m-alg-impl}

\newcommand{\fcompose}{\mathop{\mathord:\circ\mathord:}}

In light of \thmref{thm:make-initial-f-and-m-alg}, we can take the
Haskell implementation of initial $f$-algebras from
\autoref{sec:initial-f-alg-impl} and apply the construction in the
theorem to construct an initial $f$-and-$m$-algebra. We do have to
make a small alteration to satisfy Haskell's type checker, however.

The seed of our construction is the existence of an initial $(f \circ
m)$-algebra. Therefore, we need to first construct the composite
functor $f \circ m$. Since Haskell does not have type-level
$\lambda$-abstraction or application, there is no lightweight way of
constructing the composite of two functors' type operator
components. Thus to express the composition of two type operators as
a new type operator, we must introduce a $\kw{newtype}$, as
follows\footnote{This definition requires the GHC extension
  \texttt{-XTypeOperators} to be turned on, allowing infix type
  constructors.}:
\begin{displaymath}
  \kw{newtype}~(f \fcompose g)~a = \mathsf{C}~\{\mathit{unC} :: f~(g~a) \}
\end{displaymath}
We define $\mathit{fmap}_{f\fcompose g}$ straightforwardly in terms of
$\mathit{fmap}_f$ and $\mathit{fmap}_g$.

Following the proof of \thmref{thm:make-initial-f-and-m-alg}, the
carrier of our initial $f$-and-$m$-algebra is:
\begin{displaymath}
  \kw{type}~\mathit{MuFM}~f~m = m~(\mathit{Mu}~(f \fcompose m))
\end{displaymath}
with the $f$-algebra and $m$-Eilenberg-Moore-algebra structure maps
defined following the construction in
\thmref{thm:make-initial-f-and-m-alg}, augmented with a use of the
value constructor $\mathsf{C}$ in order to satisfy the type checker:
\begin{displaymath}
  \begin{array}{l}
    \mathit{construct}_f :: f~(\mathit{MuFM}~f~m) \to \mathit{MuFM}~f~m \\
    \mathit{construct}_f = \mathit{return}_m \circ \mathit{construct} \circ \mathsf{C} \\
    \\
    \mathit{construct}_m :: m~(\mathit{MuFM}~f~m) \to \mathit{MuFM}~f~m \\
    \mathit{construct}_m = \mathit{join}_m
  \end{array}
\end{displaymath}

Finally, we construct the unique $f$-and-$m$-homomorphism out of
$\mathit{MuFM}~f~m$ following the proof of
\thmref{thm:make-initial-f-and-m-alg} by building upon our
implementation of the unique homomorphisms out of the initial $(f
\fcompose m)$-algebra, albeit again augmented with a coercion
$\mathit{unC}$ to satisfy the type checker:
\begin{displaymath}
  \begin{array}{l}
    \eFold{-}{-} :: (f~a \to a) \to (m~a \to a) \to \mathit{MuFM}~f~m \to a \\
    \eFold{\mathit{fAlgebra}}{\mathit{mAlgebra}} = \mathit{mAlgebra} \circ \mathit{fmap}_m~\fold{\mathit{fAlgebra} \circ \mathit{fmap}_f~\mathit{mAlgebra} \circ \mathit{unC}}
  \end{array}
\end{displaymath}
We can also implement $\eFold{-}{-}$ directly in terms of Haskell's
general recursion, just as we did for the implementation of
$\fold{-}$. This definition arise simply by inlining the
implementation of $\fold{-}$ into the definition of $\eFold{-}{-}$
above, and performing some simple rewriting. The direct implementation
of $\eFold{-}{-}$ is as follows:
\begin{displaymath}
  \begin{array}{l}
    \eFold{-}{-} :: (f~a \to a) \to (m~a \to a) \to \mathit{MuFM}~f~m \to a \\
    \eFold{\mathit{fAlgebra}}{\mathit{mAlgebra}} = \mathit{mAlgebra} \circ \mathit{fmap}_m~\mathit{loop} \\
    \quad\begin{array}{r@{\hspace{0.4em}}l@{\hspace{0.5em}}c@{\hspace{0.5em}}l}
      \kw{where} & \mathit{loop} &=& \mathit{fAlgebra} \circ \mathit{fmap}_f~\mathit{mAlgebra} \circ \mathit{fmap}_f~(\mathit{fmap}_m~\mathit{loop}) \circ \mathit{unC} \circ \mathit{unIn}
    \end{array}
  \end{array}
\end{displaymath}

Whichever implementation of $\eFold{-}{-}$ we choose, we note that
there is an implicit precondition that the second argument (of type
$m~a \to a$) must be an Eilenberg-Moore algebra. Unfortunately, we are
unable to express this requirement in Haskell's type system. 

\bibliographystyle{jfp}
\bibliography{paper}

\newpage
\appendix

\section{Coproducts with Free Monads}

Reproduce the result of Hyland \emph{et al.}

\begin{enumerate}
\item Define what the free monad is for all functors $f$
\item Prove that it really is a monad
\item Define what the sum of two monads is
\item Show that the initial $(f + a)$-and-$m$-algebra is the sum of
  the free $f$-monad and $m$
\end{enumerate}

\subsection{Free Monads}

\begin{definition}[Free Monad]
  Let $(f, \mathit{fmap}_f)$ be a functor. A free monad on
  $(f,\mathit{fmap}_f)$ is a monad
  \begin{displaymath}
    (\mathit{FreeM}~f, \mathit{return}_{\mathit{FreeM}~f}, \mbind_{\mathit{FreeM}~f})
  \end{displaymath}
  such that for every monad $(m, \mathit{return_m}, \mbind_m)$ and
  natural transformation $g :: f~a \to m~a$, there is a unique monad
  morphism $h :: \mathit{FreeM}~f~a \to m~a$.
\end{definition}

Let $f$ be a functor, and define:
\begin{displaymath}
    \begin{array}{l}
      \kw{data}~\mathit{FreeMF}~f~a~x \\
      \quad\begin{array}{cl}
        = & \mathsf{Var}~a \\
        | & \mathsf{Term}~(f~x)
      \end{array} \\
      \\
      \mathit{fmap}_{\mathit{FreeMF}} :: (x \to y) \to \mathit{FreeMF}~f~a~x \to \mathit{FreeMF}~f~a~y \\
      \begin{array}{@{}l@{\hspace{0.4em}}l@{\hspace{0.4em}}lcl}
        \mathit{fmap}_{\mathit{FreeMF}} & g & (\mathsf{Var}~a) & = & \mathsf{Var}~a \\
        \mathit{fmap}_{\mathit{FreeMF}} & g & (\mathsf{Term}~\mathit{fx}) & = & \mathsf{Term}~(\mathit{fmap}_f~g~\mathit{fx})
    \end{array}
  \end{array}
\end{displaymath}

\begin{theorem}
  Let $(f, \mathit{fmap}_f)$ be a functor, and suppose that an initial
  $(\mathit{FreeMF}~f~a)$-algebra $(\mu (\mathit{FreeMF}~f~a),
  \mathit{construct})$ exists for all $a$. Let:
  \begin{displaymath}
    \kw{type}~\mathit{FreeM}~f~a = \mu (\mathit{FreeMF}~f~a)
  \end{displaymath}
  Then the following definitions witness $\mathit{FreeF}~f$ as a monad:
  \begin{displaymath}
    \begin{array}{l}
      \mathit{return}_{\mathit{FreeM}~f} :: a \to \mathit{FreeM}~f~a \\
      \mathit{return}_{\mathit{FreeM}~f}~a = \mathit{construct}~(\mathsf{Var}~a) \\
      \\
      \mbind_{\mathit{FreeM}~f} :: \mathit{FreeM}~f~a \to (a \to \mathit{FreeM}~f~b) \to \mathit{FreeM}~f~b \\
      c \mbind_{\mathit{FreeM}~f} g = \fold{\mathit{fAlgebra}}~c\\
      \quad\begin{array}{r@{\hspace{0.4em}}l@{\hspace{0.5em}}c@{\hspace{0.5em}}l}
        \kw{where} & \mathit{fAlgebra}~(\mathsf{Var}~a) &=& g~a \\
        & \mathit{fAlgebra}~(\mathsf{Term}~x)&=&\mathit{construct}~(\mathsf{Term}~x)
      \end{array}
    \end{array}
  \end{displaymath}
  Moreover, $\mathit{FreeM}~f$ is the free monad on the functor $f$.
\end{theorem}

Before we embark on the proof of this theorem, we first observe two
properties of $\mbind_{\mathit{FreeM}~f}$ that follow directly from
its definition in terms of an initial algebra:
\begin{displaymath}
  \begin{array}{rcl}
    \mathit{construct}~(\mathsf{Var}~a) \mbind_{\mathit{FreeM}~f} g & = & g~a \\
    \mathit{construct}~(\mathsf{Term}~\mathit{fx}) \mbind_{\mathit{FreeM}~f} g & = & \construct~(\mathsf{Term}~(\mathit{fmap}_f~(\lambda c.~c\mbind_{\mathit{FreeM}~f}g)~\mathit{fx})
  \end{array}
\end{displaymath}

\begin{proof}
  We first demonstrate that $\mathit{FreeF}~f$ is a monad with the
  given definitions of $\mathit{return}$ and $\mbind$, by checking the
  three required properties (FIXME: from defn X):
  \begin{enumerate}
  \item The first property---that $\mathit{return}_{\mathit{FreeM}~f}$
    is a left unit---is a direct consequence of the first property of
    $\mbind_{\mathit{FreeM}~f}$ that we noted above:
    \begin{displaymath}
      \begin{array}{cl}
        & \mathit{return}_{\mathit{FreeM}~f}~a \mbind_{\mathit{FreeM}~f} g \\
        =&\eqAnnotation{definition of $\mathit{return}_{\mathit{FreeM}~f}$} \\
        & \mathit{construct}~(\mathsf{Var}~a) \mbind_{\mathit{FreeM}~f} g \\
        =&\eqAnnotation{first property of $\mbind_{\mathit{FreeM}~f}$} \\
        & g~a
      \end{array}
    \end{displaymath}
  \item The second property---that
    $\mathit{return}_{\mathit{FreeM}~f}$ is a right unit---is stated
    as follows:
    \begin{displaymath}
      c \mbind_{\mathit{FreeM}~f} \mathit{return} = c
    \end{displaymath}
    Unfolding the definition of $\mbind_{\mathit{FreeM}~f}$ yields:
    \begin{displaymath}
      \fold{\mathit{fAlgebra_{return}}}~c = c
    \end{displaymath}
    Following \proofprinref{pp:initial-alg}, we can show this equation
    by proving, for all $x ::
    \mathit{FreeMF}~f~a~(\mathit{FreeM}~f~a)$, the following:
    \begin{displaymath}
      \mathit{construct}~x = \mathit{fAlgebra_{return}}~x
    \end{displaymath}
    This task splits into two cases, depending on whether $x =
    \mathsf{Var}~a$ or $x =\mathsf{Term}~\mathit{fx}$. In the former
    case we reason as follows (from right to left):
    \begin{displaymath}
      \begin{array}{cl}
        & \mathit{fAlgebra_{return}}~(\mathsf{Var}~a) \\
        =&\eqAnnotation{definition of $\mathit{fAlgebra_{return}}$} \\
        & \mathit{return}_{\mathit{FreeM}~f}~a \\
        =&\eqAnnotation{definition of $\mathit{return}_{\mathit{FreeM}~f}$} \\
        & \mathit{construct}~(\mathsf{Var}~a)
      \end{array}
    \end{displaymath}
    In the latter case, when $x = \mathsf{Term}~\mathit{fx}$, the
    equation to prove follows directly from the definition of
    $\mathit{fAlgebra_{return}}$:
    \begin{displaymath}
      \mathit{fAlgebra}_{\mathit{return}}~(\mathsf{Term}~\mathit{fx}) = \mathit{construct}~(\mathsf{Term}~\mathit{fx})
    \end{displaymath}

  \item Finally, we prove the associativity property of
    $\mbind_{\mathit{FreeM}~f}$:
    \begin{displaymath}
      c \mbind_{\mathit{FreeM}~f} (\lambda x.~g_1~x \mbind_{\mathit{FreeM}~f} g_2) = (c \mbind_{\mathit{FreeM}~f} g_1) \mbind_{\mathit{FreeM}~f} g_2
    \end{displaymath}
    Unfolding the leftmost occurrence of $\mbind_{\mathit{FreeM}~f}$
    yields the following equation to be proved:
    \begin{displaymath}
      \fold{\mathit{fAlgebra}_{\lambda x.~g_1~x \mbind_{\mathit{FreeM}~f} g_2}}~c = (c \mbind_{\mathit{FreeM}~f} g_1) \mbind_{\mathit{FreeM}~f} g_2
    \end{displaymath}
    We now apply \proofprinref{pp:initial-alg}. Thus we need to show
    the following equation for all $x ::
    \mathit{FreeMF}~f~a~(\mathit{FreeM}~f~a)$:
    \begin{displaymath}
      \begin{array}{cl}
        &(\mathit{construct}~x \mbind_{\mathit{FreeM}~f} g_1) \mbind_{\mathit{FreeM}~f} g_2\\
        = &\mathit{fAlgebra}_{\lambda x.~g_1~x \mbind_{\mathit{FreeM}~f} g_2}~(\mathit{fmap}_{\mathit{FreeMF}~f~a}~(\lambda c.~(c \mbind_{\mathit{FreeM}~f} g_1) \mbind_{\mathit{FreeM}~f} g_2)~x)
      \end{array}
    \end{displaymath}
    As above, this task splits into two, depending on whether $x =
    \mathsf{Var}~a$ or $x = \mathsf{Term}~\mathit{fx}$. In the former
    case, we reason as follows:
    \begin{displaymath}
      \begin{array}{cl}
        & (\mathit{construct}~(\mathsf{Var}~a) \mbind_{\mathit{FreeM}~f} g_1) \mbind_{\mathit{FreeM}~f} g_2\\
        =&\eqAnnotation{first property of $\mbind_{\mathit{FreeM}~f}$} \\
        & g_1~a \mbind_{\mathit{FreeM}~f} g_2 \\
        =&\eqAnnotation{definition of $\mathit{fAlgebra}_{\lambda x.~g_1~x \mbind_{\mathit{FreeM}~f} g_2}$}\\
        & \mathit{fAlgebra}_{\lambda x.~g_1~x \mbind_{\mathit{FreeM}~f} g_2}~(\mathsf{Var}~a) \\
        =&\eqAnnotation{definition of $\mathit{fmap}_{\mathit{FreeMF}~f}$} \\
        & \mathit{fAlgebra}_{\lambda x.~g_1~x \mbind_{\mathit{FreeM}~f} g_2}~(\mathit{fmap}_{\mathit{FreeMF}~f}~(\lambda c.~(c \mbind_{\mathit{FreeM}~f} g_1) \mbind_{\mathit{FreeM}~f} g_2)~(\mathsf{Var}~a))
      \end{array}
    \end{displaymath}
    In the latter case, when $x = \mathsf{Term}~\mathit{fx}$, we
    reason as follows:
    \begin{displaymath}
      \begin{array}{cl}
        & (\mathit{construct}~(\mathsf{Term}~\mathit{fx}) \mbind_{\mathit{FreeM}~f} g_1) \mbind_{\mathit{FreeM}~f} g_2\\
        =&\eqAnnotation{second property of $\mbind_{\mathit{FreeM}~f}$} \\
        & \mathit{construct}~(\mathsf{Term}~(\mathit{fmap}_f~(\lambda c.~c \mbind_{\mathit{FreeM}~f} g_1)~\mathit{fx})) \mbind_{\mathit{FreeM}~f} g_2 \\
        =&\eqAnnotation{second property of $\mbind_{\mathit{FreeM}~f}$} \\
        & \mathit{construct}~(\mathsf{Term}~(\mathit{fmap_f}~(\lambda c.~c \mbind_{\mathit{FreeM}~f} g_2)~(\mathit{fmap_f}~(\lambda c.~c \mbind_{\mathit{FreeM}~f} g_1)~\mathit{fx}))) \\
        =&\eqAnnotation{$\mathit{fmap}_f$ preserves function composition} \\
        & \mathit{construct}~(\mathsf{Term}~(\mathit{fmap_f}~(\lambda c.~(c \mbind_{\mathit{FreeM}~f} g_1) \mbind_{\mathit{FreeM}~f} g_2)~\mathit{fx})) \\
        =&\eqAnnotation{definition of $\mathit{fAlgebra}_{\lambda x.~(g_1~x \mbind_{\mathit{FreeM}~f} g_1)}$} \\
        & \mathit{fAlgebra}_{\lambda x.~g_1~x \mbind_{\mathit{FreeM}~f} g_2}~(\mathsf{Term}~(\mathit{fmap_f}~(\lambda c.~(c \mbind_{\mathit{FreeM}~f} g_1) \mbind_{\mathit{FreeM}~f} g_2)~\mathit{fx})) \\
        =&\eqAnnotation{definition of $\mathit{fmap}_{\mathit{FreeMF}~f~a}$} \\
        & \mathit{fAlgebra}_{\lambda x.~g_1~x \mbind_{\mathit{FreeM}~f} g_2}~(\mathit{fmap}_{\mathit{FreeMF}~f~a}~(\lambda c.~(c \mbind_{\mathit{FreeM}~f} g_1) \mbind_{\mathit{FreeM}~f} g_2)~(\mathsf{Term}~\mathit{fx}))
      \end{array}
    \end{displaymath}
  \end{enumerate}
  We now demonstrate the free monad property. Let $g :: f~a \to m~a$
  be a natural transformation. We define:
  \begin{displaymath}
    \begin{array}{l}
      h :: \mathit{FreeM}~f~a \to m~a \\
      h = \fold{alg} where alg (var a) = return a, alg (term fx) = join (g fx)
    \end{array}
  \end{displaymath}

  \begin{enumerate}
  \item $h$ is a monad morphism
  \item 
  \end{enumerate}
\end{proof}

\subsection{Coproducts of Monads}

\newcommand{\cprd}[2]{#1\mathord{+}#2}

\begin{definition}
  Let $(m_1, \mathit{return}_{m_1}, \mbind_{m_1})$ and $(m_2,
  \mathit{return}_{m_2}, \mbind_{m_2})$ be a pair of monads in
  Kleisli-triple form. A \emph{coproduct} of these two monads is a
  monad $(\cprd{m_1}{m_2}, \mathit{return}_{\cprd{m_1}{m_2}},
  \mbind_{\cprd{m_1}{m_2}})$ along with a pair of monad morphisms:
  \begin{displaymath}
    \begin{array}{rcl}
      \mathit{inj}_1 & :: & m_1~a \to (\cprd{m_1}{m_2})~a \\
      \mathit{inj}_2 & :: & m_1~a \to (\cprd{m_1}{m_2})~a
    \end{array}
  \end{displaymath}
  and the property that for any monad $(m,\mathit{return}_m,
  \mbind_m)$ and pair of monad morphisms $g_1 : m_1~a \to m~a$ and
  $g_2 : m_2~a \to m~a$ there is a \emph{unique} monad morphism $[g_1,g_2] :
  (\cprd{m_1}{m_2})~a \to m~a$ such that
  \begin{displaymath}
    \begin{array}{rcl}
      {}[g_1,g_2] \circ \mathit{inj}_1 & = & g_1 \\
      {}[g_1,g_2] \circ \mathit{inj}_2 & = & g_2
    \end{array}
  \end{displaymath}
\end{definition}

\subsection{Coproduct of a Free Monad with an Arbitrary Monad}

The coproduct of $\mathit{FreeM}~f$ and $m$ is the initial
$\mathit{FreeMF}~f~a$-and-$m$-algebra (FIXME: pointwise, indexed blah).

Define $\mathit{CoprodM}~f~m~a$

Define:
\begin{displaymath}
  \begin{array}{l}
  \mathit{inj_1} :: \mathit{FreeM}~f~a \to \mathit{CoprodM}~f~m~a \\
  \mathit{inj_1}~c = \fold{\mathit{fAlgebra}}~c \\
  \quad\begin{array}{r@{\hspace{0.4em}}l@{\hspace{0.5em}}c@{\hspace{0.5em}}l}
    \kw{where}&\mathit{fAlgebra}~(\mathsf{Var}~a) &=& \mathit{construct}~(\mathsf{Var}~a) \\
    &\mathit{fAlgebra}~(\mathsf{Term}~\mathit{fx}) &=& \mathit{construct}~(\mathsf{Term}~\mathit{fx})
  \end{array}
\end{array}
\end{displaymath}

FIXME: $\mathit{inj}_2$

% WRONG
% \begin{displaymath}
%   \begin{array}{l}
%     {}[-,-] :: (\forall a.~\mathit{FreeM}~f~a \to m'~a) \to (\forall a.~m~a \to m'~a) \to \mathit{CoprodM}~f~m~a \to m'~a \\
%     {}[g_1,g_2] = \eFold{\mathit{fAlgebra}}{\mathit{mAlgebra}}\\
%     \quad\begin{array}{r@{\hspace{0.4em}}l@{\hspace{0.5em}}c@{\hspace{0.5em}}l}
%       \kw{where}&\mathit{fAlgebra}~(\mathsf{Var}~a) &=& \mathit{construct}~(\mathsf{Var}~a) \\
%       &\mathit{fAlgebra}~(\mathsf{Term}~\mathit{fx}) &=& \mathit{construct}~(\mathsf{Term}~\mathit{fx})
%     \end{array}
%   \end{array}
% \end{displaymath}

To prove:
\begin{enumerate}
\item $\mathit{CoprodM}~f~m$ with the above stuff is actually a monad:
  this should go through exactly as for the non-interleaved case.
\item $\mathit{inj}_1$ is a monad morphism
\item $\mathit{inj}_2$ is a monad morphism
\item The universal map exists, is a monad morphism, and is unique
\end{enumerate}

% \section{Streams, Transformers and Readers}

% Longer worked example, and on-the-way develop the theory of effectful
% paramorphisms.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% The arrow $\eFold{f}{g} : T(\mu FT) \to A$ satisfies the following two
% properties:
% \begin{displaymath}
%   \begin{array}{c@{\hspace{3em}}c}
%     \xymatrix{
%       {F(T(\mu FT))}
%       \ar[r]^(.6){F\eFold{f}{g}}
%       \ar[d]_{\eta \circ \construct}
%       &
%       {FA}
%       \ar[d]^{f}
%       \\
%       {T(\mu FT)}
%       \ar[r]^(.6){\eFold{f}{g}}
%       &
%       {A}
%     }
%     &
%     \xymatrix{
%       {T(T(\mu FT))}
%       \ar[r]^(.6){T\eFold{f}{g}}
%       \ar[d]_\mu
%       &
%       {TA}
%       \ar[d]^{g}
%       \\
%       {T(\mu FT)}
%       \ar[r]^(.6){\eFold{f}{g}}
%       &
%       {A}
%     }
%   \end{array}
% \end{displaymath}
% and is the unique arrow to do so.

% \section{Some Examples}

% \paragraph{Non-effectful Lists}

% Do a proof of the associativity of append for normal (finite) lists,
% for the purposes of comparison to the proof for effectful lists.

% \paragraph{Effectful Lists}

% Let $FX = 1 + A \times X$. We define a function $\mathit{append} :
% T(\mu FT) \times T(\mu FT) \to T(\mu FT)$ as follows:
% \begin{displaymath}
%   \begin{array}{l}
%     \mathit{append}\ (\mathit{xs}, \mathit{ys}) = \eFold{f}{\mu}\ \mathit{xs} \\
%     \quad \mathbf{where} \\
%     \quad\quad f\ \mathsf{Nil} = \mathit{ys} \\
%     \quad\quad f\ (\mathsf{Cons}\ a\ l) = \eta\ (\construct\ (\mathsf{Cons}\ a\ l))
%   \end{array}
% \end{displaymath}

% Directly from the definition of $\mathit{append}$, we have the
% following two equational properties, derived directly from the first
% property of our effectful recursion scheme $\eFold{-}{-}$:
% \begin{displaymath}
%   \begin{array}{rcl}
%     \mathit{append}\ (\eta\ (\construct\ \mathsf{Nil}), \mathit{ys}) &=& \mathit{ys} \\
%     \mathit{append}\ (\eta\ (\construct\ (\mathsf{Cons}\ a\ \mathit{xs})), \mathit{ys}) &=& \eta\ (\construct\ (\mathsf{Cons}\ a\ (\mathit{append}\ (\mathit{xs}, \mathit{ys}))))
%   \end{array}
% \end{displaymath}
% We also have the following property, derived directly from the second
% property of our effectful recursion scheme:
% \begin{displaymath}
%   \begin{array}{rcl}
%     \mathit{append}\ (\mu\ \mathit{x}, \mathit{ys}) &=& \mu\ (T\ (\lambda\mathit{xs}.\ \mathit{append}\ (\mathit{xs}, \mathit{ys}))\ \mathit{x})
%   \end{array}
% \end{displaymath}
% We will use these three properties to prove the following lemma:

% \begin{lemma}
%   For all $\mathit{xs}, \mathit{ys}, \mathit{zs} : T(\mu FT)$,
%   \begin{displaymath}
%     \mathit{append}\ (\mathit{xs}, \mathit{append}\ (\mathit{ys}, \mathit{zs})) = \mathit{append}\ (\mathit{append}\ (\mathit{xs}, \mathit{ys}), \mathit{zs})
%   \end{displaymath}
% \end{lemma}

% \begin{proof}
%   We make use of the uniqueness of functions defined using the
%   effectful recursion scheme... FIXME

%   Need to prove that the right-hand side satisfies the two
%   properties. The first property is:
%   \begin{displaymath}
%     \mathit{append}\ (\mathit{append}\ (\eta\ (\construct\ x), \mathit{ys}), \mathit{zs}) = f_1\ (F\ (\lambda \mathit{xs}.\ \mathit{append}\ (\mathit{append}\ (\mathit{xs}, \mathit{ys}), \mathit{zs}))\ x)
%   \end{displaymath}
%   where
%   \begin{displaymath}
%     \begin{array}{l}
%       f_1\ \mathsf{Nil} = \mathit{append}\ (\mathit{xs}, \mathit{ys}) \\
%       f_1\ (\mathsf{Cons}\ a\ l) = \eta\ (\construct\ (\mathsf{Cons}\ a\ l))
%     \end{array}
%   \end{displaymath}

%   Case $x = \mathsf{Nil}$. The left hand side of the equation is:
%   \begin{displaymath}
%     \begin{array}{cl}
%       & \mathit{append}\ (\mathit{append}\ (\eta\ (\construct\ \mathsf{Nil}), \mathit{ys}), \mathit{zs}) \\
%       =&\mathit{append}\ (\mathit{ys}, \mathit{zs})
%     \end{array}
%   \end{displaymath}
%   and the right hand side of the equation is:
%   \begin{displaymath}
%     \begin{array}{cl}
%       & f_1\ (F\ (\lambda\mathit{xs}.\ \mathit{append}\ (\mathit{append}\ (\mathit{xs}, \mathit{ys}), \mathit{zs}))\ \mathsf{Nil}) \\
%       =&f_1\ \mathsf{Nil} \\
%       =&\mathit{append}\ (\mathit{xs}, \mathit{ys})
%     \end{array}
%   \end{displaymath}
%   as required.

%   Case $x = \mathsf{Cons}\ a\ \mathit{xs}$. The left hand side of the equation is:
%   \begin{displaymath}
%     \begin{array}{cl}
%       & \mathit{append}\ (\mathit{append}\ (\eta\ (\construct\ (\mathsf{Cons}\ a\ \mathit{xs})), \mathit{ys}), \mathit{zs}) \\
%       =&\mathit{append}\ (\eta\ (\construct\ (\mathsf{Cons}\ a\ (\mathit{append}\ (\mathit{xs}, \mathit{ys})))), \mathit{zs}) \\
%       =&\eta\ (\construct\ (\mathsf{Cons}\ a\ (\mathit{append}\ (\mathit{append}\ (\mathit{xs}, \mathit{ys}), \mathit{zs}))))
%     \end{array}
%   \end{displaymath}
%   and the right hand side of the equation to be proved is:
%   \begin{displaymath}
%     \begin{array}{cl}
%       & f_1\ (F\ (\lambda \mathit{xs}.\ \mathit{append}\ (\mathit{append}\ (\mathit{xs}, \mathit{ys}), \mathit{zs}))\ (\mathsf{Cons}\ a\ \mathit{xs})) \\
%       =&f_1\ (\mathsf{Cons}\ a\ (\mathit{append}\ (\mathit{append}\ (\mathit{xs}, \mathit{ys}), \mathit{zs}))) \\
%       =&\eta\ (\construct\ (\mathsf{Cons}\ a\ (\mathit{append}\ (\mathit{append}\ (\mathit{xs}, \mathit{ys}), \mathit{zs}))))
%     \end{array}
%   \end{displaymath}
%   as required.

%   The second property is:
%   \begin{displaymath}
%     \mathit{append}\ (\mathit{append}\ (\mu\ x, \mathit{ys}), \mathit{zs})
%     = \mu\ (T\ (\lambda \mathit{xs}.\ \mathit{append}\ (\mathit{append}\ \mathit{xs}, \mathit{ys}), \mathit{zs})\ x)
%   \end{displaymath}
%   We reason as follows, using the Eilenberg-Moore algebra homomorphism
%   property of $\mathit{append}$, and the fact that functors preserve
%   composition:
%   \begin{displaymath}
%     \begin{array}{cl}
%       & \mathit{append}\ (\mathit{append}\ (\mu\ x, \mathit{ys}), \mathit{zs}) \\
%       =&\mathit{append}\ (\mu\ (T\ (\lambda \mathit{xs}.\ \mathit{append}\ (\mathit{xs}, \mathit{ys}))\ x), \mathit{zs}) \\
%       =&\mu\ (T\ (\lambda\mathit{xs}.\ \mathit{append}\ (\mathit{xs}, \mathit{zs}))\ (T\ (\lambda\mathit{xs}.\ \mathit{append}\ (\mathit{xs}, \mathit{ys}))\ \mathit{x})) \\
%       =&\mu\ (T\ (\lambda\mathit{xs}.\ \mathit{append}\ (\mathit{append}\ (\mathit{xs}, \mathit{ys}), \mathit{zs}))\ \mathit{x})
%     \end{array}
%   \end{displaymath}
%   as required. Thus the property is proved.
% \end{proof}

\end{document}
